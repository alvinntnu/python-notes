{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKIP Tagger {#ckiptagger}\n",
    "\n",
    "The current state-of-art Chinese segmenter for Taiwan Mandarin available is probably the [CKIP tagger](https://github.com/ckiplab/ckiptagger), created by the [Chinese Knowledge and Information Processing (CKIP)](https://ckip.iis.sinica.edu.tw/) group at the Academia Sinica.\n",
    "\n",
    "The `ckiptagger` is released as a python module. In this chpater, I will demonstrate how to use the module for Chinese word segmentation but in an R environment, i.e., how to integrate Python modules in R coherently to perform complex tasks.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Because `ckiptagger` is built in python, we need to have python installed in our working environment. Please install the following applications on your own before you start:\n",
    "\n",
    "- [Anaconda + Python 3.6+](https://www.anaconda.com/distribution/)\n",
    "- `ckiptagger` module in Python (Please install the module using the `Anaconda Navigator` or `pip install` in the terminal)\n",
    "\n",
    "(**Please consult the github of the [`ckiptagger`](https://github.com/ckiplab/ckiptagger) for more details on installation.**)\n",
    "\n",
    "```{note}\n",
    "For some reasons, the module `ckiptagger` may not be found in the base channel. In `Anaconda Navigator`, if you cannot find this module, please add specifically the following channel to the environment so that your Anaconda can find `ckiptagger` module:\n",
    "\n",
    "`https://conda.anaconda.org/roccqqck`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Model Files\n",
    "\n",
    "All NLP applications have their models behind their fancy performances. To use the tagger provided in `ckiptagger`, we need to download their pre-trained model files. \n",
    "\n",
    "Please go to the [github of CKIP tagger](https://github.com/ckiplab/ckiptagger) to download the model files, which is provided as a zipped file. (The file is very big. It takes a while.)\n",
    "\n",
    "After you download the zipped file, unzip it under your working directory to the `data/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation\n",
    "\n",
    "Before we proceed, please check if you have everything ready (The following includes the versions of the modules used for this session):\n",
    "\n",
    "- Anaconda + Python 3.6+ (`Python 3.6.10`)\n",
    "- Python modules: `ckiptagger` (`ckiptagger 0.1.1` + `tensorflow 1.13.1`)\n",
    "- CKIP model files under your working directory `./data`\n",
    "\n",
    "If yes, then we are ready to go.\n",
    "\n",
    "\n",
    "## Creating Conda Environment for `ckiptagger`\n",
    "\n",
    "I would suggest to install all necessary Python modules in a conda environment for easier use. \n",
    "\n",
    "In the following demonstration, I assume that you have created a conda environment `ckiptagger`, where all the necessary modules (i.e., `ckiptagger`, `tensorflow`) have been pip-installed.\n",
    "\n",
    "```\n",
    "# isntsall in terminal\n",
    "## create new env\n",
    "conda create --name ckiptagger python=3.6\n",
    "conda activate ckiptagger\n",
    "pip install -U ckiptagger\n",
    "## AND INSTALL EVERYTHING NEEDED FOR YOUR PROJECR\n",
    "\n",
    "## deactivate env when you are done\n",
    "conda deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenting Texts\n",
    "\n",
    "The initialized word segmenter object, `ws()`, can tokenize any input **character vectors** into a list of **word vectors** of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alvin/opt/anaconda3/envs/ckiptagger/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Alvin/opt/anaconda3/envs/ckiptagger/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Alvin/opt/anaconda3/envs/ckiptagger/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Alvin/opt/anaconda3/envs/ckiptagger/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Alvin/opt/anaconda3/envs/ckiptagger/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Alvin/opt/anaconda3/envs/ckiptagger/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter Path\n",
    "MODEL_PATH = '../../../NTNU/CorpusLinguistics/CorpusLinguistics_bookdown/data/'\n",
    "#'/Users/Alvin/Dropbox/NTNU/CorpusLinguistics/CorpusLinguistics_bookdown/data/'\n",
    "## Loading model\n",
    "#ws = WS('/Users/Alvin/Dropbox/NTNU/CorpusLinguistics/CorpusLinguistics_bookdown/data/')\n",
    "ws = WS(MODEL_PATH)\n",
    "#ws = WS('../../../NTNU/CorpusLinguistics/CorpusLinguistics_bookdown/data/')\n",
    "pos = POS(MODEL_PATH)\n",
    "ner = NER(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-227593c91cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "## Raw text corpus \n",
    "sentence_list = ['傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。',\n",
    "              '美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。',\n",
    "              '土地公有政策?？還是土地婆有政策。',\n",
    "              '… 你確定嗎… 不要再騙了……他來亂的啦',\n",
    "              '最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.',\n",
    "              '科長說:1,坪數對人數為1:3。2,可以再增加。']\n",
    "    ## other parameters\n",
    "    # sentence_segmentation = True, # To consider delimiters\n",
    "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    "\n",
    "word_list = ws(sentence_list)\n",
    "pos_list = pos(word_list)\n",
    "entity_list = ner(word_list, pos_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。'\n",
      "傅達仁(Nb)　今(Nd)　將(D)　執行(VC)　安樂死(Na)　，(COMMACATEGORY)　卻(D)　突然(D)　爆出(VJ)　自己(Nh)　20(Neu)　年(Nf)　前(Ng)　遭(P)　緯來(Nb)　體育台(Na)　封殺(VC)　，(COMMACATEGORY)　他(Nh)　不(D)　懂(VK)　自己(Nh)　哪裡(Ncd)　得罪到(VJ)　電視台(Nc)　。(PERIODCATEGORY)　\n",
      "(0, 3, 'PERSON', '傅達仁')\n",
      "(18, 22, 'DATE', '20年前')\n",
      "(23, 28, 'ORG', '緯來體育台')\n",
      "\n",
      "'美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。'\n",
      "美國(Nc)　參議院(Nc)　針對(P)　今天(Nd)　總統(Na)　布什(Nb)　所(D)　提名(VC)　的(DE)　勞工部長(Na)　趙小蘭(Nb)　展開(VC)　認可(VC)　聽證會(Na)　，(COMMACATEGORY)　預料(VE)　她(Nh)　將(D)　會(D)　很(Dfa)　順利(VH)　通過(VC)　參議院(Nc)　支持(VC)　，(COMMACATEGORY)　成為(VG)　該(Nes)　國(Nc)　有史以來(D)　第一(Neu)　位(Nf)　的(DE)　華裔(Na)　女性(Na)　內閣(Na)　成員(Na)　。(PERIODCATEGORY)　\n",
      "(0, 2, 'GPE', '美國')\n",
      "(2, 5, 'ORG', '參議院')\n",
      "(7, 9, 'DATE', '今天')\n",
      "(11, 13, 'PERSON', '布什')\n",
      "(17, 21, 'ORG', '勞工部長')\n",
      "(21, 24, 'PERSON', '趙小蘭')\n",
      "(42, 45, 'ORG', '參議院')\n",
      "(56, 58, 'ORDINAL', '第一')\n",
      "(60, 62, 'NORP', '華裔')\n",
      "\n",
      "'土地公有政策?？還是土地婆有政策。'\n",
      "土地公有(VH)　政策(Na)　?(QUESTIONCATEGORY)　？(QUESTIONCATEGORY)　還是(Caa)　土地(Na)　婆(Na)　有(V_2)　政策(Na)　。(PERIODCATEGORY)　\n",
      "\n",
      "'… 你確定嗎… 不要再騙了……他來亂的啦'\n",
      "…(ETCCATEGORY)　 (WHITESPACE)　你(Nh)　確定(VK)　嗎(T)　…(ETCCATEGORY)　 (WHITESPACE)　不要(D)　再(D)　騙(VC)　了(Di)　…(ETCCATEGORY)　…(ETCCATEGORY)　他(Nh)　來(D)　亂(VH)　的(T)　啦(T)　\n",
      "\n",
      "'最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.'\n",
      "最多(VH)　容納(VJ)　59,000(Neu)　個(Nf)　人(Na)　,(COMMACATEGORY)　或(Caa)　5.9萬(Neu)　人(Na)　,(COMMACATEGORY)　再(D)　多(D)　就(D)　不行(VH)　了(T)　.(PERIODCATEGORY)　這(Nep)　是(SHI)　環評(Na)　的(DE)　結論(Na)　.(PERIODCATEGORY)　\n",
      "(4, 10, 'CARDINAL', '59,000')\n",
      "(14, 18, 'CARDINAL', '5.9萬')\n",
      "\n",
      "'科長說:1,坪數對人數為1:3。2,可以再增加。'\n",
      "科長(Na)　說(VE)　:1,(Neu)　坪數(Na)　對(P)　人數(Na)　為(VG)　1:3(Neu)　。(PERIODCATEGORY)　2(Neu)　,(COMMACATEGORY)　可以(D)　再(D)　增加(VHC)　。(PERIODCATEGORY)　\n",
      "(4, 6, 'CARDINAL', '1,')\n",
      "(12, 13, 'CARDINAL', '1')\n",
      "(14, 15, 'CARDINAL', '3')\n",
      "(16, 17, 'CARDINAL', '2')\n"
     ]
    }
   ],
   "source": [
    "def print_word_pos_sentence(word_sentence, pos_sentence):\n",
    "    assert len(word_sentence) == len(pos_sentence)\n",
    "    for word, pos in zip(word_sentence, pos_sentence):\n",
    "        print(f\"{word}({pos})\", end=\"\\u3000\")\n",
    "    print()\n",
    "    return\n",
    "    \n",
    "for i, sentence in enumerate(sentences):\n",
    "    print()\n",
    "    print(f\"'{sentence}'\")\n",
    "    print_word_pos_sentence(words[i],  pos[i])\n",
    "    for entity in sorted(entities[i]):\n",
    "        print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word segmenter `ws()` returns a `list` object, each element of which is a word-based vector of the original sentence.\n",
    "\n",
    "## Define Own Dictionary\n",
    "\n",
    "The performance of Chinese word segmenter depends highly on the dictionary. Texts in different disciplines may have very domain-specific vocabulary. To prioritize a set of words in a dictionary, we can further ensure the accuracy of the word segmentation.\n",
    "\n",
    "To create a dictionary for `ckiptagger`, we need a **named list**, i.e., to create a `list` with element **names** = \"the new words\" and element **values** = \"the weights\".\n",
    "\n",
    "Then we use the python function `ckip$construct_dictionary()` to create the `dictionary` Python object, which is the input argument for word segmenter `ws(..., recommend_dictionary = ...)`.\n",
    "\n",
    "```{r}\n",
    "# Define new words in own dictionary\n",
    "new_words <- c(\"土地公有\",\n",
    "               \"土地公\",\n",
    "               \"土地婆\",\n",
    "               \"來亂的\",\n",
    "               \"啦\",\n",
    "               \"緯來體育台\")\n",
    "\n",
    "# Transform the `vector` into `list` for Python\n",
    "new_words_py <- c(2, 1, 1, 1, 1, 1) %>% as.list\n",
    "\n",
    "  # cf. `list(rep, 1 , length(new_words))`\n",
    "names(new_words_py) <- new_words\n",
    "\n",
    "  # To create a dictionary for `construct_dictionary()`\n",
    "  # We need a list, with names as the words and list elements as the weights in the dictionary\n",
    "\n",
    "# Create Python `dictionary` object, required by `ckiptagger.wc()`\n",
    "dictionary<-ckip$construct_dictionary(new_words_py)\n",
    "\n",
    "# Segment texts using dictionary\n",
    "words_1 <- ws(texts, recommend_dictionary = dictionary)\n",
    "words_1\n",
    "```\n",
    "\n",
    "```{exercise}\n",
    "We usually have a list of new words saved in a text file. Can you write a R function, which loads the words in the `demo_data/dict-sample.txt` into a named `list`, i.e., `new_words`, which can easily serve as the input for `ckip$construct_dictionary()` to create the python `dictionary` object? (Note: All weights are default to 1)\n",
    "```\n",
    "\n",
    "```{r echo = F, eval = T, purl=F}\n",
    "loadDictionary <- function(input = \"\"){\n",
    "  words <- readLines(input)\n",
    "  weights <- as.list(rep(1, length(words)))\n",
    "  names(weights)<-words\n",
    "  return(weights)  \n",
    "}# endfunc\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "new_words<-loadDictionary(input = \"demo_data/dict-sample.txt\") \n",
    "dictionary<-ckip$construct_dictionary(new_words)\n",
    "# Segment texts using dictionary\n",
    "words_2 <- ws(texts, recommend_dictionary = dictionary)\n",
    "words_2\n",
    "```\n",
    "\n",
    "## Beyond Word Boundaries\n",
    "\n",
    "In addition to primitive word segmentation, the `ckiptagger` provides also the parts-of-speech tags for words and named entity recognitions for the texts. The `ckiptagger` follows the pipeline below for text processing.\n",
    "\n",
    "```{r eval= T, echo = F, purl=F}\n",
    "library(DiagrammeR)\n",
    "grViz(\"digraph flowchart {\n",
    "      # node definitions with substituted label text\n",
    "      node [fontname = Helvetica, shape = rectangle]        \n",
    "      tab1 [label = '@@1']\n",
    "      tab2 [label = '@@2']\n",
    "      tab3 [label = '@@3']\n",
    "      tab4 [label = '@@4']\n",
    "\n",
    "\n",
    "      # edge definitions with the node IDs\n",
    "      tab1 -> tab2 -> tab3 -> tab4;\n",
    "      }\n",
    "\n",
    "      [1]: 'Raw Texts'\n",
    "      [2]: 'Words'\n",
    "      [3]: 'Parts-of-Speech'\n",
    "      [4]: 'Named Entity Recognition (NER)'\n",
    "      \")\n",
    "```\n",
    "\n",
    "- Load the models\n",
    "\n",
    "To perform these additional tasks, we need to load the necessary models (pre-trained and provided by the CKIP group) first as well. \n",
    "\n",
    "They should all have been included in the model directory you unzipped earlier (cf. `./data`).\n",
    "\n",
    "```{r eval = T}\n",
    "# loading other necessary models\n",
    "system.time((pos <- ckip$POS(\"./data\"))) # 詞性 6s\n",
    "system.time((ner <- ckip$NER(\"./data\"))) # 實體辨識 8s\n",
    "```\n",
    "\n",
    "- POS tagging and NER\n",
    "\n",
    "```{r eval = T}\n",
    "# Parts-of-speech Tagging\n",
    "pos_words <- pos(words_1)\n",
    "pos_words\n",
    "\n",
    "# Named Entity Recognition\n",
    "ner <- ner(words_1, pos_words)\n",
    "ner\n",
    "```\n",
    "\n",
    "## Tidy Up the Results\n",
    "\n",
    "We can tidy up results provided by `ckiptagger` and create a word-based tidy structure of our data:\n",
    "\n",
    "```{r eval = T}\n",
    "word_df <- data.frame(text_id = mapply(rep, c(1:length(texts)), sapply(words_1, length)) %>% unlist,\n",
    "                        words = do.call(c, words_1),\n",
    "                        pos = do.call(c, pos_words))\n",
    "word_df\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "```{exercise}\n",
    "With a word-based tidy structure of the corpus, it is easy to convert it into a text-based one with both the information of word boundaries and parts-of-speech tag. \n",
    "\n",
    "Please convert the above `word_df` into a text-based data frame, as shown below.\n",
    "```\n",
    "\n",
    "```{r echo=F, purl=F}\n",
    "#require(dplyr)\n",
    "word_df %>%\n",
    "  group_by(text_id) %>%\n",
    "  summarize(text = str_c(words, pos, sep=\"/\") %>% str_c(collapse=\"\\u3000\")) %>%\n",
    "  ungroup\n",
    "```\n",
    "\n",
    "```{exercise}\n",
    "How to tidy up the results of `ner` so that we can include the recognized named entities in the same word-based data frame `word_df`?\n",
    "```\n",
    "\n",
    "- You may need to convert the output of `ner` from ckiptagger into a data frame like this:\n",
    "\n",
    "```{r eval=T, echo=F, purl=F}\n",
    "library(stringr)\n",
    "library(readr)\n",
    "\n",
    "extract_ner_df <- function(x){\n",
    "  x %>% str_extract_all(\"\\\\([^\\\\)]+?\\\\)\") %>% \n",
    "    unlist %>%\n",
    "    str_replace_all(\"\\\\((\\\\d+), (\\\\d+), '(\\\\w+)', '([^']+)'\\\\)\",\"\\\\1\\t\\\\2\\t\\\\3\\t\\\\4\") \n",
    "} # endfunc\n",
    "\n",
    "# extract ner\n",
    "tibble(ner_raw = ner) %>%\n",
    "  mutate(ner_extract = sapply(ner, extract_ner_df)) %>%\n",
    "  mutate(text_id = row_number()) %>%\n",
    "  dplyr::select(-ner_raw) %>%\n",
    "  tidyr::unnest(ner_extract) %>%\n",
    "  tidyr::separate(col=\"ner_extract\", \n",
    "                  into = c(\"start\",\"end\", \"ner_type\", \"string\"),\n",
    "                  sep =\"\\t\") %>%\n",
    "  mutate(text_id_start = paste(text_id, start, sep=\"-\"),\n",
    "         text_id_end = paste(text_id, end, sep=\"-\")) ->ner_df\n",
    "\n",
    "ner_df %>% arrange(text_id, start) %>%\n",
    "  select(text_id, start, end, ner_type, string)\n",
    "#%>%\n",
    "  #select(text_id_start, text_id_end, ner_type, string)\n",
    "```\n",
    "\n",
    "- And figure out a way to add the annotations of named entities in the word-based data frame, `word_df`, by including another column, as shown below:\n",
    "\n",
    "```{r eval=T, echo=F, purl=F}\n",
    "library(dplyr)\n",
    "library(tidytext)\n",
    "library(tidyr)\n",
    "\n",
    "# word-based to char-based\n",
    "word_df %>%\n",
    "  mutate(word_id = row_number()) %>%\n",
    "  unnest_tokens(characters,\n",
    "               words,\n",
    "               token = function(x) str_split(x, \"\")) %>%\n",
    "  group_by(text_id) %>%\n",
    "  mutate(char_id = row_number()) %>%\n",
    "  ungroup %>%\n",
    "  mutate(text_char_id = paste(text_id, char_id, sep=\"-\"))-> char_df\n",
    "\n",
    "# create `tag`\n",
    "tag <- rep(\"O\", nrow(char_df))\n",
    "names(tag) <- char_df$text_char_id\n",
    "\n",
    "# for each ner\n",
    "for(i in 1:nrow(ner_df)){\n",
    "  cur_textid <- ner_df$text_id[i]\n",
    "  char_start <- paste(cur_textid, as.numeric(ner_df$start[i])+1, sep=\"-\")\n",
    "  char_end <- paste(cur_textid, as.numeric(ner_df$end[i]), sep=\"-\")\n",
    "  tag[which(names(tag)==char_start):which(names(tag)==char_end)]<-sprintf(\"B-%s\", ner_df$ner_type[i])\n",
    "  if(which(names(tag)==char_start)!=which(names(tag)==char_end))\n",
    "    tag[(which(names(tag)==char_start)+1):which(names(tag)==char_end)] <- sprintf(\"I-%s\", ner_df$ner_type[i])\n",
    "}# endfor\n",
    "  \n",
    "char_df %>% mutate(tag = tag) %>%\n",
    "  select(-char_id, -text_char_id) %>%\n",
    "  group_by(text_id, pos, word_id) %>%\n",
    "  nest %>%\n",
    "  mutate(words = map_chr(data, function(x) x$characters %>% unlist %>% str_c(collapse=\"\")),\n",
    "         tag = map_chr(data, function(x) x$tag %>% unlist %>% .[1])) %>%\n",
    "  select(-data) -> word_df2\n",
    "\n",
    "word_df2 %>%\n",
    "  select(text_id, word_id, words, pos, tag)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "```{block, type=\"info\"}\n",
    "The above result data frame makes use of the **IOB format** (short for inside, outside, beginning) for the annotations of the named entities. \n",
    "\n",
    "It is a common tagging format for tagging (multiword) tokens in a chunking task in computational linguistics (e.g., NP-chunking, named entitity, semantic roles). \n",
    "\n",
    "- The **B-** prefix before a tag indicates that the tag is the *beginning* of a chunk. \n",
    "- The **I-** prefix before a tag indicates that the tag is *inside* a chunk. \n",
    "- The **O** tag indicates that a token belongs to no chunk (i.e., *outside* of all relevant chunks).\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckiptagger",
   "language": "python",
   "name": "ckiptagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
