{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarities from Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, install `spacy` and the Chinese language model `zh_core_web_lg` (glove embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy==2.3\n",
    "# !spacy download zh_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_zh = spacy.load('zh_core_web_lg')\n",
    "\n",
    "near_syns = ['覺得','認為','宣稱','表示','強調','顯示', '說明','指出','提出','主張']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the word vectors matrix from the spacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_word_vectors = nlp_zh.vocab.vectors\n",
    "print('Spacy GloVe word vectors Shape: (vocab_size, embedding_dim)',glove_word_vectors.shape)\n",
    "len(nlp_zh.vocab.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the similarities of 認為 with the other words in the near-syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = nlp_zh.vocab['認為']\n",
    "w2 = nlp_zh.vocab['覺得']\n",
    "\n",
    "# w1 similarities with others\n",
    "for w in near_syns:\n",
    "    if w !=w1:\n",
    "        w_text = w\n",
    "        w =nlp_zh.vocab[w]\n",
    "        print(w_text, ':', w1.similarity(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the computation cost, extract the vocabulary of the Chinense model by excluding:\n",
    "- ascii characters\n",
    "- digits\n",
    "- punctuations\n",
    "\n",
    "And also, consider only two-character words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544337\n",
      "['2022', '022', '乌拉特后旗', '特后旗', '温差', '湘潭', 'play', '留學', '索取', '透明度', '孤立', '伊始', '安全法', '故居', '中医院', '番茄', '07月', '历任', '預算', '十字', '手柄', '利润率', '133', '涛', 'Office', '宝博', '企稳', '加�', '代辦', '紧缺', '重现', '冲着', '大利', '播种', '随手', '克什克腾旗', '克腾旗', \"'s\", \"'x\", '换来', '受灾', '亮眼', '峦�', '峦', '计数', '操穴', '100米', '00米', '展品', '帶動', '前任', 'a站', '表率', '社科', '供奉', '安检', '吉泽明', '学生会', '三线', '清凉', '取暖', '隐蔽', '无所谓', '不在乎', '粗大', '串联', '切尔西', '時光', '增殖', '宜宾市', '溫暖', '燕子', '燕', '后天', '冒出', '權力', '倫敦', '波司登', '胜地', '值当', '康健', '协和', '朴素', '胸口', '樱花', '樱', '孔明', '少许', '嵌入', '镍', '掘金', '掘', '推�', '项链', '包赢', '制作人', '增产', '交流区', '妆品', '妆', '温恒', '未婚', '非金属', '事前', '台账', '强强', '银行家', '大树', '小哥', '纱', '肤色', '肤', '陡然', '陡', '打水', '電源', '项目部', '樂團', '兩位', '来不及', '邻家', '外星人', '黄网站', '南充', '市直', '带入', '電影院', '摔倒', '礼服', '建造师', '５', '自拍区', '贯通', '沿岸', '透玩', 'LOGO', 'logo', 'OGO', '他家', '领空', '稀少', '13%', '山林', '频w', '算单', '田野', '猜想', '這裏', '增強', '文山', '不俗', '收費', '配电', '利害', '萌', '捡', '开播', '依规', '深知', '株洲', '产视', '\\x07\\x06\\x05', '玩场', '事務', '常州市', 'MB', 'mb', '一早', '乐网', '祖母', '二季度', 'AV天堂', 'V天堂', 'XXxx', '石川', '解散', '天国', '开房', '野狼', '法语', '承德市', '赶来', '光临', '涉事', '著作权', '老人们', '怀化', '节水', '米兜', '腰部', '尤物', '频自', '黑洞', '编导', '永利网', '乾淨', '戀愛', '戀', '私阴', '脱水']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(nlp_zh.vocab.strings)\n",
    "#       if np.count_nonzero(w.vector) and not w.is_ascii and not w.is_punct and len(w.text)==2\n",
    "# ]\n",
    "print(len(vocab))\n",
    "print(vocab[20000:20200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = '覺得'\n",
    "word_sim = []\n",
    "# check each word in vocab its simi with target_Word\n",
    "\n",
    "target_word_vocab = nlp_zh.vocab[target_word]\n",
    "for w in vocab:\n",
    "    w_vocab = nlp_zh.vocab[w]\n",
    "    if w_vocab.vector is not None and np.count_nonzero(w_vocab.vector):\n",
    "        word_sim.append((w, target_word_vocab.similarity(w_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('覺得', 1.0),\n",
       " ('覺', 0.8478886),\n",
       " ('其實', 0.7956978),\n",
       " ('會覺', 0.7882689),\n",
       " ('以為', 0.78638536),\n",
       " ('感覺', 0.78400886),\n",
       " ('看來', 0.77983254),\n",
       " ('畢竟', 0.7633344),\n",
       " ('看起來', 0.76294947),\n",
       " ('因為', 0.7625315)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_sim, key=lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `vocab` has several properties that are useful for filtering irrelevant words before computing the word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#w.is_lower == word.is_lower and w.prob >= -15\n",
    "\n",
    "w1 = nlp_zh.vocab['覺得']\n",
    "w2 = nlp_zh.vocab['ship']\n",
    "\n",
    "w2.is_ascii\n",
    "w2.is_currency\n",
    "w2.is_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to extract top-N similar words\n",
    "\n",
    "- Functions taken from [this SO discussion thread](https://stackoverflow.com/questions/57697374/list-most-similar-words-in-spacy-in-pretrained-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def cosine_similarity_numba(u:np.ndarray, v:np.ndarray):\n",
    "    assert(u.shape[0] == v.shape[0])\n",
    "    uv = 0\n",
    "    uu = 0\n",
    "    vv = 0\n",
    "    for i in range(u.shape[0]):\n",
    "        uv += u[i]*v[i]\n",
    "        uu += u[i]*u[i]\n",
    "        vv += v[i]*v[i]\n",
    "    cos_theta = 1\n",
    "    if uu != 0 and vv != 0:\n",
    "        cos_theta = uv/np.sqrt(uu*vv)\n",
    "    return cos_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_v1(word, topn=5):\n",
    "  word = nlp_zh.vocab[str(word)]\n",
    "  queries = [\n",
    "      w for w in nlp_zh.vocab \n",
    "      if np.count_nonzero(w.vector) and not w.is_ascii and not w.is_punct and len(w.text)==2\n",
    "  ]\n",
    "\n",
    "  #by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "\n",
    "  by_similarity = sorted(queries, key=lambda w: cosine_similarity_numba(w.vector, word.vector), reverse=True)\n",
    "    \n",
    "    \n",
    "  return [(w.text,w.similarity(word)) for w in by_similarity[:topn+1] if w.text != word.text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_v2(word, topn=5):\n",
    "  word = nlp_zh.vocab[str(word)]\n",
    "  queries = [\n",
    "      w for w in nlp_zh.vocab \n",
    "      if np.count_nonzero(w.vector) and not w.is_ascii and not w.is_punct and len(w.text)==2\n",
    "  ]\n",
    "\n",
    "  by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "  #by_similarity = sorted(queries, key=lambda w: cosine_similarity_numba(w.vector, word.vector), reverse=True)\n",
    "\n",
    "  return [(w.text,w.similarity(word)) for w in by_similarity[:topn+1] if w.text != word.text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.19 s, sys: 319 ms, total: 7.51 s\n",
      "Wall time: 28.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('其實', 0.7956978), ('會覺', 0.7882689), ('以為', 0.78638536)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "most_similar_v1(\"覺得\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "most_similar_v2(\"覺得\", topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract top N similar words for all near-syns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "near_syn_topn = dict([(w, most_similar_v1(w, topn=500)) for w in near_syns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 similar words for `near_syns[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "near_syn_topn[near_syns[0]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_syn_topn_list = []\n",
    "for w, s in near_syn_topn.items():\n",
    "    for s_w, s_s in s:\n",
    "        near_syn_topn_list.append((w, s_w, s_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(near_syn_topn_list[:10])\n",
    "print(len(near_syn_topn_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(near_syn_topn_list,columns=['w1','w2','sim'])\n",
    "df[df['sim']>0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import cosine\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['sim'] > 0.5]\n",
    "nodes_id = list(set(list(df2['w2'].values) + list(df2['w1'].values)))\n",
    "#print(nodes_id)\n",
    "print(len(nodes_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word vectors of all nodes from spaCy\n",
    "- Pairwise cosine similarity of all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vectors of all nodes\n",
    "#x = np.array([nlp_zh(w).vector for w in nodes_id])\n",
    "\n",
    "m = len(list(nodes_id))\n",
    "distances = np.zeros((m,m))\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(m):  \n",
    "        distances[i,j] = nlp_zh.vocab[nodes_id[i]].similarity(nlp_zh.vocab[nodes_id[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten        \n",
    "\n",
    "\n",
    "#print(node_names)\n",
    "distances_flat = []\n",
    "CUTOFF = 0.8\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        if distances[i,j]> 0.8 and i != j:\n",
    "            distances_flat.append((nodes_id[i], nodes_id[j], distances[i,j]))\n",
    "\n",
    "edges_df = pd.DataFrame(distances_flat, columns=['w1','w2','sim'])\n",
    "edges_df.loc[100:140,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(edges_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = edges_df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G= nx.from_pandas_edgelist(edges_df, 'w1','w2','sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myRescaler(x):\n",
    "    x = np.array(x)\n",
    "    y = np.interp(x, (x.min(), x.max()), (5, 20))\n",
    "    return list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = pd.DataFrame({'id':list(G.nodes),\n",
    "#                          'visnodes':Gvis.get_nodes(),\n",
    "                         'betweenness': myRescaler(list(nx.betweenness_centrality(G).values())),\n",
    "                         'eigenvector': myRescaler(list(nx.eigenvector_centrality(G).values()))})\n",
    "nodes_df['size']=[5 if i not in near_syns else 10 for i in nodes_id]\n",
    "nodes_df['size2']= [i if i not in near_syns else 30 for i in nodes_df['eigenvector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gvis = Network(\"1024px\",\"1024px\", notebook=False)\n",
    "# # Gvis.from_nx(G)\n",
    "Gvis.add_nodes(list(G.nodes), value=nodes_df['size2'])\n",
    "Gvis.add_edges(list(G.edges))\n",
    "Gvis.show_buttons(filter_=['physics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Gvis.show('Gvis.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
