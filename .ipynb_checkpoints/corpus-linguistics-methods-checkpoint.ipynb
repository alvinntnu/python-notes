{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Lingustics Methods\n",
    "\n",
    "- With `nltk`, we can easily implement quite a few corpus-linguistic methods\n",
    "    - Concordance Analysis (Simple Words)\n",
    "    - Frequency Lists\n",
    "    - Collocations\n",
    "    - Data Analysis with R\n",
    "    - Concordance Analysis (Patterns, Constructions?)\n",
    "        - Patterns on sentence strings\n",
    "        - Patterns on sentence word-tag strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Corpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.text import Text\n",
    "import pandas as pd\n",
    "\n",
    "brown_text = Text(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "- Documentation [nltk.collocations](https://www.nltk.org/howto/collocations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collocations based on Text\n",
    "brown_text.collocation_list()[:10]\n",
    "#brown_text.collocations()\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$10,000-per-year', 'French-born'),\n",
       " ('$79.89', 'nothing-down'),\n",
       " ('$8.50', 'tab'),\n",
       " (\"'low\", 'nigras'),\n",
       " ('0.5-mv./m.', '50-percent'),\n",
       " ('0.78', 'mEq'),\n",
       " ('1,100', 'circumscriptions'),\n",
       " ('1,257,700', 'non-farm'),\n",
       " ('11-inch', 'headroom'),\n",
       " ('11-shot', 'hammerless')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## bigram collocations based on different association measures\n",
    "finder.nbest(bigram_measures.likelihood_ratio,10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hong', 'Kong'),\n",
       " ('Viet', 'Nam'),\n",
       " ('Pathet', 'Lao'),\n",
       " ('Simms', 'Purdew'),\n",
       " ('El', 'Paso'),\n",
       " ('Lo', 'Shu'),\n",
       " ('Internal', 'Revenue'),\n",
       " ('Puerto', 'Rico'),\n",
       " ('Saxon', 'Shore'),\n",
       " ('carbon', 'tetrachloride')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply freq-based filers for bigram collocations\n",
    "finder.apply_freq_filter(10)\n",
    "\n",
    "## Apply word filer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_en = stopwords.words('english')\n",
    "finder.apply_word_filter(lambda x: not x.isalpha())\n",
    "\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('A', 'AT-NC'), ('charge', 'NN-NC')),\n",
       " (('A', 'FW-IN'), ('quibusdam', 'FW-WPO')),\n",
       " (('ACTH', 'NP'), ('Gel', 'NN-TL')),\n",
       " (('Abner', 'NP'), ('Haynes', 'NP')),\n",
       " (('Ace', 'NN-TL'), ('Driver', 'NN-TL')),\n",
       " (('Acoustical', 'JJ-HL'), ('interferometer', 'NN-HL')),\n",
       " (('Actively', 'RB-HL'), ('modernizing', 'VBG-HL')),\n",
       " (('Ad', 'FW-IN-TL'), ('Amicam', 'FW-NN-TL')),\n",
       " (('Adjusted', 'VBN-HL'), ('gross', 'JJ-HL')),\n",
       " (('Aerobacter', 'NP'), ('aerogenes', 'NP'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collocations based on tagged words\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    brown.tagged_words())\n",
    "finder.apply_word_filter(lambda x: not x[0].isalpha())\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X', 'X'),\n",
       " ('PRON', 'VERB'),\n",
       " ('PRT', 'VERB'),\n",
       " ('ADP', 'DET'),\n",
       " ('DET', 'ADJ'),\n",
       " ('ADJ', 'NOUN'),\n",
       " ('.', 'CONJ'),\n",
       " ('DET', 'NOUN'),\n",
       " ('VERB', 'PRT'),\n",
       " ('.', 'PRON')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collcoations based on tags only\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    t for w, t in brown.tagged_words(tagset='universal'))\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hong', 'Kong'),\n",
       " ('Viet', 'Nam'),\n",
       " ('Pathet', 'Lao'),\n",
       " ('Simms', 'Purdew'),\n",
       " ('El', 'Paso'),\n",
       " ('Lo', 'Shu'),\n",
       " ('Internal', 'Revenue'),\n",
       " ('Puerto', 'Rico'),\n",
       " ('Saxon', 'Shore'),\n",
       " ('carbon', 'tetrachloride')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collocations with intervneing words (gapped n-grams)\n",
    "finder = BigramCollocationFinder.from_words(brown.words(), window_size=2)\n",
    "finder.apply_word_filter(lambda x: not x.isalpha())\n",
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 0.008288896237659233),\n",
       " (('in', 'the'), 0.004776126600941102),\n",
       " (('to', 'the'), 0.002950416468594341),\n",
       " (('on', 'the'), 0.0019781397047172215),\n",
       " (('and', 'the'), 0.001839489076741831),\n",
       " (('for', 'the'), 0.0015148226994329964),\n",
       " (('to', 'be'), 0.0014614292899021006),\n",
       " (('at', 'the'), 0.0012969431411859538),\n",
       " (('with', 'the'), 0.0012676628843464302),\n",
       " (('of', 'a'), 0.0012581898600748196)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finders\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "scored[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "How to get the document frequency of the bigrams???\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq = nltk.FreqDist(brown.words())\n",
    "bigram_freq = nltk.FreqDist('_'.join(x) for x in nltk.bigrams(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_per_file = [nltk.FreqDist(words) \n",
    "                         for words in [brown.words(fileids=f) for f in brown.fileids()]]\n",
    "bigram_freq_per_file = [nltk.FreqDist('_'.join(x) for x in nltk.bigrams(words))\n",
    "                         for words in [brown.words(fileids=f) for f in brown.fileids()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get unigram dispersion\n",
    "def createUnigramDipsersionDist(uni_freq, uni_freq_per_file):\n",
    "    len(uni_freq_per_file)\n",
    "    unigram_dispersion = {}\n",
    "\n",
    "    for fid in uni_freq_per_file:\n",
    "        for w, f in fid.items():\n",
    "            if w in unigram_dispersion:\n",
    "                unigram_dispersion[w] += 1\n",
    "            else:\n",
    "                unigram_dispersion[w] = 1\n",
    "    return(unigram_dispersion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 500),\n",
       " ('Fulton', 3),\n",
       " ('County', 45),\n",
       " ('Grand', 17),\n",
       " ('Jury', 4),\n",
       " ('said', 314),\n",
       " ('Friday', 34),\n",
       " ('an', 498),\n",
       " ('investigation', 34),\n",
       " ('of', 500),\n",
       " (\"Atlanta's\", 2),\n",
       " ('recent', 114),\n",
       " ('primary', 59),\n",
       " ('election', 28),\n",
       " ('produced', 66),\n",
       " ('``', 462),\n",
       " ('no', 455),\n",
       " ('evidence', 119),\n",
       " (\"''\", 463),\n",
       " ('that', 500)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_dispersion = createUnigramDipsersionDist(unigram_freq, unigram_freq_per_file)\n",
    "# Dictionary cannot be sliced/subset\n",
    "# Get the items() and convert to list for subsetting\n",
    "list(unigram_dispersion.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The_Fulton', 1),\n",
       " ('Fulton_County', 6),\n",
       " ('County_Grand', 1),\n",
       " ('Grand_Jury', 2),\n",
       " ('Jury_said', 1),\n",
       " ('said_Friday', 4),\n",
       " ('Friday_an', 1),\n",
       " ('an_investigation', 7),\n",
       " ('investigation_of', 15),\n",
       " (\"of_Atlanta's\", 1),\n",
       " (\"Atlanta's_recent\", 1),\n",
       " ('recent_primary', 1),\n",
       " ('primary_election', 2),\n",
       " ('election_produced', 1),\n",
       " ('produced_``', 1),\n",
       " ('``_no', 6),\n",
       " ('no_evidence', 14),\n",
       " (\"evidence_''\", 3),\n",
       " (\"''_that\", 16),\n",
       " ('that_any', 31)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dict(sorted(bigram_freq.items()[:3]))\n",
    "list(bigram_freq.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The_Fulton', 1),\n",
       " ('Fulton_County', 1),\n",
       " ('County_Grand', 1),\n",
       " ('Grand_Jury', 2),\n",
       " ('Jury_said', 1),\n",
       " ('said_Friday', 3),\n",
       " ('Friday_an', 1),\n",
       " ('an_investigation', 7),\n",
       " ('investigation_of', 14),\n",
       " (\"of_Atlanta's\", 1),\n",
       " (\"Atlanta's_recent\", 1),\n",
       " ('recent_primary', 1),\n",
       " ('primary_election', 2),\n",
       " ('election_produced', 1),\n",
       " ('produced_``', 1),\n",
       " ('``_no', 6),\n",
       " ('no_evidence', 12),\n",
       " (\"evidence_''\", 3),\n",
       " (\"''_that\", 16),\n",
       " ('that_any', 30)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_dispersion = createUnigramDipsersionDist(bigram_freq, bigram_freq_per_file)\n",
    "list(bigram_dispersion.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unigram_freq)\n",
    "type(unigram_dispersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 569 matches:\n",
      "will deliver tomorrow night to the American people over nationwide television \n",
      "ocial security taxes on 70 million American workers would be raised to pay the\n",
      "o retired as vice president of the American Screw Co. in 1955 said , `` Both p\n",
      "wice elected overwhelmingly by the American people as president of the United \n",
      "n example : Last month in Ghana an American missionary discovered when he came\n"
     ]
    }
   ],
   "source": [
    "## Simple Concordances\n",
    "brown_text.concordance('American', width=79, lines = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.app.concordance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' .\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Regular Expression Concordances\n",
    "import re\n",
    "sents = [' '.join(s) for s in brown.sents()]\n",
    "regex_1 = r'(is|was) \\w+ing'\n",
    "targets = [sent for sent in sents[:100] if re.search(regex_1, sent)]\n",
    "targets[0]\n",
    "#if targets:\n",
    "#    for match in targets:\n",
    "#        print(match.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## word frequencies\n",
    "brown_fd_words = nltk.FreqDist(brown.words())\n",
    "brown_fd_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fulton</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>county</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jury</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investigation</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atlanta's</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30585</th>\n",
       "      <td>bilharziasis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30586</th>\n",
       "      <td>perelman</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30587</th>\n",
       "      <td>aviary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30588</th>\n",
       "      <td>cherokee</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30589</th>\n",
       "      <td>boucle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30590 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  freq\n",
       "0             fulton    17\n",
       "1             county   155\n",
       "2               jury    67\n",
       "3      investigation    51\n",
       "4          atlanta's     4\n",
       "...              ...   ...\n",
       "30585   bilharziasis     1\n",
       "30586       perelman     1\n",
       "30587         aviary     1\n",
       "30588       cherokee     1\n",
       "30589         boucle     1\n",
       "\n",
       "[30590 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## nouns freq\n",
    "brown_df_nouns = nltk.FreqDist([w.lower() for w,t in brown.tagged_words() \n",
    "                                 if any (noun_tag in t for noun_tag in ['NP','NN'])])\n",
    "brown_df_nouns.most_common(10)\n",
    "\n",
    "brown_df_nouns_df = pd.DataFrame(brown_df_nouns.items(), columns=['word','freq'])\n",
    "brown_df_nouns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               word freq\n",
      "663               a  116\n",
      "510              a.  112\n",
      "274             act  209\n",
      "301          action  291\n",
      "3565     activities  115\n",
      "1278       activity  116\n",
      "667        addition  142\n",
      "48   administration  161\n",
      "5114             af  995\n",
      "254       afternoon  106\n",
      "169             age  225\n",
      "1736      agreement  106\n",
      "321             aid  112\n",
      "1250            air  257\n",
      "179              aj  118\n",
      "1139        america  194\n",
      "1223         amount  142\n",
      "1012       analysis  108\n",
      "5644         answer  109\n",
      "1925       approach  108\n",
      "899            area  324\n",
      "257           areas  236\n",
      "997            arms  121\n",
      "685            army  132\n",
      "2531            art  202\n",
      "97      association  132\n",
      "757       attention  179\n",
      "6987       attitude  107\n",
      "202        audience  115\n",
      "5866              b  105\n",
      "1336           back  178\n",
      "2457           ball  110\n",
      "1224          basis  184\n",
      "2119            bed  127\n",
      "412            bill  143\n",
      "6866          blood  121\n",
      "335           board  234\n",
      "3455           body  276\n",
      "2830           book  197\n",
      "2118            boy  240\n",
      "2707           boys  143\n",
      "2510          brown  110\n",
      "810        building  106\n",
      "756        business  393\n",
      "3453              c  121\n",
      "1812            car  274\n",
      "992            cars  112\n",
      "701            case  360\n",
      "717           cases  148\n",
      "224            cent  155\n"
     ]
    }
   ],
   "source": [
    "%%R -i brown_df_nouns_df\n",
    "\n",
    "library(dplyr)\n",
    "brown_df_nouns_df %>%\n",
    "filter(freq > 100) %>%\n",
    "arrange(word) %>% \n",
    "head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Frequency List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NOUN': 5, 'VERB': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Word by POS Frequency Distribution\n",
    "\n",
    "brown_news_tagged_words = brown.tagged_words(categories='news', tagset='universal')\n",
    "brown_news_cfd = nltk.ConditionalFreqDist(brown_news_tagged_words)\n",
    "brown_news_cfd['yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 732),\n",
       " ('was', 717),\n",
       " ('be', 526),\n",
       " ('said', 402),\n",
       " ('will', 388),\n",
       " ('are', 328),\n",
       " ('has', 300),\n",
       " ('had', 279),\n",
       " ('have', 265),\n",
       " ('were', 252)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## POS by Word Frequency Distribution\n",
    "brown_news_cfd2 = nltk.ConditionalFreqDist([(t, w) for (w, t) in brown_news_tagged_words])\n",
    "brown_news_cfd2['VERB'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word by Genre Frequency Distribution\n",
    "brown_genre_cfd = nltk.ConditionalFreqDist(\n",
    "    (word, genre)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'belles_lettres': 6, 'fiction': 4, 'lore': 3, 'religion': 3, 'romance': 3, 'learned': 2, 'reviews': 2, 'adventure': 1, 'humor': 1, 'science_fiction': 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_genre_cfd.conditions()[:50]\n",
    "brown_genre_cfd['mysterious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('belles_lettres', 6), ('fiction', 4), ('lore', 3), ('religion', 3), ('romance', 3), ('learned', 2), ('reviews', 2), ('adventure', 1), ('humor', 1), ('science_fiction', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(brown_genre_cfd['mysterious'].items(),key=lambda x:x[1],reverse=True)) # with freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Genre by Word Frequency Distribution\n",
    "brown_genre_cdf2 = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Genre by Word Frequency Distribution\n",
    "brown_genre_cdf2 = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           the   of  and   to    a   in that   is  was  for \n",
      "adventure 3370 1322 1622 1309 1354  847  494   98  914  331 \n",
      "editorial 3508 1976 1302 1554 1095 1001  578  744  308  509 \n",
      "  fiction 3423 1419 1696 1489 1281  916  530  144 1082  392 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_n_word = [word for (word, freq) in brown_fd_words.most_common(20) if word[0].isalpha()]\n",
    "\n",
    "brown_genre_cdf2.tabulate(conditions=['adventure','editorial','fiction'],\n",
    "                         samples=top_n_word[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_word2 = [word for (word, tag) in brown.tagged_words(tagset='universal') \n",
    "               if tag.startswith('NOUN')]\n",
    "top_n_word2_fd = nltk.FreqDist(top_n_word2).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('time', 1555), ('man', 1148), ('Af', 994), ('years', 942), ('way', 883), ('Mr.', 844), ('people', 809), ('men', 736), ('world', 684), ('life', 676)]\n",
      "            time    man     Af  years    way    Mr. people    men  world   life \n",
      "adventure    127    165      0     32     65     22     24     81     15     29 \n",
      "editorial     72     56      0     63     43    110     75     38     66     35 \n",
      "  fiction     99    111      0     44     62     39     39     72     24     44 \n"
     ]
    }
   ],
   "source": [
    "print(top_n_word2_fd)\n",
    "brown_genre_cdf2.tabulate(conditions=['adventure','editorial','fiction'],\n",
    "                         samples=[w for (w, f) in top_n_word2_fd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckiptagger",
   "language": "python",
   "name": "ckiptagger"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
