{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Lingustics Methods\n",
    "\n",
    "- With `nltk`, we can easily implement quite a few corpus-linguistic methods\n",
    "    - Concordance Analysis (Simple Words)\n",
    "    - Frequency Lists\n",
    "    - Collocations\n",
    "    - Data Analysis with R\n",
    "    - Concordance Analysis (Patterns, Constructions?)\n",
    "        - Patterns on sentence strings\n",
    "        - Patterns on sentence word-tag strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Corpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.text import Text\n",
    "import pandas as pd\n",
    "\n",
    "brown_text = Text(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "- Documentation [nltk.collocations](https://www.nltk.org/howto/collocations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collocations based on Text\n",
    "brown_text.collocation_list()[:10]\n",
    "#brown_text.collocations()\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$10,000-per-year', 'French-born'),\n",
       " ('$79.89', 'nothing-down'),\n",
       " ('$8.50', 'tab'),\n",
       " (\"'low\", 'nigras'),\n",
       " ('0.5-mv./m.', '50-percent'),\n",
       " ('0.78', 'mEq'),\n",
       " ('1,100', 'circumscriptions'),\n",
       " ('1,257,700', 'non-farm'),\n",
       " ('11-inch', 'headroom'),\n",
       " ('11-shot', 'hammerless')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## bigram collocations based on different association measures\n",
    "finder.nbest(bigram_measures.likelihood_ratio,10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hong', 'Kong'),\n",
       " ('Viet', 'Nam'),\n",
       " ('Pathet', 'Lao'),\n",
       " ('Simms', 'Purdew'),\n",
       " ('El', 'Paso'),\n",
       " ('Lo', 'Shu'),\n",
       " ('Internal', 'Revenue'),\n",
       " ('Puerto', 'Rico'),\n",
       " ('Saxon', 'Shore'),\n",
       " ('carbon', 'tetrachloride')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply freq-based filers for bigram collocations\n",
    "finder.apply_freq_filter(10)\n",
    "\n",
    "## Apply word filer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_en = stopwords.words('english')\n",
    "finder.apply_word_filter(lambda x: not x.isalpha())\n",
    "\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('A', 'AT-NC'), ('charge', 'NN-NC')),\n",
       " (('A', 'FW-IN'), ('quibusdam', 'FW-WPO')),\n",
       " (('ACTH', 'NP'), ('Gel', 'NN-TL')),\n",
       " (('Abner', 'NP'), ('Haynes', 'NP')),\n",
       " (('Ace', 'NN-TL'), ('Driver', 'NN-TL')),\n",
       " (('Acoustical', 'JJ-HL'), ('interferometer', 'NN-HL')),\n",
       " (('Actively', 'RB-HL'), ('modernizing', 'VBG-HL')),\n",
       " (('Ad', 'FW-IN-TL'), ('Amicam', 'FW-NN-TL')),\n",
       " (('Adjusted', 'VBN-HL'), ('gross', 'JJ-HL')),\n",
       " (('Aerobacter', 'NP'), ('aerogenes', 'NP'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collocations based on tagged words\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    brown.tagged_words())\n",
    "finder.apply_word_filter(lambda x: not x[0].isalpha())\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X', 'X'),\n",
       " ('PRON', 'VERB'),\n",
       " ('PRT', 'VERB'),\n",
       " ('ADP', 'DET'),\n",
       " ('DET', 'ADJ'),\n",
       " ('ADJ', 'NOUN'),\n",
       " ('.', 'CONJ'),\n",
       " ('DET', 'NOUN'),\n",
       " ('VERB', 'PRT'),\n",
       " ('.', 'PRON')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collcoations based on tags only\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    t for w, t in brown.tagged_words(tagset='universal'))\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hong', 'Kong'),\n",
       " ('Viet', 'Nam'),\n",
       " ('Pathet', 'Lao'),\n",
       " ('Simms', 'Purdew'),\n",
       " ('El', 'Paso'),\n",
       " ('Lo', 'Shu'),\n",
       " ('Internal', 'Revenue'),\n",
       " ('Puerto', 'Rico'),\n",
       " ('Saxon', 'Shore'),\n",
       " ('carbon', 'tetrachloride')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collocations with intervneing words (gapped n-grams)\n",
    "finder = BigramCollocationFinder.from_words(brown.words(), window_size=2)\n",
    "finder.apply_word_filter(lambda x: not x.isalpha())\n",
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 0.008288896237659233),\n",
       " (('in', 'the'), 0.004776126600941102),\n",
       " (('to', 'the'), 0.002950416468594341),\n",
       " (('on', 'the'), 0.0019781397047172215),\n",
       " (('and', 'the'), 0.001839489076741831),\n",
       " (('for', 'the'), 0.0015148226994329964),\n",
       " (('to', 'be'), 0.0014614292899021006),\n",
       " (('at', 'the'), 0.0012969431411859538),\n",
       " (('with', 'the'), 0.0012676628843464302),\n",
       " (('of', 'a'), 0.0012581898600748196)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finders\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "scored[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "How to get the document frequency of the bigrams???\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq = nltk.FreqDist(brown.words())\n",
    "bigram_freq = nltk.FreqDist('_'.join(x) for x in nltk.bigrams(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_per_file = [nltk.FreqDist(words) \n",
    "                         for words in [brown.words(fileids=f) for f in brown.fileids()]]\n",
    "bigram_freq_per_file = [nltk.FreqDist('_'.join(x) for x in nltk.bigrams(words))\n",
    "                         for words in [brown.words(fileids=f) for f in brown.fileids()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get unigram dispersion\n",
    "def createUnigramDipsersionDist(uni_freq, uni_freq_per_file):\n",
    "    len(uni_freq_per_file)\n",
    "    unigram_dispersion = {}\n",
    "\n",
    "    for fid in uni_freq_per_file:\n",
    "        for w, f in fid.items():\n",
    "            if w in unigram_dispersion:\n",
    "                unigram_dispersion[w] += 1\n",
    "            else:\n",
    "                unigram_dispersion[w] = 1\n",
    "    return(unigram_dispersion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 500),\n",
       " ('Fulton', 3),\n",
       " ('County', 45),\n",
       " ('Grand', 17),\n",
       " ('Jury', 4),\n",
       " ('said', 314),\n",
       " ('Friday', 34),\n",
       " ('an', 498),\n",
       " ('investigation', 34),\n",
       " ('of', 500),\n",
       " (\"Atlanta's\", 2),\n",
       " ('recent', 114),\n",
       " ('primary', 59),\n",
       " ('election', 28),\n",
       " ('produced', 66),\n",
       " ('``', 462),\n",
       " ('no', 455),\n",
       " ('evidence', 119),\n",
       " (\"''\", 463),\n",
       " ('that', 500)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_dispersion = createUnigramDipsersionDist(unigram_freq, unigram_freq_per_file)\n",
    "# Dictionary cannot be sliced/subset\n",
    "# Get the items() and convert to list for subsetting\n",
    "list(unigram_dispersion.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The_Fulton', 1),\n",
       " ('Fulton_County', 6),\n",
       " ('County_Grand', 1),\n",
       " ('Grand_Jury', 2),\n",
       " ('Jury_said', 1),\n",
       " ('said_Friday', 4),\n",
       " ('Friday_an', 1),\n",
       " ('an_investigation', 7),\n",
       " ('investigation_of', 15),\n",
       " (\"of_Atlanta's\", 1),\n",
       " (\"Atlanta's_recent\", 1),\n",
       " ('recent_primary', 1),\n",
       " ('primary_election', 2),\n",
       " ('election_produced', 1),\n",
       " ('produced_``', 1),\n",
       " ('``_no', 6),\n",
       " ('no_evidence', 14),\n",
       " (\"evidence_''\", 3),\n",
       " (\"''_that\", 16),\n",
       " ('that_any', 31)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dict(sorted(bigram_freq.items()[:3]))\n",
    "list(bigram_freq.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The_Fulton', 1),\n",
       " ('Fulton_County', 1),\n",
       " ('County_Grand', 1),\n",
       " ('Grand_Jury', 2),\n",
       " ('Jury_said', 1),\n",
       " ('said_Friday', 3),\n",
       " ('Friday_an', 1),\n",
       " ('an_investigation', 7),\n",
       " ('investigation_of', 14),\n",
       " (\"of_Atlanta's\", 1),\n",
       " (\"Atlanta's_recent\", 1),\n",
       " ('recent_primary', 1),\n",
       " ('primary_election', 2),\n",
       " ('election_produced', 1),\n",
       " ('produced_``', 1),\n",
       " ('``_no', 6),\n",
       " ('no_evidence', 12),\n",
       " (\"evidence_''\", 3),\n",
       " (\"''_that\", 16),\n",
       " ('that_any', 30)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_dispersion = createUnigramDipsersionDist(bigram_freq, bigram_freq_per_file)\n",
    "list(bigram_dispersion.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unigram_freq)\n",
    "type(unigram_dispersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 569 matches:\n",
      "will deliver tomorrow night to the American people over nationwide television \n",
      "ocial security taxes on 70 million American workers would be raised to pay the\n",
      "o retired as vice president of the American Screw Co. in 1955 said , `` Both p\n",
      "wice elected overwhelmingly by the American people as president of the United \n",
      "n example : Last month in Ghana an American missionary discovered when he came\n"
     ]
    }
   ],
   "source": [
    "## Simple Concordances\n",
    "brown_text.concordance('American', width=79, lines = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regular Expression Concordances\n",
    "import re\n",
    "sents = [' '.join(s) for s in brown.sents()]\n",
    "regex_1 = r'(is|was) \\w+ing'\n",
    "targets = [sent for sent in sents[:100] if re.search(regex_1, sent)]\n",
    "targets[0]\n",
    "#if targets:\n",
    "#    for match in targets:\n",
    "#        print(match.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word frequencies\n",
    "brown_fd_words = nltk.FreqDist(brown.words())\n",
    "brown_fd_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fulton</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>county</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jury</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investigation</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atlanta's</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30585</th>\n",
       "      <td>bilharziasis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30586</th>\n",
       "      <td>perelman</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30587</th>\n",
       "      <td>aviary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30588</th>\n",
       "      <td>cherokee</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30589</th>\n",
       "      <td>boucle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30590 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  freq\n",
       "0             fulton    17\n",
       "1             county   155\n",
       "2               jury    67\n",
       "3      investigation    51\n",
       "4          atlanta's     4\n",
       "...              ...   ...\n",
       "30585   bilharziasis     1\n",
       "30586       perelman     1\n",
       "30587         aviary     1\n",
       "30588       cherokee     1\n",
       "30589         boucle     1\n",
       "\n",
       "[30590 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## nouns freq\n",
    "brown_df_nouns = nltk.FreqDist([w.lower() for w,t in brown.tagged_words() \n",
    "                                 if any (noun_tag in t for noun_tag in ['NP','NN'])])\n",
    "brown_df_nouns.most_common(10)\n",
    "\n",
    "brown_df_nouns_df = pd.DataFrame(brown_df_nouns.items(), columns=['word','freq'])\n",
    "brown_df_nouns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                word freq\n",
      "663                a  116\n",
      "510               a.  112\n",
      "274              act  209\n",
      "301           action  291\n",
      "3565      activities  115\n",
      "1278        activity  116\n",
      "667         addition  142\n",
      "48    administration  161\n",
      "5114              af  995\n",
      "254        afternoon  106\n",
      "169              age  225\n",
      "1736       agreement  106\n",
      "321              aid  112\n",
      "1250             air  257\n",
      "179               aj  118\n",
      "1139         america  194\n",
      "1223          amount  142\n",
      "1012        analysis  108\n",
      "5644          answer  109\n",
      "1925        approach  108\n",
      "899             area  324\n",
      "257            areas  236\n",
      "997             arms  121\n",
      "685             army  132\n",
      "2531             art  202\n",
      "97       association  132\n",
      "757        attention  179\n",
      "6987        attitude  107\n",
      "202         audience  115\n",
      "5866               b  105\n",
      "1336            back  178\n",
      "2457            ball  110\n",
      "1224           basis  184\n",
      "2119             bed  127\n",
      "412             bill  143\n",
      "6866           blood  121\n",
      "335            board  234\n",
      "3455            body  276\n",
      "2830            book  197\n",
      "2118             boy  240\n",
      "2707            boys  143\n",
      "2510           brown  110\n",
      "810         building  106\n",
      "756         business  393\n",
      "3453               c  121\n",
      "1812             car  274\n",
      "992             cars  112\n",
      "701             case  360\n",
      "717            cases  148\n",
      "224             cent  155\n",
      "1814          center  212\n",
      "2182         century  207\n",
      "1293          chance  129\n",
      "2317          change  163\n",
      "959          changes  121\n",
      "6753       character  118\n",
      "14            charge  107\n",
      "67             child  213\n",
      "459         children  355\n",
      "588           choice  108\n",
      "356           church  348\n",
      "497           cities  107\n",
      "12              city  393\n",
      "3452           class  206\n",
      "1310            club  145\n",
      "634          college  267\n",
      "720            color  135\n",
      "557       commission  103\n",
      "13         committee  168\n",
      "718        community  231\n",
      "1768         company  290\n",
      "1140      conditions  180\n",
      "334         congress  152\n",
      "1091         control  195\n",
      "1528          corner  115\n",
      "684            corps  110\n",
      "47              cost  185\n",
      "104            costs  164\n",
      "185          council  103\n",
      "1064       countries  151\n",
      "1114         country  324\n",
      "1             county  155\n",
      "153           couple  122\n",
      "1801          course  465\n",
      "21             court  228\n",
      "5041            data  173\n",
      "303              day  687\n",
      "371             days  384\n",
      "369            death  277\n",
      "1727        decision  119\n",
      "738          defense  167\n",
      "642           degree  125\n",
      "49        department  225\n",
      "4585          design  110\n",
      "595      development  333\n",
      "1349      difference  148\n",
      "1124       direction  134\n",
      "235         director  101\n",
      "1054        distance  108\n",
      "506         district  135\n",
      "1510        division  107\n",
      "4208            door  312\n",
      "650              dr.  192\n",
      "5468           earth  150\n",
      "322        education  214\n",
      "102           effect  197\n",
      "4098         effects  108\n",
      "596           effort  145\n",
      "1138         efforts  127\n",
      "908         elements  107\n",
      "39               end  369\n",
      "2315         england  154\n",
      "1231       equipment  167\n",
      "3617          europe  118\n",
      "3517         evening  133\n",
      "1030          events  101\n",
      "7           evidence  203\n",
      "1688         example  292\n",
      "4887       existence  107\n",
      "1497      experience  258\n",
      "7494          extent  110\n",
      "1267             eye  122\n",
      "1869            eyes  401\n",
      "2278            face  320\n",
      "101             fact  447\n",
      "1762         factors  105\n",
      "2206           faith  111\n",
      "704           family  331\n",
      "2524            farm  121\n",
      "2171          father  182\n",
      "1621         feeling  131\n",
      "1532            feet  283\n",
      "903            field  272\n",
      "2106          figure  189\n",
      "856          figures  110\n",
      "84              fire  176\n",
      "927            floor  158\n",
      "4078            food  147\n",
      "1829           force  206\n",
      "1065          forces  169\n",
      "1260            form  319\n",
      "6229           forms  122\n",
      "885          freedom  128\n",
      "3371          friend  133\n",
      "1852         friends  161\n",
      "2123           front  145\n",
      "825         function  107\n",
      "78            future  138\n",
      "2226            game  122\n",
      "564          general  123\n",
      "366           george  129\n",
      "3621            girl  220\n",
      "4749           girls  142\n",
      "2275             god  314\n",
      "896       government  418\n",
      "1078          ground  169\n",
      "525            group  386\n",
      "1132          groups  125\n",
      "1192          growth  155\n",
      "376              gun  117\n",
      "7495            hair  148\n",
      "3308            hall  152\n",
      "1343            hand  423\n",
      "1910           hands  289\n",
      "1217            head  407\n",
      "141           health  105\n",
      "998            heart  173\n",
      "631          history  286\n",
      "765             home  238\n",
      "3246            hope  109\n",
      "475            horse  117\n",
      "140         hospital  110\n",
      "1313           hotel  126\n",
      "938             hour  145\n",
      "633            hours  174\n",
      "299            house  582\n",
      "3405         husband  131\n",
      "578             idea  195\n",
      "1184           ideas  143\n",
      "1462           image  119\n",
      "8630      importance  108\n",
      "1423          income  109\n",
      "304         increase  113\n",
      "1415        industry  171\n",
      "4882       influence  113\n",
      "586      information  269\n",
      "31          interest  327\n",
      "669           island  167\n",
      "263            issue  137\n",
      "159               j.  120\n",
      "204            james  101\n",
      "1226             job  238\n",
      "212             john  362\n",
      "565          justice  114\n",
      "755          kennedy  140\n",
      "568             kind  296\n",
      "909        knowledge  145\n",
      "1201           labor  145\n",
      "1550            land  209\n",
      "6843        language  109\n",
      "58               law  299\n",
      "1452         leaders  107\n",
      "2094          length  116\n",
      "1261          letter  145\n",
      "484          letters  115\n",
      "1256           level  196\n",
      "1438            life  715\n",
      "2313           light  260\n",
      "1000            line  294\n",
      "911            lines  197\n",
      "1288            list  126\n",
      "6831      literature  133\n",
      "2761             lot  126\n",
      "8727            love  174\n",
      "4404         machine  103\n",
      "174              man 1203\n",
      "5051           man's  125\n",
      "18            manner  124\n",
      "370            march  111\n",
      "4492          market  155\n",
      "5972        material  170\n",
      "795           matter  283\n",
      "1360         meaning  115\n",
      "404            means  205\n",
      "199          meeting  125\n",
      "1320          member  137\n",
      "328          members  325\n",
      "1227             men  763\n",
      "1063          method  142\n",
      "628          methods  142\n",
      "1782           miles  173\n",
      "1025            mind  287\n",
      "1424         minutes  196\n",
      "1262            miss  233\n",
      "5055          moment  246\n",
      "75             money  265\n",
      "1689           month  130\n",
      "1031          months  189\n",
      "1963         morning  210\n",
      "3503          mother  215\n",
      "1534           mouth  103\n",
      "1354        movement  128\n",
      "1011             mr.  844\n",
      "158             mrs.  534\n",
      "3810           music  216\n",
      "1443            name  273\n",
      "1195          nation  139\n",
      "1161         nations  175\n",
      "956           nature  191\n",
      "559             need  160\n",
      "931            negro  107\n",
      "2480            news  102\n",
      "121            night  411\n",
      "32            number  470\n",
      "6366         numbers  124\n",
      "134           office  255\n",
      "1723         officer  101\n",
      "287             ones  116\n",
      "130        operation  113\n",
      "1577     opportunity  121\n",
      "354            order  359\n",
      "951     organization  128\n",
      "750           others  323\n",
      "1475           paper  156\n",
      "865             part  496\n",
      "857            parts  113\n",
      "203            party  215\n",
      "5791         pattern  112\n",
      "1675           peace  198\n",
      "486           people  845\n",
      "1587     performance  122\n",
      "1153          period  265\n",
      "640           person  174\n",
      "411          persons  121\n",
      "1884         picture  160\n",
      "6051           piece  128\n",
      "9              place  496\n",
      "128             plan  175\n",
      "2573           plane  113\n",
      "294            plans  102\n",
      "1422           plant  121\n",
      "1246           point  369\n",
      "1014          points  124\n",
      "131           police  155\n",
      "981           policy  222\n",
      "6252            pool  108\n",
      "458       population  136\n",
      "1437        position  237\n",
      "1108           power  340\n",
      "656        president  382\n",
      "924         pressure  184\n",
      "382            price  107\n",
      "7035       principle  108\n",
      "54           problem  313\n",
      "714         problems  247\n",
      "823          process  195\n",
      "5715      production  148\n",
      "2187        products  108\n",
      "72           program  388\n",
      "837         programs  139\n",
      "1668        progress  116\n",
      "164         property  156\n",
      "1251          public  135\n",
      "113          purpose  149\n",
      "4501         quality  114\n",
      "487         question  240\n",
      "397        questions  138\n",
      "762            radio  119\n",
      "700            range  145\n",
      "1166            rate  205\n",
      "910         reaction  124\n",
      "2185          reason  238\n",
      "1634         reasons  102\n",
      "928           record  126\n",
      "234        relations  102\n",
      "8041        religion  119\n",
      "495           report  135\n",
      "307         research  171\n",
      "3391         respect  120\n",
      "1219  responsibility  118\n",
      "4425            rest  138\n",
      "51            result  197\n",
      "1482         results  132\n",
      "529           return  104\n",
      "668            rhode  105\n",
      "547            right  127\n",
      "5037           river  165\n",
      "285             road  197\n",
      "1715            role  104\n",
      "819             room  383\n",
      "1694              s.  114\n",
      "275            sales  133\n",
      "1845           scene  106\n",
      "340           school  489\n",
      "453          schools  195\n",
      "662          science  131\n",
      "2640          season  105\n",
      "785        secretary  191\n",
      "1240         section  189\n",
      "1206           sense  301\n",
      "536           series  130\n",
      "871          service  311\n",
      "68          services  138\n",
      "1001            side  375\n",
      "1107       situation  196\n",
      "34              size  136\n",
      "2176         society  237\n",
      "155              son  165\n",
      "1913            sort  157\n",
      "8781           sound  128\n",
      "1020          soviet  129\n",
      "1556           space  183\n",
      "1181          spirit  182\n",
      "1056          spring  121\n",
      "1806          square  107\n",
      "826              st.  164\n",
      "787            staff  111\n",
      "1681           stage  173\n",
      "63             state  787\n",
      "830        statement  141\n",
      "875           states  586\n",
      "1238         station  105\n",
      "958             step  110\n",
      "53             steps  118\n",
      "3757           stock  145\n",
      "5857           story  153\n",
      "1793          street  244\n",
      "1717        strength  137\n",
      "889          student  131\n",
      "455         students  213\n",
      "4069         studies  102\n",
      "256            study  203\n",
      "637          subject  121\n",
      "764           summer  134\n",
      "2605             sun  111\n",
      "327          support  126\n",
      "9147         surface  193\n",
      "229           system  416\n",
      "858          systems  129\n",
      "1077           table  197\n",
      "132              tax  196\n",
      "7386           teeth  103\n",
      "12473    temperature  135\n",
      "1559           terms  160\n",
      "272             test  101\n",
      "7190          theory  129\n",
      "921            thing  333\n",
      "1254          things  368\n",
      "9345         thought  103\n",
      "243             time 1597\n",
      "1282           times  296\n",
      "2357             top  129\n",
      "2188           total  131\n",
      "1340            town  212\n",
      "1137           trade  130\n",
      "2774       treatment  127\n",
      "3644           trees  101\n",
      "732            trial  134\n",
      "360          trouble  130\n",
      "1535           truth  126\n",
      "1314            type  198\n",
      "1283           types  116\n",
      "573             u.s.  162\n",
      "1041           union  182\n",
      "228             unit  103\n",
      "659       university  214\n",
      "1061             use  361\n",
      "1380           value  199\n",
      "11831         values  183\n",
      "1583            view  168\n",
      "2204           voice  226\n",
      "716           volume  135\n",
      "2120            wall  158\n",
      "1173             war  463\n",
      "330       washington  206\n",
      "590            water  444\n",
      "779              way  899\n",
      "1429            ways  128\n",
      "396             week  275\n",
      "977            weeks  141\n",
      "148             wife  228\n",
      "7316            will  108\n",
      "143          william  148\n",
      "7575          window  119\n",
      "2642           woman  223\n",
      "1392           women  195\n",
      "313             word  274\n",
      "3383           words  274\n",
      "278             work  583\n",
      "1099           world  787\n",
      "175             year  656\n",
      "248            years  949\n",
      "698             york  302\n"
     ]
    }
   ],
   "source": [
    "%%R -i brown_df_nouns_df\n",
    "\n",
    "library(dplyr)\n",
    "brown_df_nouns_df %>%\n",
    "filter(freq > 100) %>%\n",
    "arrange(word) %>% \n",
    "head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckiptagger",
   "language": "python",
   "name": "ckiptagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
