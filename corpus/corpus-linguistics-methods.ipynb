{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Lingustics Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With `nltk`, we can easily implement quite a few corpus-linguistic methods\n",
    "    - Concordance Analysis (Simple Word Search)\n",
    "    - Frequency Lists\n",
    "    - Collocations\n",
    "    - Data Analysis with R\n",
    "    - Concordance Analysis (Patterns, Constructions?)\n",
    "        - Patterns on sentence strings\n",
    "        - Patterns on sentence word-tag strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Corpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.text import Text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "brown_text = Text(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "- Documentation [nltk.collocations](https://www.nltk.org/howto/collocations.html)\n",
    "- `nltk.collocations`: Get the `BigramCollocationFinder` which we can use to find n-grams\n",
    "- `nltk.metrics`: Get the `BigramAssocMeasures` to define collocations (It's also available in `nltk.collocations`)\n",
    "- Use `finder.nbest()` methods to select/filter collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collocations based on Text\n",
    "brown_text.collocation_list()[:10]\n",
    "#brown_text.collocations()\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures() # measures\n",
    "finder = BigramCollocationFinder.from_words(brown.words()) # finders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$10,000-per-year', 'French-born'),\n",
       " ('$79.89', 'nothing-down'),\n",
       " ('$8.50', 'tab'),\n",
       " (\"'low\", 'nigras'),\n",
       " ('0.5-mv./m.', '50-percent'),\n",
       " ('0.78', 'mEq'),\n",
       " ('1,100', 'circumscriptions'),\n",
       " ('1,257,700', 'non-farm'),\n",
       " ('11-inch', 'headroom'),\n",
       " ('11-shot', 'hammerless')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## bigram collocations based on different association measures\n",
    "finder.nbest(bigram_measures.likelihood_ratio,10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an anonymous function as a helper to remove irrelevant word tokens before collocation computation.\n",
    "\n",
    "For example, we remove:\n",
    "\n",
    "- word tokens whose char length < 3\n",
    "- word tokens that belong to the stopwords\n",
    "- word tokens that include at least one non-alphabetic char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hong', 'Kong'),\n",
       " ('Viet', 'Nam'),\n",
       " ('Pathet', 'Lao'),\n",
       " ('Simms', 'Purdew'),\n",
       " ('Internal', 'Revenue'),\n",
       " ('Puerto', 'Rico'),\n",
       " ('Saxon', 'Shore'),\n",
       " ('carbon', 'tetrachloride'),\n",
       " ('unwed', 'mothers'),\n",
       " ('Armed', 'Forces')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply freq-based filers for bigram collocations\n",
    "finder.apply_freq_filter(10)\n",
    "\n",
    "## Apply word filer function\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_en = stopwords.words('english')\n",
    "\n",
    "\n",
    "filter_stops = lambda w: len(w)<3 or w in stop_words_en or not w.isalpha()\n",
    "\n",
    "\n",
    "finder.apply_word_filter(filter_stops) # filter on word tokens\n",
    "finder.apply_freq_filter(10) # filter on bigram min frequencies \n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ADP', 'DET'),\n",
       " ('DET', 'NOUN'),\n",
       " ('PRON', 'VERB'),\n",
       " ('ADJ', 'NOUN'),\n",
       " ('NOUN', '.'),\n",
       " ('NOUN', 'DET'),\n",
       " ('DET', 'ADJ'),\n",
       " ('NOUN', 'ADP'),\n",
       " ('PRT', 'VERB'),\n",
       " ('ADP', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collcoations based on tags only\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    t for w, t in brown.tagged_words(tagset='universal') if t != 'X')\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations based on Skipped Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('United', 'States'),\n",
       " ('New', 'York'),\n",
       " ('per', 'cent'),\n",
       " ('Rhode', 'Island'),\n",
       " ('years', 'ago'),\n",
       " ('Los', 'Angeles'),\n",
       " ('White', 'House'),\n",
       " ('Peace', 'Corps'),\n",
       " ('World', 'War'),\n",
       " ('San', 'Francisco')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collocations with intervneing words (gapped n-grams)\n",
    "finder = BigramCollocationFinder.from_words(brown.words(), window_size=2)\n",
    "finder.apply_word_filter(filter_stops)\n",
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('United', 'States'), 0.0003375841376792124),\n",
       " (('New', 'York'), 0.00025491047130879306),\n",
       " (('per', 'cent'), 0.00012573286760501277),\n",
       " (('years', 'ago'), 0.0001171210273580941),\n",
       " (('The', 'first'), 8.267366637041936e-05),\n",
       " (('Rhode', 'Island'), 7.750656222226816e-05),\n",
       " (('could', 'see'), 7.492301014819255e-05),\n",
       " (('last', 'year'), 5.856051367904705e-05),\n",
       " (('first', 'time'), 5.769932965435518e-05),\n",
       " (('White', 'House'), 5.5976961604971446e-05)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finders\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "scored[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hong', 'Kong')\n",
      "('Viet', 'Nam')\n",
      "('Pathet', 'Lao')\n",
      "('Simms', 'Purdew')\n",
      "('Internal', 'Revenue')\n",
      "('Puerto', 'Rico')\n",
      "('Saxon', 'Shore')\n",
      "('carbon', 'tetrachloride')\n",
      "('unwed', 'mothers')\n"
     ]
    }
   ],
   "source": [
    "scored = finder.above_score(bigram_measures.pmi, min_score = 15)\n",
    "for s in scored:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion\n",
    "\n",
    "- Dispersion of a linguistic unit is also important.\n",
    "- There should be a metric that indicates how evenly distributed the linguistic unit is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "How to get the document frequency of the bigrams???\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq = nltk.FreqDist(brown.words())\n",
    "bigram_freq = nltk.FreqDist('_'.join(x) for x in nltk.bigrams(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram freq list of each file in the corpus\n",
    "unigram_freq_per_file = [nltk.FreqDist(words) \n",
    "                         for words in [brown.words(fileids=f) for f in brown.fileids()]]\n",
    "bigram_freq_per_file = [nltk.FreqDist('_'.join(x) for x in nltk.bigrams(words))\n",
    "                         for words in [brown.words(fileids=f) for f in brown.fileids()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get unigram dispersion\n",
    "def createDipsersionDist(uni_freq, uni_freq_per_file):\n",
    "    len(uni_freq_per_file)\n",
    "    unigram_dispersion = {}\n",
    "\n",
    "    for fid in uni_freq_per_file:\n",
    "        for w, f in fid.items():\n",
    "            if w in unigram_dispersion:\n",
    "                unigram_dispersion[w] += 1\n",
    "            else:\n",
    "                unigram_dispersion[w] = 1\n",
    "    return(unigram_dispersion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 500),\n",
       " ('Fulton', 3),\n",
       " ('County', 45),\n",
       " ('Grand', 17),\n",
       " ('Jury', 4),\n",
       " ('said', 314),\n",
       " ('Friday', 34),\n",
       " ('an', 498),\n",
       " ('investigation', 34),\n",
       " ('of', 500),\n",
       " (\"Atlanta's\", 2),\n",
       " ('recent', 114),\n",
       " ('primary', 59),\n",
       " ('election', 28),\n",
       " ('produced', 66),\n",
       " ('``', 462),\n",
       " ('no', 455),\n",
       " ('evidence', 119),\n",
       " (\"''\", 463),\n",
       " ('that', 500)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_dispersion = createDipsersionDist(unigram_freq, unigram_freq_per_file)\n",
    "# Dictionary cannot be sliced/subset\n",
    "# Get the items() and convert to list for subsetting\n",
    "list(unigram_dispersion.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The_Fulton', 1),\n",
       " ('Fulton_County', 6),\n",
       " ('County_Grand', 1),\n",
       " ('Grand_Jury', 2),\n",
       " ('Jury_said', 1),\n",
       " ('said_Friday', 4),\n",
       " ('Friday_an', 1),\n",
       " ('an_investigation', 7),\n",
       " ('investigation_of', 15),\n",
       " (\"of_Atlanta's\", 1),\n",
       " (\"Atlanta's_recent\", 1),\n",
       " ('recent_primary', 1),\n",
       " ('primary_election', 2),\n",
       " ('election_produced', 1),\n",
       " ('produced_``', 1),\n",
       " ('``_no', 6),\n",
       " ('no_evidence', 14),\n",
       " (\"evidence_''\", 3),\n",
       " (\"''_that\", 16),\n",
       " ('that_any', 31)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dict(sorted(bigram_freq.items()[:3]))\n",
    "list(bigram_freq.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The_Fulton', 1),\n",
       " ('Fulton_County', 1),\n",
       " ('County_Grand', 1),\n",
       " ('Grand_Jury', 2),\n",
       " ('Jury_said', 1),\n",
       " ('said_Friday', 3),\n",
       " ('Friday_an', 1),\n",
       " ('an_investigation', 7),\n",
       " ('investigation_of', 14),\n",
       " (\"of_Atlanta's\", 1),\n",
       " (\"Atlanta's_recent\", 1),\n",
       " ('recent_primary', 1),\n",
       " ('primary_election', 2),\n",
       " ('election_produced', 1),\n",
       " ('produced_``', 1),\n",
       " ('``_no', 6),\n",
       " ('no_evidence', 12),\n",
       " (\"evidence_''\", 3),\n",
       " (\"''_that\", 16),\n",
       " ('that_any', 30)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_dispersion = createDipsersionDist(bigram_freq, bigram_freq_per_file)\n",
    "list(bigram_dispersion.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unigram_freq)\n",
    "type(unigram_dispersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "We can implement the Delta P dispersion metric proposed by [Gries (2008)](https://www.researchgate.net/publication/233685362_Dispersions_and_adjusted_frequencies_in_corpora).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a directional association metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inherit BigramAssocMeasures\n",
    "class AugmentedBigramAssocMeasures(BigramAssocMeasures):\n",
    "    @classmethod\n",
    "    def raw_freq2(cls,*marginals):          \n",
    "        \"\"\"Scores ngrams by their frequency\"\"\"\n",
    "        n_ii, n_io, n_oi, n_oo = cls._contingency(*marginals)\n",
    "        return n_ii\n",
    "    \n",
    "    @classmethod\n",
    "    def dp_fwd(cls, *marginals):\n",
    "        \"\"\"Scores bigrams using DP forward\n",
    "        This may be shown with respect to a contingency table::\n",
    "\n",
    "                w1    ~w1\n",
    "             ------ ------\n",
    "         w2 | n_ii | n_oi | = n_xi\n",
    "             ------ ------\n",
    "        ~w2 | n_io | n_oo |\n",
    "             ------ ------\n",
    "             = n_ix        TOTAL = n_xx\n",
    "        \"\"\"\n",
    "        \n",
    "        n_ii, n_oi, n_io, n_oo = cls._contingency(*marginals)\n",
    "\n",
    "        return (n_ii/(n_ii+n_io)) - (n_oi/(n_oi+n_oo))\n",
    "\n",
    "    @classmethod\n",
    "    def dp_bwd(cls, *marginals):\n",
    "        \"\"\"Scores bigrams using DP backward\n",
    "        This may be shown with respect to a contingency table::\n",
    "\n",
    "                w1    ~w1\n",
    "             ------ ------\n",
    "         w2 | n_ii | n_oi | = n_xi\n",
    "             ------ ------\n",
    "        ~w2 | n_io | n_oo |\n",
    "             ------ ------\n",
    "             = n_ix        TOTAL = n_xx\n",
    "        \"\"\"\n",
    "        \n",
    "        n_ii, n_oi, n_io, n_oo = cls._contingency(*marginals)\n",
    "\n",
    "        return (n_ii/(n_ii+n_oi)) - (n_io/(n_io+n_oo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = AugmentedBigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('$10,000-per-year', 'French-born'), 1.0),\n",
       " (('$79.89', 'nothing-down'), 1.0),\n",
       " (('$8.50', 'tab'), 1.0),\n",
       " ((\"'low\", 'nigras'), 1.0),\n",
       " (('0.5-mv./m.', '50-percent'), 1.0),\n",
       " (('0.78', 'mEq'), 1.0),\n",
       " (('1,100', 'circumscriptions'), 1.0),\n",
       " (('1,257,700', 'non-farm'), 1.0),\n",
       " (('11-inch', 'headroom'), 1.0),\n",
       " (('11-shot', 'hammerless'), 1.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_dpfwd = finder.score_ngrams(bigram_measures.dp_fwd)\n",
    "bigrams_dpfwd[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('$10,000-per-year', 'French-born'), 1.0),\n",
       " (('$79.89', 'nothing-down'), 1.0),\n",
       " (('$8.50', 'tab'), 1.0),\n",
       " ((\"'low\", 'nigras'), 1.0),\n",
       " (('0.5-mv./m.', '50-percent'), 1.0),\n",
       " (('0.78', 'mEq'), 1.0),\n",
       " (('1,100', 'circumscriptions'), 1.0),\n",
       " (('1,257,700', 'non-farm'), 1.0),\n",
       " (('11-inch', 'headroom'), 1.0),\n",
       " (('11-shot', 'hammerless'), 1.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_dpbwd = finder.score_ngrams(bigram_measures.dp_bwd)\n",
    "bigrams_dpbwd[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Computation Accuracy\n",
    "\n",
    "- Check if DP is correctly computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_rawfreq = finder.score_ngrams(bigram_measures.raw_freq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 9625.0),\n",
       " ((',', 'and'), 6288.0),\n",
       " (('.', 'The'), 6081.0),\n",
       " (('in', 'the'), 5546.0),\n",
       " ((',', 'the'), 3754.0),\n",
       " (('.', '``'), 3515.0),\n",
       " (('to', 'the'), 3426.0),\n",
       " ((\"''\", '.'), 3332.0),\n",
       " ((';', ';'), 2784.0),\n",
       " (('.', 'He'), 2660.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_rawfreq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_rawfreq = nltk.FreqDist(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36080 62713 9625.0 1161192\n"
     ]
    }
   ],
   "source": [
    "w1f = unigrams_rawfreq['of']\n",
    "w2f = unigrams_rawfreq['the']\n",
    "w1w2 = [freq for (w1,w2),freq in bigrams_rawfreq if w1==\"of\" and w2==\"the\"][0]\n",
    "corpus_size = np.sum(list(unigrams_rawfreq.values()))\n",
    "\n",
    "\"\"\"\n",
    "        w1     _w1\n",
    "w2      w1w2   ____    w2f\n",
    "_w2     ____   ____\n",
    "        w1f            corpus_size\n",
    "\"\"\"\n",
    "\n",
    "print(w1f, w2f, w1w2,corpus_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta P Forward for `of the`: 0.2195836568422283\n",
      "Delta P Backward for `of the`: 0.12939364991590951\n"
     ]
    }
   ],
   "source": [
    "print('Delta P Forward for `of the`:', (w1w2/(w1f))-((w2f-w1w2)/(corpus_size-w1f)))\n",
    "print('Delta P Backward for `of the`:', (w1w2/(w2f))-((w1f-w1w2)/(corpus_size-w2f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2195836568422283]\n",
      "[0.12939364991590951]\n"
     ]
    }
   ],
   "source": [
    "print([dp for (w1, w2),dp in bigrams_dpfwd if w1==\"of\" and w2==\"the\"])\n",
    "print([dp for (w1, w2),dp in bigrams_dpbwd if w1==\"of\" and w2==\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "How to implement the delta P of trigrams?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit Trigram\n",
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "class AugmentedTrigramAssocMeasures(TrigramAssocMeasures):\n",
    "    \"\"\"\n",
    "    A collection of trigram association measures. Each association measure\n",
    "        is provided as a function with four arguments::\n",
    "\n",
    "            trigram_score_fn(n_iii,\n",
    "                             (n_iix, n_ixi, n_xii),\n",
    "                             (n_ixx, n_xix, n_xxi),\n",
    "                             n_xxx)\n",
    "\n",
    "        The arguments constitute the marginals of a contingency table, counting\n",
    "        the occurrences of particular events in a corpus. The letter i in the\n",
    "        suffix refers to the appearance of the word in question, while x indicates\n",
    "        the appearance of any word. Thus, for example:\n",
    "        n_iii counts (w1, w2, w3), i.e. the trigram being scored\n",
    "        n_ixx counts (w1, *, *)\n",
    "        n_xxx counts (*, *, *), i.e. any trigram\n",
    "    \"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def dp_fwd(cls, *marginals):\n",
    "        \"\"\"\n",
    "        Scores trigrams using delta P forward, i.e. conditional prob of w3 given w1,w2\n",
    "        minus conditional prob of w3, in the absence of w1,w2\n",
    "        \"\"\"\n",
    "        n_iii, n_oii, n_ioi, n_ooi, n_iio, n_oio, n_ioo, n_ooo = cls._contingency(*marginals)\n",
    "\n",
    "        return ((n_iii)/(n_iii+n_iio)) - ((n_ooi)/(n_ooi+n_ooo))\n",
    "    @classmethod\n",
    "    def dp_bwd(cls, *marginals):\n",
    "        \"\"\"\n",
    "        Scores trigrams using delta P backward, i.e. conditional prob of w1 given w2,w3\n",
    "        minus conditional prob of w1, in the absence of w2,w3\n",
    "        \"\"\"\n",
    "        n_iii, n_oii, n_ioi, n_ooi, n_iio, n_oio, n_ioo, n_ooo = cls._contingency(*marginals)\n",
    "\n",
    "        return ((n_iii)/(n_iii+n_oii)) - ((n_ioo)/(n_ioo+n_ooo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = AugmentedTrigramAssocMeasures()\n",
    "finder3 = TrigramCollocationFinder.from_words(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder3.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Drug's\", 'chemical', 'name'),\n",
       " ('Brown', '&', 'Sharpe'),\n",
       " ('B.', '&', 'O.'),\n",
       " ('per', 'capita', 'income'),\n",
       " ('John', 'A.', 'Notte'),\n",
       " ('average', 'per', 'capita'),\n",
       " ('General', 'Motors', 'stock'),\n",
       " ('basic', 'wage', 'rate'),\n",
       " ('World', 'War', '2'),\n",
       " ('New', 'York', 'Times')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder3.nbest(trigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'Lo', 'Shu'),\n",
       " ('average', 'per', 'capita'),\n",
       " ('of', 'Economic', 'Affairs'),\n",
       " ('the', 'minimal', 'polynomial'),\n",
       " ('B.', '&', 'O.'),\n",
       " ('the', 'Export-Import', 'Bank'),\n",
       " ('Chamber', 'of', 'Commerce'),\n",
       " ('Notte', ',', 'Jr.'),\n",
       " ('.', \"Drug's\", 'chemical'),\n",
       " ('v.', 'United', 'States')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder3.nbest(trigram_measures.dp_fwd, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Puerto', 'Rico', ','),\n",
       " ('Los', 'Angeles', ','),\n",
       " ('dominant', 'stress', 'will'),\n",
       " ('couple', 'of', 'weeks'),\n",
       " ('A.', 'Notte', ','),\n",
       " ('United', 'States', 'is'),\n",
       " ('Brown', '&', 'Sharpe'),\n",
       " ('Department', 'of', 'Economic'),\n",
       " ('boys', 'and', 'girls'),\n",
       " ('General', 'Motors', 'stock')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder3.nbest(trigram_measures.dp_bwd,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 569 matches:\n",
      "will deliver tomorrow night to the American people over nationwide television \n",
      "ocial security taxes on 70 million American workers would be raised to pay the\n",
      "o retired as vice president of the American Screw Co. in 1955 said , `` Both p\n",
      "wice elected overwhelmingly by the American people as president of the United \n",
      "n example : Last month in Ghana an American missionary discovered when he came\n"
     ]
    }
   ],
   "source": [
    "## Simple Concordances\n",
    "brown_text.concordance('American', width=79, lines = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.app.concordance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' .\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Regular Expression Concordances\n",
    "import re\n",
    "sents = [' '.join(s) for s in brown.sents()]\n",
    "regex_1 = r'(is|was) \\w+ing'\n",
    "targets = [sent for sent in sents[:100] if re.search(regex_1, sent)]\n",
    "targets[0]\n",
    "#if targets:\n",
    "#    for match in targets:\n",
    "#        print(match.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## word frequencies\n",
    "brown_fd_words = nltk.FreqDist(brown.words())\n",
    "brown_fd_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nouns freq\n",
    "brown_fd_nouns = nltk.FreqDist([w.lower() for w,t in brown.tagged_words() \n",
    "                                 if any (noun_tag in t for noun_tag in ['NP','NN'])])\n",
    "brown_fd_nouns.most_common(10)\n",
    "\n",
    "brown_fd_nouns_df = pd.DataFrame(brown_fd_nouns.items(), columns=['word','freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>time</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>man</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>af</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>years</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>way</td>\n",
       "      <td>899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>events</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>james</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>officer</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>test</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3644</th>\n",
       "      <td>trees</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  freq\n",
       "243      time  1597\n",
       "174       man  1203\n",
       "5114       af   995\n",
       "248     years   949\n",
       "779       way   899\n",
       "...       ...   ...\n",
       "1030   events   101\n",
       "204     james   101\n",
       "1723  officer   101\n",
       "272      test   101\n",
       "3644    trees   101\n",
       "\n",
       "[433 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_fd_nouns_df[brown_fd_nouns_df['freq']>100].sort_values([\"freq\",\"word\"],ascending=[False,True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "We can also pass the data frame to R for data exploration.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word freq\n",
      "243         time 1597\n",
      "174          man 1203\n",
      "5114          af  995\n",
      "248        years  949\n",
      "779          way  899\n",
      "486       people  845\n",
      "1011         mr.  844\n",
      "63         state  787\n",
      "1099       world  787\n",
      "1227         men  763\n",
      "1438        life  715\n",
      "303          day  687\n",
      "175         year  656\n",
      "875       states  586\n",
      "278         work  583\n",
      "299        house  582\n",
      "158         mrs.  534\n",
      "865         part  496\n",
      "9          place  496\n",
      "340       school  489\n",
      "32        number  470\n",
      "1801      course  465\n",
      "1173         war  463\n",
      "101         fact  447\n",
      "590        water  444\n",
      "1343        hand  423\n",
      "896   government  418\n",
      "229       system  416\n",
      "121        night  411\n",
      "1217        head  407\n",
      "1869        eyes  401\n",
      "756     business  393\n",
      "12          city  393\n",
      "72       program  388\n",
      "525        group  386\n",
      "371         days  384\n",
      "819         room  383\n",
      "656    president  382\n",
      "1001        side  375\n",
      "39           end  369\n",
      "1246       point  369\n",
      "1254      things  368\n",
      "212         john  362\n",
      "1061         use  361\n",
      "701         case  360\n",
      "354        order  359\n",
      "459     children  355\n",
      "356       church  348\n",
      "1108       power  340\n",
      "595  development  333\n"
     ]
    }
   ],
   "source": [
    "%%R -i brown_fd_nouns_df\n",
    "\n",
    "library(dplyr)\n",
    "brown_fd_nouns_df %>%\n",
    "filter(freq > 100) %>%\n",
    "arrange(desc(freq), word) %>% \n",
    "head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Frequency List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NOUN': 5, 'VERB': 1})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Word by POS Frequency Distribution\n",
    "\n",
    "brown_news_tagged_words = brown.tagged_words(categories='news', tagset='universal')\n",
    "brown_news_cfd = nltk.ConditionalFreqDist(brown_news_tagged_words)\n",
    "brown_news_cfd['yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 732),\n",
       " ('was', 717),\n",
       " ('be', 526),\n",
       " ('said', 402),\n",
       " ('will', 388),\n",
       " ('are', 328),\n",
       " ('has', 300),\n",
       " ('had', 279),\n",
       " ('have', 265),\n",
       " ('were', 252)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## POS by Word Frequency Distribution\n",
    "brown_news_cfd2 = nltk.ConditionalFreqDist([(t, w) for (w, t) in brown_news_tagged_words])\n",
    "brown_news_cfd2['VERB'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word by Genre Frequency Distribution\n",
    "brown_genre_cfd = nltk.ConditionalFreqDist(\n",
    "    (word, genre)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'belles_lettres': 6, 'fiction': 4, 'lore': 3, 'religion': 3, 'romance': 3, 'learned': 2, 'reviews': 2, 'adventure': 1, 'humor': 1, 'science_fiction': 1})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_genre_cfd.conditions()[:50]\n",
    "brown_genre_cfd['mysterious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('belles_lettres', 6), ('fiction', 4), ('lore', 3), ('religion', 3), ('romance', 3), ('learned', 2), ('reviews', 2), ('adventure', 1), ('humor', 1), ('science_fiction', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(brown_genre_cfd['mysterious'].items(),key=lambda x:x[1],reverse=True)) # with freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Genre by Word Frequency Distribution\n",
    "brown_genre_cdf2 = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Genre by Word Frequency Distribution\n",
    "brown_genre_cdf2 = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           the   of  and   to    a   in that   is  was  for \n",
      "adventure 3370 1322 1622 1309 1354  847  494   98  914  331 \n",
      "editorial 3508 1976 1302 1554 1095 1001  578  744  308  509 \n",
      "  fiction 3423 1419 1696 1489 1281  916  530  144 1082  392 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_n_word = [word for (word, freq) in brown_fd_words.most_common(20) if word[0].isalpha()]\n",
    "\n",
    "brown_genre_cdf2.tabulate(conditions=['adventure','editorial','fiction'],\n",
    "                         samples=top_n_word[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_word2 = [word for (word, tag) in brown.tagged_words(tagset='universal') \n",
    "               if tag.startswith('NOUN')]\n",
    "top_n_word2_fd = nltk.FreqDist(top_n_word2).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('time', 1555), ('man', 1148), ('Af', 994), ('years', 942), ('way', 883), ('Mr.', 844), ('people', 809), ('men', 736), ('world', 684), ('life', 676)]\n",
      "            time    man     Af  years    way    Mr. people    men  world   life \n",
      "adventure    127    165      0     32     65     22     24     81     15     29 \n",
      "editorial     72     56      0     63     43    110     75     38     66     35 \n",
      "  fiction     99    111      0     44     62     39     39     72     24     44 \n"
     ]
    }
   ],
   "source": [
    "print(top_n_word2_fd)\n",
    "brown_genre_cdf2.tabulate(conditions=['adventure','editorial','fiction'],\n",
    "                         samples=[w for (w, f) in top_n_word2_fd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
