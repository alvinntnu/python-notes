

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Network From Scratch &#8212; Python Notes for Linguistics</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Neural Language Model: A Start" href="neural-language-model-primer.html" />
    <link rel="prev" title="Sentiment Analysis with Traditional Machine Learning" href="sentiment-analysis-ml.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ntnu03.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Python Notes for Linguistics</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../python-basics/python-basics.html">
   Python Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../corpus/corpus-processing.html">
   Corpus Linguistics with Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../statistical-analyses/statistical-analyses.html">
   Statistical Analyses
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="nlp.html">
   Natural Language Processing with Python
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="nlp-spacy.html">
     Natural Language Processing (spaCy)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nlp-spacy-zh.html">
     Chinese Natural Language Processing (spaCy)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nlp-ckipnlp.html">
     Natural Language Processing (ckipnlp)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-normalization-eng.html">
     Text Normalization (English)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="text-normalization-chinese.html">
     Text Normalization (Chinese)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sklearn.html">
     Machine Learning with Sci-Kit Learn
    </a>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="naive-bayes.html">
       Naive Bayes
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="sentiment-analysis-ml.html">
       Sentiment Analysis with Traditional Machine Learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Neural Network From Scratch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neural-language-model-primer.html">
     Neural Language Model: A Start
    </a>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="neural-language-model-zh.html">
       Neural Language Model of Chinese
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="text-gen-lstm-v1.html">
       Text Generation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="word-embeddings.html">
     Word Embeddings
    </a>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="word2vec-chinese.html">
       Word Embeddings with Chinese Texts
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="word2vec.html">
       Word2Vec
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="doc2vec.html">
       Dov2Vec
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="transfer-learning-sent-encoding.html">
       Universal Sentence Embeddings
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sentiment-analysis-dl.html">
     Sentiment Analysis with Deep Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sentiment-analysis-lstm-v1.html">
     Sentiment Analysis with LSTM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="seq-to-seq-types.html">
     Intutions for Types of Sequence-to-Sequence Models
    </a>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="seq-to-seq-types-date.html">
       Types of Seqeunce Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="seq-to-seq-m21-sentiment-attention.html">
       Sequence Model (many-to-one) with Attention
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="seq-to-seq-attention-addition.html">
       Seqeunce Model with Attention for Addition Learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="seq-to-seq-machine-translation.html">
     Machine Translation (Sequence-to-Sequence)
    </a>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="seq-to-seq-machine-translation-attention.html">
       Machine Translation with Attention (Thushan)
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="hyperparameter-tuning.html">
     Hyper-Parameter Tuning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sentiment-analysis-using-bert-chinese.html">
     Sentiment Analysis Using BERT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ktrain-tutorial-explaining-predictions.html">
     Explainable AI
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/todo.html">
   To-do List
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div style="text-align:left">
    <a href="https://alvinchen.myftp.org/" target='_blank'>Alvin Chen's Homepage</a>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nlp/neural-network-from-scratch.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/alvinntnu/python-notes/master?urlpath=tree/nlp/neural-network-from-scratch.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/alvinntnu/python-notes/blob/master/nlp/neural-network-from-scratch.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#workflow-of-neural-network">
   Workflow of Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network">
   Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-a-neural-network-model">
   Building a Neural Network Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-algebra-and-matrix">
   Linear Algebra and Matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forward-propagation">
   Forward Propagation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#weights-biases-and-activation-functions">
   Weights, Biases, and Activation Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-and-training">
   Learning and Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-functions">
   Loss Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#error-based-on-one-sample">
     Error based on One Sample
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#errors-based-on-batch-samples">
     Errors based on Batch Samples
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient Descent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-derivative">
     Partial Derivative
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chain-rule-and-back-propagation">
     Chain Rule and Back Propagation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#math-on-derivatives">
   Math on Derivatives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#math-on-partial-derivaites">
   Math on Partial Derivaites
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#math-on-gradients">
   Math on Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sequence-model">
   Sequence Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neural-network-from-scratch">
<h1>Neural Network From Scratch<a class="headerlink" href="#neural-network-from-scratch" title="Permalink to this headline">¶</a></h1>
<p>This notebook provides an intuitive understanding of the mechanism of the neural network, or deep learning.</p>
<p>Important steps in neural network:</p>
<ul class="simple">
<li><p>forward propagation</p>
<ul>
<li><p>matrix multiplication</p></li>
<li><p>weights, biases, and activation functions</p></li>
</ul>
</li>
<li><p>back propagation</p>
<ul>
<li><p>derivatives and partial derivatives</p></li>
<li><p>chain rules</p></li>
</ul>
</li>
<li><p>gradient descent</p>
<ul>
<li><p>Batch</p></li>
<li><p>Mini-batch</p></li>
<li><p>Stochastic gradient descent</p></li>
</ul>
</li>
</ul>
<div class="section" id="workflow-of-neural-network">
<h2>Workflow of Neural Network<a class="headerlink" href="#workflow-of-neural-network" title="Permalink to this headline">¶</a></h2>
<img alt="Drawing" src="../_images/nn-flowchart.png" />
<ul class="simple">
<li><p>Three major variations of neural networks</p>
<ul>
<li><p>Multi-layer Perceptron</p></li>
<li><p>Convolutional Neural Network</p></li>
<li><p>Recurrent Neural Network</p></li>
</ul>
</li>
<li><p>Keras Model API for model building</p>
<ul>
<li><p>Sequential API</p></li>
<li><p>Functional API</p></li>
</ul>
</li>
<li><p>Core Modules API</p>
<ul>
<li><p>Activations</p></li>
<li><p>Optimizers</p></li>
<li><p>Losses</p></li>
<li><p>Metrics</p></li>
<li><p>Initializers</p></li>
<li><p>Regularizers</p></li>
<li><p>Dataset</p></li>
<li><p>Callback Modules</p></li>
<li><p>Visualizations</p></li>
<li><p>Utilites</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="neural-network">
<h2>Neural Network<a class="headerlink" href="#neural-network" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Neural network is a type of machine learning algorithm modeled on human brains and nervous system.</p></li>
<li><p>The model is believed to process information in a similar way to the human brain.</p></li>
<li><p>A neural network often consists of a large number of elements, known as <strong>nodes</strong>, working in parallel to solve a specific problem. These nodes are often organized into different <strong>layers</strong>.</p></li>
</ul>
<hr class="docutils" />
<p><img alt="" src="../_images/neural-network-sample.png" /></p>
</div>
<hr class="docutils" />
<div class="section" id="building-a-neural-network-model">
<h2>Building a Neural Network Model<a class="headerlink" href="#building-a-neural-network-model" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/neural-network-flowchart.png" /></p>
<ul class="simple">
<li><p>Forward Propagation: It refers to the process of transforming the data values by moving the data through the network to get output.</p></li>
<li><p>Calculate Total Error</p></li>
<li><p>Calculate Gradients</p></li>
<li><p>Update the weights</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="linear-algebra-and-matrix">
<h2>Linear Algebra and Matrix<a class="headerlink" href="#linear-algebra-and-matrix" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>2D matrix</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
5&amp;6 \\
\end{pmatrix}
\end{split}\]</div>
<ul class="simple">
<li><p>Matrix Multiplication</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
1&amp;2 \\
3&amp;4 \\
\end{pmatrix}
\begin{pmatrix}
5&amp;6 \\
7&amp;8
\end{pmatrix} =
\begin{pmatrix}
19&amp;22 \\
43&amp;50
\end{pmatrix}
\end{split}\]</div>
</div>
<div class="section" id="activation-functions">
<h2>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h2>
<p>In neural network, the activation function of a node determines whether the node would activate the output given the input values. Different types of activation functions may determine the cut-offs for output activation in different ways.</p>
<ul class="simple">
<li><p>sigmoid function</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ h(x) = \frac{1}{1 + \exp(-x)}\]</div>
<ul class="simple">
<li><p>step function</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} h(x)= \left\{ 
     \begin{array}\\
     0 &amp; (x \leq 0) \\
     1 &amp; (x &gt; 0)
     \end{array}
\right.
\end{split}\]</div>
<ul class="simple">
<li><p>ReLU function</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} h(x)= \left\{ 
     \begin{array}\\
     x &amp; (x &gt; 0) \\
     0 &amp; (x \leq 0)
     \end{array}
\right.
\end{split}\]</div>
<ul class="simple">
<li><p>Softmax function</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y_k = \frac{\exp(a_k)}{\sum_{i = 1}^{n} {a_i}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>


<span class="c1"># def softmax(x):</span>
<span class="c1">#     exp_x = np.exp(x)</span>
<span class="c1">#     sum_exp_x = np.sum(exp_x)</span>
<span class="c1">#     y = exp_x/sum_exp_x</span>
<span class="c1">#     return y</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">c</span><span class="p">)</span> <span class="c1"># avoid overflow issues</span>
    <span class="n">sum_exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">exp_x</span><span class="o">/</span><span class="n">sum_exp_x</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># step function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">step_function</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/neural-network-from-scratch_18_0.png" src="../_images/neural-network-from-scratch_18_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## sigmoid function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/neural-network-from-scratch_19_0.png" src="../_images/neural-network-from-scratch_19_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ReLU</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/neural-network-from-scratch_20_0.png" src="../_images/neural-network-from-scratch_20_0.png" />
</div>
</div>
</div>
<div class="section" id="forward-propagation">
<h2>Forward Propagation<a class="headerlink" href="#forward-propagation" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/neural-network-sample2.png" /></p>
<ul class="simple">
<li><p>Neural network is a model with weights for data/value transformation.</p></li>
<li><p>The input data values will be transformed according to the weights of the neural network.</p></li>
<li><p>Given a two-layer network, with two input values <span class="math notranslate nohighlight">\(x1\)</span> and <span class="math notranslate nohighlight">\(x2\)</span>, to get the values of the three outputs in the second layer, <span class="math notranslate nohighlight">\(a_1^{(1)}\)</span>, <span class="math notranslate nohighlight">\(a_2^{(1)}\)</span>, <span class="math notranslate nohighlight">\(a_3^{(1)}\)</span>, we compute the dot product of the <em>X</em> and <em>W</em>.</p>
<ul>
<li><p><em>X</em> refers to the input vector/matrix</p></li>
<li><p><em>W</em> refers to the network weights, which is a 2 x 3 matrix in the current example</p></li>
<li><p>The weights are represented as the links in-between the first and second layers</p></li>
<li><p>These weights can be mathematically represesnted as a 2 x 3 Matrix <em>W</em></p></li>
</ul>
</li>
<li><p>Taking the dot product of the input values <em>X</em> and the weight matrix <em>W</em> is referred to as the <strong>forward propagation</strong> of the network.</p></li>
<li><p>Forward propagation gives us the values of the nodes in the second layer</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">W</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2, 3)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 5 11 17]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="weights-biases-and-activation-functions">
<h2>Weights, Biases, and Activation Functions<a class="headerlink" href="#weights-biases-and-activation-functions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The output of a node in the network is computed as the sum of the weighted inputs and the bias. Take <span class="math notranslate nohighlight">\(a^{(1)}_1 \)</span> for example:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ a^{(1)}_1 = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1\]</div>
<ul class="simple">
<li><p>Then the output values go through the activation function and this result would indicate the final output of the node.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ z^{(1)}_1= h(a^{(1)}_1) \]</div>
<ul class="simple">
<li><p>Not all the nodes need to have an activation function.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
<span class="n">B1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2,)
(2, 3)
(3,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">B1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A1</span><span class="p">)</span>


<span class="n">Z1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">A1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>

<span class="n">Z2</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">A1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.3 0.7 1.1]
[0.57444252 0.66818777 0.75026011]
[0.21198272 0.31624106 0.47177622]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="learning-and-training">
<h2>Learning and Training<a class="headerlink" href="#learning-and-training" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Forward propagation shows how the network takes the input values, transforms them, and produces the output values based on the network parameters (i.e., weights).</p></li>
<li><p>The network needs to learn the weights that best produce the output values according to some loss function.</p></li>
<li><p>The key is we compute the differences between the real outputs of the network and the target outputs. The model should aim to minimize these differences, which are commonly referred to as <strong>errors</strong> of the model.</p></li>
</ul>
</div>
<div class="section" id="loss-functions">
<h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/neural-network-sample3.png" /></p>
<ul class="simple">
<li><p>If the target ouputs are numeric values, we can evaluate the errors (i.e., the differences between the actual model outputs and the target outputs) using the <strong>mean square error</strong> function.</p></li>
<li><p>If the target outputs are labels, we can evaluate the errors (i.e., the differences between the actual model labels and the target labels) using the <strong>cross entory error</strong> function.</p></li>
<li><p>The function used to compute the errors of the model is referred to as the <strong>loss function</strong>.</p></li>
</ul>
<div class="section" id="error-based-on-one-sample">
<h3>Error based on One Sample<a class="headerlink" href="#error-based-on-one-sample" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Mean Square Error</p></li>
</ul>
<div class="math notranslate nohighlight">
\[E = \frac{1}{2}\sum(y_k - t_k)^2\]</div>
<ul class="simple">
<li><p>Cross Entropy Error</p></li>
</ul>
<div class="math notranslate nohighlight">
\[E= -\sum_{k}t_k\log(y_k)\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mean_square_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-7</span> <span class="c1"># avoid log(0)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">delta</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## mean square error</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span> <span class="c1"># predicted values</span>
<span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># true label</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mean_square_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cross_entropy_error</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">t</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.09750000000000003
0.510825457099338
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="errors-based-on-batch-samples">
<h3>Errors based on Batch Samples<a class="headerlink" href="#errors-based-on-batch-samples" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>If the training is based on a sample of batch size <em>N</em>, we can compute the average loss (or total errors) of the batch sample:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ E = - \frac{1}{N}\sum_n\sum_k t_{nk}\log y_{nk}\]</div>
<ul class="simple">
<li><p>We can revise the <code class="docutils literal notranslate"><span class="pre">cross_entropy_error()</span></code> function to work with outputs from a min-batch sample.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># adjust the function to for batch sample outputs</span>
<span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span><span class="o">/</span><span class="n">batch_size</span> 
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When the labels uses one-hot encoding, the function can be simplified as follows:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># because for one-hot labels</span>
    <span class="c1"># cross-entropy sums only the values of the true labels `1`</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span><span class="o">/</span><span class="n">batch_size</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>With the Loss Function, we can now perform the most important step in model training – adjusting the weights of the model.</p></li>
<li><p>The mechanism behind the neural network training steps is that we need to figure out:</p>
<ul>
<li><p>how the change of a specific parameter (i.e., weight) in the model may lead to the change (i.e., decrease) of the values from the Loss Function? (i.e., How much does a change in a specific weight affect the total error?)</p></li>
</ul>
</li>
<li><p>Then we would know how much of the total error each wight in the model is responsible for.</p></li>
<li><p>These turn out to be the basis for parameter adjustments.</p></li>
<li><p>The error that a specific weight is responsible for is referred to as the <strong>gradient</strong> of the parameter.</p></li>
<li><p>Mathematically, the gradient of a weight is the partial derivative of a weight in relation to the loss function.</p></li>
</ul>
<div class="section" id="partial-derivative">
<h3>Partial Derivative<a class="headerlink" href="#partial-derivative" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Once if we have defined the loss function for the model, we can calculate to what extent the change in weights would affect the change in loss function</p></li>
<li><p>The partial derivative refers to how a change in a specific weight <span class="math notranslate nohighlight">\(W_1\)</span> affects the total error.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial E}{\partial W_1}\]</div>
</div>
<div class="section" id="chain-rule-and-back-propagation">
<h3>Chain Rule and Back Propagation<a class="headerlink" href="#chain-rule-and-back-propagation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Because there are many parameters in a network, we can compute the gradients (or partial derivatives) of all the weights using the chain rules of derivatives.</p></li>
<li><p>Specifically, the total error is essentially broken up and distributed back through the network to every single weight with the help of chain rule:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y}.\frac{\partial y}{\partial x}\]</div>
<ul class="simple">
<li><p>This process is referred to as <strong>back propagation</strong>: moving back through the network, back-propagating the total errors to every single weight, and updating the weights.</p></li>
<li><p>The principle of weights-updating: the larger the gradient, the more the adjustments.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[W_1 = W_1 - \eta \frac{\partial E}{\partial W_1}\]</div>
<ul class="simple">
<li><p>The above adjustment formula suggests that the weight updates are proportional to the partial derivatives of the weight.</p></li>
<li><p>The <strong><span class="math notranslate nohighlight">\(\eta\)</span></strong> in the formula controls the amount of adjustment, which is referred to as the <strong>learning rate</strong>.</p></li>
</ul>
</div>
</div>
<div class="section" id="math-on-derivatives">
<h2>Math on Derivatives<a class="headerlink" href="#math-on-derivatives" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Function for derivatives</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span><span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">h</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">tangent_line</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="c1">#print(d)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">d</span><span class="o">*</span><span class="n">x</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">d</span><span class="o">*</span><span class="n">t</span> <span class="o">+</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Take the derivates of the following function when x = 5 and 10:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[y = 0.01x^2 + 0.1x\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.01</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">fun_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">tf</span> <span class="o">=</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">fun_1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y5</span> <span class="o">=</span> <span class="n">tf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/neural-network-from-scratch_56_0.png" src="../_images/neural-network-from-scratch_56_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.1999999999990898
0.2999999999986347
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="math-on-partial-derivaites">
<h2>Math on Partial Derivaites<a class="headerlink" href="#math-on-partial-derivaites" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Take the partial derivatives of the following function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ f(x_0, x_1)=x_0^2 + x_1^2 \]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## fun_2 has two variables/weights</span>
<span class="k">def</span> <span class="nf">fun_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(x_0=3\)</span> and <span class="math notranslate nohighlight">\(x_1=4\)</span>, compute the partial derivative of <span class="math notranslate nohighlight">\(x_0\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_0}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_2_tmp1</span><span class="p">(</span><span class="n">x0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x0</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mf">4.0</span><span class="o">**</span><span class="mi">2</span>

<span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_2_tmp1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.00000000000378
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(x_0=3\)</span> and <span class="math notranslate nohighlight">\(x_1=4\)</span>, compute the partial derivative of <span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_1}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fun_2_tmp2</span><span class="p">(</span><span class="n">x1</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">3.0</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span>

<span class="n">numerical_diff</span><span class="p">(</span><span class="n">fun_2_tmp2</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7.999999999999119
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="math-on-gradients">
<h2>Math on Gradients<a class="headerlink" href="#math-on-gradients" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We can calculate the partial derivatives all at once,and represent them in a vector, which is referred to as <strong>gradient</strong>: <span class="math notranslate nohighlight">\((\frac{\partial f}{\partial x_0}, \frac{\partial f}{\partial x_1})\)</span></p></li>
<li><p>Each value pair, <span class="math notranslate nohighlight">\((x_0,x_1)\)</span>, should correspond to a gradient.</p></li>
<li><p>Intuitive understanding of the gradient:</p>
<ul>
<li><p>The gradient of a specific <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> is a <strong>vector</strong> with the direction pointing at the <strong>global minimum</strong> of the function.</p></li>
<li><p>The more farther the <span class="math notranslate nohighlight">\((x_0,x_1)\)</span> is way from the global minimum, the larger the gradient vector.</p></li>
</ul>
</li>
<li><p>These gradients turn out to be the most important basis in neural network training.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>


<span class="k">def</span> <span class="nf">_numerical_gradient_no_batch</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span>  <span class="c1"># 0.0001</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">tmp_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">tmp_val</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span>
        <span class="n">fxh1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># f(x+h)</span>
        
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">-</span> <span class="n">h</span> 
        <span class="n">fxh2</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># f(x-h)</span>
        <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh1</span> <span class="o">-</span> <span class="n">fxh2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
        
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span>  <span class="c1"># 値を元に戻す</span>
        
    <span class="k">return</span> <span class="n">grad</span>


<span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_numerical_gradient_no_batch</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">_numerical_gradient_no_batch</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">grad</span>


<span class="k">def</span> <span class="nf">function_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">tangent_line</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">d</span><span class="o">*</span><span class="n">x</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">d</span><span class="o">*</span><span class="n">t</span> <span class="o">+</span> <span class="n">y</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="n">grad</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">function_2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="o">-</span><span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>  <span class="n">angles</span><span class="o">=</span><span class="s2">&quot;xy&quot;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;#666666&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x0&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/neural-network-from-scratch_67_0.png" src="../_images/neural-network-from-scratch_67_0.png" />
</div>
</div>
</div>
<div class="section" id="sequence-model">
<h2>Sequence Model<a class="headerlink" href="#sequence-model" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../_images/lstm-many2one.png" /></p>
<p><img alt="" src="../_images/lstm-many2many.png" /></p>
<ul class="simple">
<li><p>The following graph is taken from <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Christopher Olah’s blog post: Understanding LSTM Networks</a></p></li>
</ul>
<p><img alt="" src="../_images/colah-LSTM3-chain.png" /></p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.books.com.tw/products/0010761759">Deep Learning From Scratch</a></p></li>
<li><p><a class="reference external" href="https://buzzorange.com/techorange/2017/08/21/the-best-ai-lesson/">史上最完整機器學習自學攻略！我不相信有人看完這份不會把它加進我的最愛</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python-notes",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="sentiment-analysis-ml.html" title="previous page">Sentiment Analysis with Traditional Machine Learning</a>
    <a class='right-next' id="next-link" href="neural-language-model-primer.html" title="next page">Neural Language Model: A Start</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alvin Chen<br/>
        
            &copy; Copyright 2020 Alvin Chen.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>