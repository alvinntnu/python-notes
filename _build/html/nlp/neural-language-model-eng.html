
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Language Model of English &#8212; Python Notes for Linguistics</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mycss.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/ntnu-word-2.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Python Notes for Linguistics</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../python-basics/python-basics.html">
   Python Basics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/jupyter-notebook.html">
     Jupyer Notebook
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/notebook-to-slides.html">
     Notebook to Slides
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/google-colab.html">
     Google Colaboratory (Colab)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/google-colab-r.html">
     Google Colab R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/data-structure.html">
     Data Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/regex.html">
     Regular Expression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/numpy.html">
     Numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/pandas.html">
     Pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/program-structure.html">
     Program Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/input-output.html">
     Input and Output
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/pickle.html">
     Object Serialization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/data-visualization-1.html">
     Data Visualization I
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/data-visualization-2.html">
     Data Visualization 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/oop.html">
     Object-Oriented Programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/magic-r.html">
     Magic R
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/docstrings.html">
     Docstrings Format
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/python-tricks.html">
     Python Tricks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/web-applications.html">
     Web Applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/google-drive-ubuntu.html">
     Google Drive with Ubuntu
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/productive-tech.html">
     Phythonic Productivity Techniques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python-basics/miscellaneous-notes.html">
     Miscellaneous Notes
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../corpus/corpus-processing.html">
   Corpus Linguistics with Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../corpus/nltk.html">
     Natural Language Tool-Kits (NLTK)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../corpus/web-crawler.html">
     Web Crawler
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../corpus/unicode.html">
     Unicode
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../corpus/corpus-linguistics-methods.html">
     Corpus Lingustics Methods
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/lexical-bundles.html">
       Lexical Bundles
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/tokenization.html">
       Tokenization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/wordnet.html">
       WordNet
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/word-cloud.html">
       Word Cloud
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/patterns-constructions.html">
       Patterns and Constructions
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../corpus/structured-corpus-processing.html">
     Structured Corpus Processing
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/bnc.html">
       BNC-XML
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/childes.html">
       CHILDES Corpus
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/praat-textgrid.html">
       Praat TextGrid Data
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../corpus/vectorizing-text.html">
     Vectorizing Texts
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../corpus/chinese-processing.html">
     Chinese Processing
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/jieba.html">
       Chinese Word Segmentation (jieba)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../corpus/ckiptagger.html">
       Chinese Word Segmentation (ckiptagger)
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../statistical-analyses/statistical-analyses.html">
   Statistical Analyses
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../statistical-analyses/descriptive-statistics.html">
     Descriptive Statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistical-analyses/analytic-statistics.html">
     Analytic Statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistical-analyses/network-analysis.html">
     Network Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../statistical-analyses/network-analysis-igraph.html">
     Network Analysis Using Igraph
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="nlp.html">
   Natural Language Processing with Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="nlp-primer.html">
     Natural Language Processing: A Primer
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="nlp-pipeline.html">
       NLP Pipeline
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="nlp-spacy.html">
       Natural Language Processing (spaCy)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="nlp-spacy-zh.html">
       Chinese Natural Language Processing (spaCy)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="nlp-ckipnlp.html">
       Natural Language Processing (ckipnlp)
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="text-normalization-intro.html">
     Text Normalization
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="text-normalization-eng.html">
       Text Normalization (English)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="text-normalization-chinese.html">
       Text Normalization (Chinese)
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="ml-overview.html">
     Machine Learning Overview
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="ml-sklearn-regression.html">
       Machine Learning with Sklearn – Regression
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="ml-sklearn-classification.html">
       Machine Learning with Sci-Kit Learn
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="naive-bayes.html">
       Naive Bayes
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="sentiment-analysis-ml.html">
       Sentiment Analysis with Traditional Machine Learning
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neural-network-from-scratch.html">
     Neural Network From Scratch
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="language-model.html">
     Language Model
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="neural-language-model-primer.html">
       Neural Language Model: A Start
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="neural-language-model-zh.html">
       Neural Language Model of Chinese
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="text-gen-lstm-v1.html">
       Text Generation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="gpt2.html">
       Transformer-based Language Model - GPT2
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="word-embeddings.html">
     Word Embeddings
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="word2vec-chinese.html">
       Word Embeddings with Chinese Texts
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="word2vec.html">
       Word2Vec
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="doc2vec.html">
       Dov2Vec
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="word-embeddings-autoencoder.html">
       Generate Text Embeddings Using AutoEncoder
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="transfer-learning-sent-encoding.html">
       Universal Sentence Embeddings
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sentiment-analysis-dl.html">
     Sentiment Analysis with Deep Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sentiment-analysis-lstm-v1.html">
     Sentiment Analysis with LSTM
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="seq-to-seq-types.html">
     Intutions for Types of Sequence-to-Sequence Models
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="seq-to-seq-types-date.html">
       Types of Seqeunce Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="seq-to-seq-m21-sentiment-attention.html">
       Sequence Model (many-to-one) with Attention
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="seq-to-seq-attention-addition.html">
       Seqeunce Model with Attention for Addition Learning
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="seq-to-seq-machine-translation.html">
     Machine Translation (Sequence-to-Sequence)
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="seq-to-seq-machine-translation-attention.html">
       Machine Translation with Attention (Thushan)
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="hyperparameter-tuning.html">
     Hyper-Parameter Tuning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sentiment-analysis-using-bert-chinese.html">
     Sentiment Analysis Using BERT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ktrain-tutorial-explaining-predictions.html">
     Explainable AI
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../appendix/todo.html">
   To-do List
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../appendix/references.html">
     References
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  <div style="text-align:left">
<i class="fas fa-home fa-2x" style="color:Maroon;margin-right:5px"></i><a href="https://alvinchen.myftp.org/" target='_blank'>Alvin Chen's Homepage</a>
</div>

</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/nlp/neural-language-model-eng.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/alvinntnu/python-notes/master?urlpath=tree/nlp/neural-language-model-eng.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/alvinntnu/python-notes/blob/master/nlp/neural-language-model-eng.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neural-language-model-of-english">
<h1>Neural Language Model of English<a class="headerlink" href="#neural-language-model-of-english" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/">How to development a word-level neural language model in keras</a></p></li>
<li><p>English texts</p></li>
<li><p>Word-based neural language model based on word sequences of 50 words</p></li>
<li><p>Use the republic texts</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">string</span>


<span class="c1"># load doc into memory</span>
<span class="k">def</span> <span class="nf">load_doc</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="c1"># open the file as read only</span>
    <span class="n">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="c1"># read all text</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="c1"># close the file</span>
    <span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">text</span>


<span class="c1"># turn a doc into clean tokens</span>
<span class="k">def</span> <span class="nf">clean_doc</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
    <span class="c1"># replace &#39;--&#39; with a space &#39; &#39;</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="c1"># split into tokens by white space</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="c1"># remove punctuation from each token</span>
    <span class="n">table</span> <span class="o">=</span> <span class="nb">str</span><span class="o">.</span><span class="n">maketrans</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="n">table</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    <span class="c1"># remove remaining tokens that are not alphabetic</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
    <span class="c1"># make lower case</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tokens</span>


<span class="c1"># save tokens to file, one dialog per line</span>
<span class="k">def</span> <span class="nf">save_doc</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
    <span class="n">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="c1"># load document</span>
<span class="n">in_filename</span> <span class="o">=</span> <span class="s1">&#39;republic_clean.txt&#39;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">load_doc</span><span class="p">(</span><span class="n">in_filename</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">[:</span><span class="mi">200</span><span class="p">])</span>

<span class="c1"># clean document</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">clean_doc</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="mi">200</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total Tokens: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Unique Tokens: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">)))</span>

<span class="c1"># organize into sequences of tokens</span>
<span class="n">length</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)):</span>
    <span class="c1"># select sequence of tokens</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">length</span><span class="p">:</span><span class="n">i</span><span class="p">]</span>
    <span class="c1"># convert into a line</span>
    <span class="n">line</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="c1"># store</span>
    <span class="n">sequences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total Sequences: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>

<span class="c1"># save sequences to file</span>
<span class="n">out_filename</span> <span class="o">=</span> <span class="s1">&#39;republic_sequences.txt&#39;</span>
<span class="n">save_doc</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">out_filename</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The Project Gutenberg EBook of The Republic, by Plato

This eBook is for the use of anyone anywhere at no cost and with
almost no restrictions whatsoever.  You may copy it, give it away or
re-use it u
[&#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;ebook&#39;, &#39;of&#39;, &#39;the&#39;, &#39;republic&#39;, &#39;by&#39;, &#39;plato&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;is&#39;, &#39;for&#39;, &#39;the&#39;, &#39;use&#39;, &#39;of&#39;, &#39;anyone&#39;, &#39;anywhere&#39;, &#39;at&#39;, &#39;no&#39;, &#39;cost&#39;, &#39;and&#39;, &#39;with&#39;, &#39;almost&#39;, &#39;no&#39;, &#39;restrictions&#39;, &#39;whatsoever&#39;, &#39;you&#39;, &#39;may&#39;, &#39;copy&#39;, &#39;it&#39;, &#39;give&#39;, &#39;it&#39;, &#39;away&#39;, &#39;or&#39;, &#39;reuse&#39;, &#39;it&#39;, &#39;under&#39;, &#39;the&#39;, &#39;terms&#39;, &#39;of&#39;, &#39;the&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;license&#39;, &#39;included&#39;, &#39;with&#39;, &#39;this&#39;, &#39;ebook&#39;, &#39;or&#39;, &#39;online&#39;, &#39;at&#39;, &#39;wwwgutenbergorg&#39;, &#39;title&#39;, &#39;the&#39;, &#39;republic&#39;, &#39;author&#39;, &#39;plato&#39;, &#39;translator&#39;, &#39;b&#39;, &#39;jowett&#39;, &#39;posting&#39;, &#39;date&#39;, &#39;august&#39;, &#39;ebook&#39;, &#39;release&#39;, &#39;date&#39;, &#39;october&#39;, &#39;last&#39;, &#39;updated&#39;, &#39;june&#39;, &#39;language&#39;, &#39;english&#39;, &#39;start&#39;, &#39;of&#39;, &#39;this&#39;, &#39;project&#39;, &#39;gutenberg&#39;, &#39;ebook&#39;, &#39;the&#39;, &#39;republic&#39;, &#39;produced&#39;, &#39;by&#39;, &#39;sue&#39;, &#39;asscher&#39;, &#39;the&#39;, &#39;republic&#39;, &#39;by&#39;, &#39;plato&#39;, &#39;translated&#39;, &#39;by&#39;, &#39;benjamin&#39;, &#39;jowett&#39;, &#39;note&#39;, &#39;the&#39;, &#39;republic&#39;, &#39;by&#39;, &#39;plato&#39;, &#39;jowett&#39;, &#39;etext&#39;, &#39;introduction&#39;, &#39;and&#39;, &#39;analysis&#39;, &#39;the&#39;, &#39;republic&#39;, &#39;of&#39;, &#39;plato&#39;, &#39;is&#39;, &#39;the&#39;, &#39;longest&#39;, &#39;of&#39;, &#39;his&#39;, &#39;works&#39;, &#39;with&#39;, &#39;the&#39;, &#39;exception&#39;, &#39;of&#39;, &#39;the&#39;, &#39;laws&#39;, &#39;and&#39;, &#39;is&#39;, &#39;certainly&#39;, &#39;the&#39;, &#39;greatest&#39;, &#39;of&#39;, &#39;them&#39;, &#39;there&#39;, &#39;are&#39;, &#39;nearer&#39;, &#39;approaches&#39;, &#39;to&#39;, &#39;modern&#39;, &#39;metaphysics&#39;, &#39;in&#39;, &#39;the&#39;, &#39;philebus&#39;, &#39;and&#39;, &#39;in&#39;, &#39;the&#39;, &#39;sophist&#39;, &#39;the&#39;, &#39;politicus&#39;, &#39;or&#39;, &#39;statesman&#39;, &#39;is&#39;, &#39;more&#39;, &#39;ideal&#39;, &#39;the&#39;, &#39;form&#39;, &#39;and&#39;, &#39;institutions&#39;, &#39;of&#39;, &#39;the&#39;, &#39;state&#39;, &#39;are&#39;, &#39;more&#39;, &#39;clearly&#39;, &#39;drawn&#39;, &#39;out&#39;, &#39;in&#39;, &#39;the&#39;, &#39;laws&#39;, &#39;as&#39;, &#39;works&#39;, &#39;of&#39;, &#39;art&#39;, &#39;the&#39;, &#39;symposium&#39;, &#39;and&#39;, &#39;the&#39;, &#39;protagoras&#39;, &#39;are&#39;, &#39;of&#39;, &#39;higher&#39;, &#39;excellence&#39;, &#39;but&#39;, &#39;no&#39;, &#39;other&#39;, &#39;dialogue&#39;, &#39;of&#39;, &#39;plato&#39;, &#39;has&#39;, &#39;the&#39;, &#39;same&#39;, &#39;largeness&#39;, &#39;of&#39;, &#39;view&#39;, &#39;and&#39;, &#39;the&#39;, &#39;same&#39;, &#39;perfection&#39;, &#39;of&#39;, &#39;style&#39;, &#39;no&#39;, &#39;other&#39;, &#39;shows&#39;, &#39;an&#39;, &#39;equal&#39;, &#39;knowledge&#39;, &#39;of&#39;]
Total Tokens: 216791
Unique Tokens: 10454
Total Sequences: 216740
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">pickle</span> <span class="kn">import</span> <span class="n">dump</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span>

<span class="c1"># load doc into memory</span>
<span class="k">def</span> <span class="nf">load_doc</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
	<span class="c1"># open the file as read only</span>
	<span class="n">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
	<span class="c1"># read all text</span>
	<span class="n">text</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
	<span class="c1"># close the file</span>
	<span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
	<span class="k">return</span> <span class="n">text</span>

<span class="c1"># load</span>
<span class="n">in_filename</span> <span class="o">=</span> <span class="s1">&#39;republic_sequences.txt&#39;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">load_doc</span><span class="p">(</span><span class="n">in_filename</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># integer encode sequences of words</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
<span class="c1"># vocabulary size</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># separate into input and output</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sequences</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>  <span class="c1"># one-hot encode y</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span>
                    <span class="n">input_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">))</span>  <span class="c1"># word ebmedding layer</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>  <span class="c1"># LSTM 1</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>  <span class="c1"># LSTM 2</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="c1"># compile model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="c1"># fit model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 50, 50)            522750    
_________________________________________________________________
lstm (LSTM)                  (None, 50, 100)           60400     
_________________________________________________________________
lstm_1 (LSTM)                (None, 100)               80400     
_________________________________________________________________
dense (Dense)                (None, 100)               10100     
_________________________________________________________________
dense_1 (Dense)              (None, 10455)             1055955   
=================================================================
Total params: 1,729,605
Trainable params: 1,729,605
Non-trainable params: 0
_________________________________________________________________
None
424/424 [==============================] - 236s 557ms/step - loss: 6.4235 - accuracy: 0.0734
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.callbacks.History at 0x7f8f39670790&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># save the model to file</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;model.h5&#39;</span><span class="p">)</span>
<span class="c1"># save the tokenizer</span>
<span class="n">dump</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;tokenizer.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>
<span class="kn">from</span> <span class="nn">pickle</span> <span class="kn">import</span> <span class="n">load</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>


<span class="c1"># load doc into memory</span>
<span class="k">def</span> <span class="nf">load_doc</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="c1"># open the file as read only</span>
    <span class="n">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="c1"># read all text</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="c1"># close the file</span>
    <span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">text</span>


<span class="c1"># generate a sequence from a language model</span>
<span class="k">def</span> <span class="nf">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">seed_text</span><span class="p">,</span> <span class="n">n_words</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">in_text</span> <span class="o">=</span> <span class="n">seed_text</span>
    <span class="c1"># generate a fixed number of words</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_words</span><span class="p">):</span>
        <span class="c1"># encode the text as integer</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">in_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># truncate sequences to a fixed length</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">([</span><span class="n">encoded</span><span class="p">],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="s1">&#39;pre&#39;</span><span class="p">)</span>
        <span class="c1"># predict probabilities for each word</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_classes</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># map predicted word index to word</span>
        <span class="n">out_word</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">yhat</span><span class="p">:</span>
                <span class="n">out_word</span> <span class="o">=</span> <span class="n">word</span>
                <span class="k">break</span>
        <span class="c1"># append to input</span>
        <span class="n">in_text</span> <span class="o">+=</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">out_word</span>
        <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_word</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>


<span class="c1"># load cleaned text sequences</span>
<span class="n">in_filename</span> <span class="o">=</span> <span class="s1">&#39;republic_sequences.txt&#39;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">load_doc</span><span class="p">(</span><span class="n">in_filename</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># load the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;model.h5&#39;</span><span class="p">)</span>

<span class="c1"># load the tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;tokenizer.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">))</span>

<span class="c1"># select a seed text</span>
<span class="n">seed_text</span> <span class="o">=</span> <span class="n">lines</span><span class="p">[</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">))]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">seed_text</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># generate new text</span>
<span class="n">generated</span> <span class="o">=</span> <span class="n">generate_seq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">seed_text</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python-notes",
            path: "./nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python-notes'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alvin Chen<br/>
        
            &copy; Copyright 2020 Alvin Chen.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>