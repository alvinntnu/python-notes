{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"neural-language-model-zh.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0sSBsVXBpwvc"},"source":["# Neural Language Model of Chinese\n","\n","- [How to development a word-level neural language model in keras](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/)\n","- Chinese texts\n","- Word-based neural language model based on:\n","    - character sequences\n","    - word sequences\n","- Use two novels by Jing-Yong"]},{"cell_type":"code","metadata":{"id":"wtgcPXVLp7im","executionInfo":{"status":"ok","timestamp":1603918207787,"user_tz":-480,"elapsed":29621,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}},"outputId":"dc092527-8553-47a7-8e94-a36c5af1cb26","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kVunfvneqFnc","executionInfo":{"status":"ok","timestamp":1603918209661,"user_tz":-480,"elapsed":31489,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}}},"source":["#import sys\n","#sys.path.insert(1, '/content/drive/My Drive/_MySyncDrive/Repository/python-notes/nlp')\n","import os\n","os.chdir('/content/drive/My Drive/_MySyncDrive/Repository/python-notes/nlp')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"YmWy6y8Epwvd","executionInfo":{"status":"ok","timestamp":1603918214863,"user_tz":-480,"elapsed":36687,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}}},"source":["import string\n","import text_normalizer_zh as tn\n","import re\n","# load doc into memory\n","def load_doc(filename):\n","    # open the file as read only\n","    file = open(filename, 'r', encoding='utf-8')\n","    # read all text\n","    text = file.read()\n","    # close the file\n","    file.close()\n","    return text\n","\n","\n","# turn a doc into clean tokens\n","def clean_doc_paras(doc):\n","    # get content paragraphs only\n","    paras = [p for p in doc.split(sep=\"\\n\")[7:] if p.startswith('  ')]\n","    paras = [tn.remove_symbols(p) for p in paras]\n","    paras = [tn.remove_extra_spaces(p) for p in paras]\n","    return paras\n","\n","def clean_doc_lines(doc):\n","    # get content paragraphs only\n","    paras = [p for p in re.split(\"[\\n，。]\", doc)[7:] if p.startswith('  ')]\n","    paras = [tn.remove_symbols(p) for p in paras]\n","    paras = [tn.remove_extra_spaces(p) for p in paras]\n","    return paras\n","\n","\n","# save tokens to file, one dialog per line\n","def save_doc(lines, filename):\n","    data = '\\n'.join(lines)\n","    file = open(filename, 'w')\n","    file.write(data)\n","    file.close()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ng3yUAjipwvg","executionInfo":{"status":"ok","timestamp":1603918214864,"user_tz":-480,"elapsed":36684,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}},"outputId":"bc6d7545-6c04-4419-a43c-c8102f6cd560","colab":{"base_uri":"https://localhost:8080/","height":211}},"source":["# load document\n","in_filename = \"../../../RepositoryData/data/jingyong-part-cht-utf8.txt\"\n","doc=load_doc(in_filename)\n","print(doc[:200])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["『金庸作品集/作者:金庸』\n","『狀態:已完結』\n","『內容簡介:\n","    金庸的所有的書&lt;/p&gt;\n","』\n","愛下電子書Txt版閱讀,下載和分享更多電子書請訪問:http://www.ixdzs.com,手機訪問:http://m.ixdzs.com,E-mail:support@ixdzs.com\n","------章節內容開始-------\n","天龍八部\n","第一章 青衫磊落險峰行\n","    青光閃動，一柄青鋼\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RoxSROTupwvj","executionInfo":{"status":"ok","timestamp":1603918215158,"user_tz":-480,"elapsed":36974,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}},"outputId":"b5ca9b7a-f95b-4646-b8bf-7e3f451265ca","colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# clean document\n","paras = clean_doc_lines(doc)\n","print(paras[:20])\n","print('Total Paragraphs: %d' % len(paras))\n","print('Unique Tokens: %d' % len(set(''.join(paras))))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["['青光閃動', '兩人劍法迅捷', '練武廳東坐著二人', '眼見那少年與中年漢子已拆到七十余招', '便在這時', '那長須老者滿臉得色', '這老者姓左', '無量劍原分東北西三宗', '西首錦凳上所坐的則是別派人士', '當下左子穆笑道辛師妹今年派出的四名弟子', '馬五德臉上微微一紅', '左子穆心想他若是你弟子', '那姓段青年微笑道在下單名一譽字', '馬五德和段譽也是初交', '左子穆道段兄既然不是馬五哥的好朋友', '那中年漢子龔光杰巴不得師父有這句話', '段譽輕揮折扇', '他這番說什麼你師父我師父的', '龔光杰大踏步過來', '段譽道你這位大爺怎地如此狠霸霸的我平生最不愛瞧人打架']\n","Total Paragraphs: 34031\n","Unique Tokens: 3388\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ToWMwS1opwvl"},"source":["## Line-based Language Model"]},{"cell_type":"code","metadata":{"id":"GDmy9FZepwvm","executionInfo":{"status":"ok","timestamp":1603918217009,"user_tz":-480,"elapsed":38820,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}}},"source":["from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical, plot_model\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnYO3OCCpwvo"},"source":["### Text to Sequences"]},{"cell_type":"code","metadata":{"id":"i9dOWLljpwvp","executionInfo":{"status":"ok","timestamp":1603918217009,"user_tz":-480,"elapsed":38817,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}},"outputId":"f463b22b-046c-4ce0-c98c-25a612c90750","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# prepare data\n","data = '\\n'.join(paras)  # collapse the entire corpus into one string\n","# prepare the tokenizer on the source text\n","tokenizer = Tokenizer(\n","    oov_token=1, char_level=True\n",")  ## specify the word id for unknown words + char_level tokenizer\n","tokenizer.fit_on_texts([data])\n","\n","# determine the vocabulary size\n","## zero index is reserved in keras as the padding token (+1) and one unknown word id\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)\n","\n","# create paragraph-based sequences\n","sequences = list()\n","for line in data.split('\\n'):\n","    encoded = tokenizer.texts_to_sequences([line])[0]\n","    ## For each line, after converting words into indexes\n","    ## prepare sequences for training\n","    ## given a line, w1,w2,w3,w4\n","    ## create input sequences:\n","    ## w1,w2\n","    ## w1,w2,w3\n","    ## w1,w2,w3,w4\n","    for i in range(1, len(encoded)):\n","        sequence = encoded[:i + 1]\n","        sequences.append(sequence)\n","print('Total Sequences: %d' % len(sequences))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Vocabulary Size: 3391\n","Total Sequences: 222346\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2GtgSyE4pwvr"},"source":["### Word ID to Texts"]},{"cell_type":"code","metadata":{"id":"74gBA-YZpwvs","executionInfo":{"status":"ok","timestamp":1603918217010,"user_tz":-480,"elapsed":38814,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}}},"source":["# Creating a reverse dictionary\n","reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n","# Function takes a tokenized sentence and returns the words\n","def sequence_to_text(list_of_indices):\n","    # Looking up words in dictionary\n","    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n","    return(words)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhoASBn5pwvu","executionInfo":{"status":"ok","timestamp":1603918217011,"user_tz":-480,"elapsed":38811,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}},"outputId":"d8c0193f-598c-4368-b458-ec2df7d0566c","colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["sequences[:10]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[256, 105],\n"," [256, 105, 506],\n"," [256, 105, 506, 195],\n"," [82, 8],\n"," [82, 8, 34],\n"," [82, 8, 34, 90],\n"," [82, 8, 34, 90, 913],\n"," [82, 8, 34, 90, 913, 1650],\n"," [437, 129],\n"," [437, 129, 796]]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"njqEC5Ztpwvx","executionInfo":{"status":"ok","timestamp":1603918217011,"user_tz":-480,"elapsed":38807,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}},"outputId":"c8c23665-ae02-4eee-adbd-c88a1798caac","colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["[print(sequence_to_text(s)) for s in sequences[:10]]"],"execution_count":10,"outputs":[{"output_type":"stream","text":["['青', '光']\n","['青', '光', '閃']\n","['青', '光', '閃', '動']\n","['兩', '人']\n","['兩', '人', '劍']\n","['兩', '人', '劍', '法']\n","['兩', '人', '劍', '法', '迅']\n","['兩', '人', '劍', '法', '迅', '捷']\n","['練', '武']\n","['練', '武', '廳']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[None, None, None, None, None, None, None, None, None, None]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"hFWPYTFmpwvz"},"source":["### Padding"]},{"cell_type":"code","metadata":{"id":"uqMwMVHIpwvz","executionInfo":{"status":"ok","timestamp":1603918218059,"user_tz":-480,"elapsed":39850,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}},"outputId":"a468ef07-5f37-40d5-cd1d-32a64e1985b2","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["max_length = max([len(seq) for seq in sequences])\n","sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n","print('Max Sequence Length: %d' % max_length)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Max Sequence Length: 133\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n2mPNCwMpwv1"},"source":["### Train and Test Sets"]},{"cell_type":"code","metadata":{"id":"t1UfTcHipwv2","executionInfo":{"status":"ok","timestamp":1603918219975,"user_tz":-480,"elapsed":41762,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}}},"source":["# split into input and output elements\n","sequences = array(sequences)\n","X, y = sequences[:,:-1],sequences[:,-1]\n","y = to_categorical(y, num_classes=vocab_size)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kj8dzy9Kpwv4"},"source":["### Define Model"]},{"cell_type":"code","metadata":{"id":"dPB0xHSQpwv4","executionInfo":{"status":"ok","timestamp":1603918225410,"user_tz":-480,"elapsed":47193,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}},"outputId":"01d40c48-9950-4210-96b8-16fb30a95806","colab":{"base_uri":"https://localhost:8080/","height":347}},"source":["# define model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 128, input_length=max_length-1))\n","model.add(LSTM(100, return_sequences=True))  # LSTM 1\n","model.add(LSTM(100))  # LSTM 2\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(vocab_size, activation='softmax'))\n","print(model.summary())\n","# compile network\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 132, 128)          434048    \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 132, 100)          91600     \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100)               80400     \n","_________________________________________________________________\n","dense (Dense)                (None, 100)               10100     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3391)              342491    \n","=================================================================\n","Total params: 958,639\n","Trainable params: 958,639\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WUHsVPb7pwv6"},"source":["### Train Model"]},{"cell_type":"code","metadata":{"id":"gorozb0vpwv6","executionInfo":{"status":"ok","timestamp":1603918225411,"user_tz":-480,"elapsed":47193,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}}},"source":["# # fit network\n","# model.fit(X, y, batch_size= 256, epochs=500, verbose=1)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_v00pwFDtqWb"},"source":["### Save Model"]},{"cell_type":"code","metadata":{"id":"LTZ90tn0tsV4","executionInfo":{"status":"ok","timestamp":1603918225411,"user_tz":-480,"elapsed":47187,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}}},"source":["# from pickle import dump\n","# # save the model to file\n","# model.save('jing-yong-line-lm-model.h5')\n","# # save the tokenizer\n","# dump(tokenizer, open('jing-yong-line-lm-tokenizer.pkl', 'wb'))"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKRB10EZ9-yF"},"source":["## Load Model"]},{"cell_type":"code","metadata":{"id":"WoPZbAgw98dP","executionInfo":{"status":"ok","timestamp":1603918272877,"user_tz":-480,"elapsed":1446,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}}},"source":["import pickle\n","model.load_weights('jing-yong-line-lm-model.h5')\n","pickle_in = open('jing-yong-line-lm-tokenizer.pkl',\n","                 \"rb\")\n","tokenizer = pickle.load(pickle_in)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"620lhtkYt9cH"},"source":["### Generate Sequence"]},{"cell_type":"code","metadata":{"id":"R-61bUdkuQa9","executionInfo":{"status":"ok","timestamp":1603918272878,"user_tz":-480,"elapsed":1441,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}}},"source":["# generate a sequence from a language model\n","def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n","\tin_text = seed_text\n","\t# generate a fixed number of words\n","\tfor _ in range(n_words):\n","\t\t# encode the text as integer\n","\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pre-pad sequences to a fixed length\n","\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n","\t\t# predict probabilities for each word\n","\t\tyhat = model.predict_classes(encoded, verbose=0)\n","\t\t# map predicted word index to word\n","\t\tout_word = ''\n","\t\tfor word, index in tokenizer.word_index.items():\n","\t\t\tif index == yhat:\n","\t\t\t\tout_word = word\n","\t\t\t\tbreak\n","\t\t# append to input\n","\t\tin_text += ' ' + out_word\n","\treturn in_text"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"scEKEon-ubsl","executionInfo":{"status":"ok","timestamp":1603918321665,"user_tz":-480,"elapsed":4024,"user":{"displayName":"Alvin Chen","photoUrl":"","userId":"12962786962925949010"}},"outputId":"f8ad8a85-34de-4e1a-b644-77e96cfd3ef4","colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# evaluate model\n","print(generate_seq(model, tokenizer, max_length-1, '師父', 100))\n","# print(generate_seq(model, tokenizer, max_length-1, 'Jill', 4))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["師父 師 姊 娘 我 段 正 結 交 視 那 儀 雖 尚 後 裡 當 年 子 師 父 段 譽 頭 子 引 兒 兒 滑 叛 得 不 過 身 子 道 你 你 什 了 身 子 李 延 慶 忽 袱 畔 起 了 我 呢 尚 穆 道 有 段 正 淳 嗎 朝 皇 此 也 羞 著 譚 婆 兒 子 我 沒 給 丐 姊 平 之 負 他 不 讓 徒 主 不 知 道 人 的 好 你 我 早 睛 兩 個 英 雄 母 殿 想 起\n"],"name":"stdout"}]}]}