{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intutions for Types of Sequence-to-Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/s2s-rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vanishing gradient problems when modeling long-distance relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM vs GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/s2s-lstm.png)\n",
    "![](../images/s2s-gru.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Possible solutions to the issues of vanishing gradients\n",
    "- Better modeling of long-term relationships\n",
    "- GRU is a simpler version of LSTM, rendering it easier to train computationally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/s2s-simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A schematic model representation of many-to-many sequence model\n",
    "- The input and output sizes can be different\n",
    "- A typical task is machine translation\n",
    "- Two sequence models need to be trained: encoder and decoder\n",
    "- Encoder:\n",
    "    - A vanilla version of the seq-to-seq model takes the **last** return state $h_t$ of the encoder as the initial and only input for the decoder\n",
    "    - If the encoder uses the LSTM cell, the output of the encoder would be the last return state and the last memory cell, i.e., $h_t$ and $c_t$\n",
    "- Decoder\n",
    "    - During the training stage, the decoder takes the previous return state $h_{t-1}$ and the current $Y_t$ as the input for the LSTM (concatenated). This is referred to as **teacher forcing**.\n",
    "    - During the testing stage, the decoder would decode the output one at a time, taking the previous return state $h_{t-1}$ and the previous return output $Y_{t-1}$ as the inputs of the LSTM (concatenated). That is, no **teacher-forcing** during the testing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peeky Sequence-to-Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/s2s-peeky.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A variant of the seq-to-seq model, which makes available the last return state $h_{t}$ from the encoder to every time step in the decoder.\n",
    "- An intuitive understanding of this peeky approach is that when decoding the contexts from the source input should be made available to all decoding steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/s2s-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attention mechansim can be seen as much more sophisticated design of the peeky approach.\n",
    "- The idea is that during the decoding stage, we need to consider the pairwise relationship (similarity) in-between the decoder state $h_{t}$ and **ALL** the return states from the encoder.\n",
    "- An intuitive understanding is as follows. When decoding the translation of $Y_{1}$, it is very likely that its translation is more relevant to some of the input words and less relevant to the others.\n",
    "- Therefore, the attention mechanism first needs to determine the relative pairwise relationship in-between the decoder $h_{1}$ and all the encoder return states in order to get the **attention weights**. \n",
    "- There are many proposals regarding how to compute the attention weights. Please see a very nice review of Lilian Weng's [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html).\n",
    "- In the current Tensorflow implementation, there are three types of [Attention layers](https://keras.io/api/layers/attention_layers/):\n",
    "    - AdditiveAttention Layer: Bahdanau's style attention [Bahdanau2015](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - Attention Layer: Luong's style attention [Luong2015](https://arxiv.org/pdf/1508.4025.pdf)\n",
    "    - MultiHeadAttention Layer: transformer's style attention [“Attention is All you Need” (Vaswani, et al., 2017)](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n",
    "- With the attention weights, we can compute a context vector, which is like a weighted sum of the encoder return states.\n",
    "- We then use this weighted sum, along of the decoder return state $h_{1}$, to compute the final output $Y_{1}$.\n",
    "- This attention-mediated mechanism applies to every time step in the decoder."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq-to-seq-attention-date.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
