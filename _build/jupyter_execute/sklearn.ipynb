{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Sci-Kit Learn\n",
    "\n",
    "- Based on [Keith Galli's sklearn tutorial](https://github.com/KeithGalli/sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class\n",
    "\n",
    "- Create a Review class for each token of the data\n",
    "- This also demonstrates how Object-Oriented language works with the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Sentiment:\n",
    "    NEGATIVE = 'NEGATIVE'\n",
    "    NEUTRAL = 'NEUTRAL'\n",
    "    POSITIVE = 'POSITIVE'\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "        \n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return Sentiment.NEGATIVE\n",
    "        elif self.score == 3:\n",
    "            return Sentiment.NEUTRAL\n",
    "        else:\n",
    "            return Sentiment.POSITIVE\n",
    "\n",
    "class ReviewContainer:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews]\n",
    "    def get_sentiment(self):\n",
    "        return [x.sentiment for x in self.reviews]\n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.sentiment == Sentiment.NEGATIVE, self.reviews))\n",
    "        positive = list(filter(lambda x: x.sentiment == Sentiment.POSITIVE, self.reviews))\n",
    "        positive_shrunk = positive[:len(negative)]\n",
    "        self.reviews = negative + positive_shrunk\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fname = '../data/books_small_10000.json'\n",
    "\n",
    "reviews = []\n",
    "with open(fname) as f:\n",
    "    for line in f:\n",
    "        cur_review = json.loads(line)\n",
    "        reviews.append(Review(cur_review['reviewText'], cur_review['overall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 10000\n",
      "Sample Text of Doc 1:\n",
      "------------------------------\n",
      "I bought both boxed sets, books 1-5.  Really a great series!  Start book 1 three weeks ago and just finished book 5.  Sloane Monroe is a great character and being able to follow her through both private life and her PI life gets a reader very involved!  Although clues may be right in front of the reader, there are twists and turns that keep one guessing until the last page!  These are books you won't be disappointed with.\n"
     ]
    }
   ],
   "source": [
    "print('Number of Reviews: {}'.format(len(reviews)))\n",
    "print('Sample Text of Doc 1:')\n",
    "print('-'*30)\n",
    "print(reviews[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'POSITIVE': 8378, 'NEUTRAL': 978, 'NEGATIVE': 644})\n"
     ]
    }
   ],
   "source": [
    "## Check Sentiment Distribution of the Current Dataset\n",
    "from collections import Counter\n",
    "sentiment_distr = Counter([r.get_sentiment() for r in reviews])\n",
    "print(sentiment_distr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(reviews, test_size = 0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'POSITIVE': 5611, 'NEUTRAL': 653, 'NEGATIVE': 436})\n",
      "Counter({'POSITIVE': 2767, 'NEUTRAL': 325, 'NEGATIVE': 208})\n"
     ]
    }
   ],
   "source": [
    "## Sentiment Distrubtion for Train and Test\n",
    "print(Counter([r.get_sentiment() for r in train]))\n",
    "print(Counter([r.get_sentiment() for r in test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance the Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NEGATIVE': 436, 'POSITIVE': 436})\n",
      "Counter({'POSITIVE': 208, 'NEGATIVE': 208})\n"
     ]
    }
   ],
   "source": [
    "train_container = ReviewContainer(train)\n",
    "test_container = ReviewContainer(test)\n",
    "\n",
    "# balance\n",
    "train_container.evenly_distribute()\n",
    "test_container.evenly_distribute()\n",
    "\n",
    "# check sentiment distribution again\n",
    "print(Counter([r.get_sentiment() for r in train_container.reviews]))\n",
    "print(Counter([r.get_sentiment() for r in test_container.reviews]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NEGATIVE': 436, 'POSITIVE': 436})\n"
     ]
    }
   ],
   "source": [
    "train_text = train_container.get_text()\n",
    "train_label = train_container.get_sentiment()\n",
    "test_text = test_container.get_text()\n",
    "test_label = test_container.get_sentiment()\n",
    "\n",
    "# print(train_label.count(Sentiment.POSITIVE))\n",
    "# print(train_label.count(Sentiment.NEGATIVE))\n",
    "print(Counter(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization: Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition,important}\n",
    "\n",
    "- Always split the data into train and test first before vectorizing the texts\n",
    "- Otherwise, you would leak information to the training process, which may lead to over-fitting\n",
    "- When vectorizing the texts, `fit_transform()` the train and `transform()` the test\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "train_text_bow = tfidf_vec.fit_transform(train_text) # fit train\n",
    "test_text_bow = tfidf_vec.transform(test_text) # transform test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(872, 8906)\n",
      "(416, 8906)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 8627)\t0.09836318876375709\n",
      "  (0, 7654)\t0.23134463284680223\n",
      "  (0, 423)\t0.041330774273820904\n",
      "  (0, 447)\t0.16338559600565572\n",
      "  (0, 3933)\t0.20139897947953658\n",
      "  (0, 4412)\t0.23134463284680223\n",
      "  (0, 3661)\t0.23134463284680223\n",
      "  (0, 7401)\t0.23134463284680223\n",
      "  (0, 1181)\t0.08558092058645371\n",
      "  (0, 8843)\t0.10730277619102119\n",
      "  (0, 2416)\t0.1116143514761565\n",
      "  (0, 4646)\t0.1450965750416578\n",
      "  (0, 8095)\t0.20139897947953658\n",
      "  (0, 8608)\t0.11287695621393898\n",
      "  (0, 81)\t0.1984242074516195\n",
      "  (0, 416)\t0.15756634623003665\n",
      "  (0, 3374)\t0.089810254157827\n",
      "  (0, 8052)\t0.04247238151178253\n",
      "  (0, 2854)\t0.17017153874932028\n",
      "  (0, 8823)\t0.08732692921302046\n",
      "  (0, 3464)\t0.19544045785402064\n",
      "  (0, 7972)\t0.15297808590666914\n",
      "  (0, 7929)\t0.07813897665337834\n",
      "  (0, 4034)\t0.10724130075428619\n",
      "  (0, 7976)\t0.043812815607210474\n",
      "  (0, 4684)\t0.07909149038735167\n",
      "  (0, 7311)\t0.11757287310167247\n",
      "  (0, 8845)\t0.17874596719915442\n",
      "  (0, 3968)\t0.08356630374440875\n",
      "  (0, 3177)\t0.05643847810696949\n",
      "  (0, 5760)\t0.16774958732203998\n",
      "  (0, 213)\t0.1397269231942697\n",
      "  (0, 469)\t0.13875128690835245\n",
      "  (0, 996)\t0.09249790939400998\n",
      "  (0, 5478)\t0.04754393191943527\n",
      "  (0, 4711)\t0.17874596719915442\n",
      "  (0, 3264)\t0.08269007462407735\n",
      "  (0, 6548)\t0.23134463284680223\n",
      "  (0, 7115)\t0.23823432597636854\n",
      "  (0, 8879)\t0.0718919145417484\n",
      "  (0, 1330)\t0.23134463284680223\n",
      "  (0, 8817)\t0.1397269231942697\n",
      "  (0, 5408)\t0.060384739968219366\n",
      "  (0, 4264)\t0.05319712753673744\n",
      "  (0, 4277)\t0.19017572767774107\n"
     ]
    }
   ],
   "source": [
    "print(train_text_bow.shape)\n",
    "print(test_text_bow.shape)\n",
    "print(type(train_text_bow))\n",
    "print(train_text_bow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model_svm = svm.SVC(kernel='linear')\n",
    "model_svm.fit(train_text_bow, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'POSITIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE',\n",
       "       'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'NEGATIVE', 'POSITIVE'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm.predict(test_text_bow[:10])\n",
    "#print(model_svm.score(test_text_bow, test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'POSITIVE',\n",
       "       'NEGATIVE', 'POSITIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_dec = DecisionTreeClassifier()\n",
    "model_dec.fit(train_text_bow, train_label)\n",
    "\n",
    "model_dec.predict(test_text_bow[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE',\n",
       "       'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'POSITIVE'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model_gnb = GaussianNB()\n",
    "model_gnb.fit(train_text_bow.toarray(), train_label)\n",
    "\n",
    "model_gnb.predict(test_text_bow[:10].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'POSITIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE',\n",
       "       'NEGATIVE', 'POSITIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_lg = LogisticRegression()\n",
    "model_lg.fit(train_text_bow, train_label)\n",
    "\n",
    "model_lg.predict(test_text_bow[:10].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076923076923077\n",
      "0.6538461538461539\n",
      "0.6610576923076923\n",
      "0.8052884615384616\n"
     ]
    }
   ],
   "source": [
    "#Mean Accuracy\n",
    "print(model_svm.score(test_text_bow, test_label))\n",
    "print(model_dec.score(test_text_bow, test_label))\n",
    "print(model_gnb.score(test_text_bow.toarray(), test_label))\n",
    "print(model_lg.score(test_text_bow, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80582524, 0.80952381])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(test_label, model_svm.predict(test_text_bow), average=None, labels = [Sentiment.POSITIVE, Sentiment.NEGATIVE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## try a whole new self-created review:)\n",
    "new_review =['This book looks soso like the content but the cover is weird',\n",
    "             'This book looks soso like the content and the cover is weird'\n",
    "            ]\n",
    "new_review_bow = tfidf_vec.transform(new_review)\n",
    "model_svm.predict(new_review_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': (1, 4, 8, 16, 32), 'kernel': ('linear', 'rbf')})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'kernel': ('linear', 'rbf'), 'C': (1,4,8,16,32)}\n",
    "\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(train_text_bow, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 4, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "sorted(clf.cv_results_.keys())\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8197115384615384\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(test_text_bow, test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import pickle\n",
    "\n",
    "# with open('../ml-sent-svm.pkl', 'wb') as f:\n",
    "#     pickle.dump(clf, f)\n",
    "# with open('../ml-sent-svm.pkl' 'rb') as f:\n",
    "#     loaded_svm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pkg_resources\n",
    "# import types\n",
    "# def get_imports():\n",
    "#     for name, val in globals().items():\n",
    "#         if isinstance(val, types.ModuleType):\n",
    "#             # Split ensures you get root package, \n",
    "#             # not just imported function\n",
    "#             name = val.__name__.split(\".\")[0]\n",
    "\n",
    "#         elif isinstance(val, type):\n",
    "#             name = val.__module__.split(\".\")[0]\n",
    "\n",
    "#         # Some packages are weird and have different\n",
    "#         # imported names vs. system/pip names. Unfortunately,\n",
    "#         # there is no systematic way to get pip names from\n",
    "#         # a package's imported name. You'll have to add\n",
    "#         # exceptions to this list manually!\n",
    "#         poorly_named_packages = {\n",
    "#             \"PIL\": \"Pillow\",\n",
    "#             \"sklearn\": \"scikit-learn\"\n",
    "#         }\n",
    "#         if name in poorly_named_packages.keys():\n",
    "#             name = poorly_named_packages[name]\n",
    "\n",
    "#         yield name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports = list(set(get_imports()))\n",
    "\n",
    "# # The only way I found to get the version of the root package\n",
    "# # from only the name of the package is to cross-check the names \n",
    "# # of installed packages vs. imported packages\n",
    "# requirements = []\n",
    "# for m in pkg_resources.working_set:\n",
    "#     if m.project_name in imports and m.project_name!=\"pip\":\n",
    "#         requirements.append((m.project_name, m.version))\n",
    "\n",
    "# for r in requirements:\n",
    "#     print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckiptagger",
   "language": "python",
   "name": "ckiptagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}