{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patterns and Constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<(DT)>)?(<(NN[^\\\\{\\\\}<>]*)>)+'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.chunk.regexp import tag_pattern2re_pattern\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "tag_pattern2re_pattern('<DT>?<NN.*>+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT article/NN)\n",
      "  looks/VBZ\n",
      "  (NP like/IN a/DT work/NN)\n",
      "  written/VBN\n",
      "  (NP by/IN a/DT foreigner/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "chunker = RegexpParser('''\n",
    "NP:\n",
    "    {<DT><NN.*><.*>*<NN.*>}\n",
    "    }<VB.*>{\n",
    "''')\n",
    "\n",
    "#sent = brown.tagged_sents()[10]\n",
    "#chunker.parse(sent)\n",
    "\n",
    "sent = 'The article looks like a work written by a foreigner.'\n",
    "sent = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "sent_ct = chunker.parse(sent)\n",
    "print(sent_ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('NP', [('The', 'DT'), ('article', 'NN')])"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "filenames": {
       "image/png": "/Users/Alvin/GoogleDrive/_MySyncDrive/Repository/python-notes/_build/jupyter_execute/corpus/patterns-constructions_3_0.png"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_ct.productions()\n",
    "#sent_ct.chomsky_normal_form()\n",
    "sent_ct\n",
    "sent_ct[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'method'>\n",
      "(NP The/DT article/NN)\n",
      "<class 'method'>\n",
      "<class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nltk.tree.Tree"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(sent_ct[0].label))\n",
    "print(sent_ct[0])\n",
    "print(type(sent_ct[0].leaves))\n",
    "print(type(sent_ct[1]))\n",
    "type(sent_ct[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "label: S\n",
      "(S\n",
      "  (NP The/DT article/NN)\n",
      "  looks/VBZ\n",
      "  (NP like/IN a/DT work/NN)\n",
      "  written/VBN\n",
      "  (NP by/IN a/DT foreigner/NN)\n",
      "  ./.)\n",
      "2\n",
      "label: NP\n",
      "(NP The/DT article/NN)\n",
      "3\n",
      "label: NP\n",
      "(NP like/IN a/DT work/NN)\n",
      "4\n",
      "label: NP\n",
      "(NP by/IN a/DT foreigner/NN)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for subtree in sent_ct.subtrees():\n",
    "    i=i+1\n",
    "    print(str(i))\n",
    "    print('label: {}'.format(subtree.label()))\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(S\\n  (NP The/DT article/NN)\\n  looks/VBZ\\n  (NP like/IN a/DT work/NN)\\n  written/VBN\\n  (NP by/IN a/DT foreigner/NN)\\n  ./.)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sent_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP The/DT article/NN)\n",
      "(NP like/IN a/DT work/NN)\n",
      "(NP by/IN a/DT foreigner/NN)\n"
     ]
    }
   ],
   "source": [
    "for subtree in sent_ct.subtrees(filter=lambda t: t.label().endswith(\"NP\")):\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ADJ_AND_ADJ', [('outmoded', 'JJ'), ('or', 'CC'), ('inadequate', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('fair', 'JJ'), ('and', 'CC'), ('equitable', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('rural', 'JJ'), ('and', 'CC'), ('urban', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('junior', 'JJ'), ('or', 'CC'), ('senior', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('medical', 'JJ'), ('and', 'CC'), ('dental', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('ambitious', 'JJ'), ('and', 'CC'), ('costly', 'JJ')]), Tree('ADJ_AND_ADJ', [('medical', 'JJ'), ('and', 'CC'), ('dental', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('medical', 'JJ'), ('and', 'CC'), ('dental', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('medical', 'JJ'), ('and', 'CC'), ('dental', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('tentative', 'JJ'), ('and', 'CC'), ('exploratory', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('military', 'JJ'), ('and', 'CC'), ('economic', 'JJ')])]\n",
      "[Tree('ADJ_AND_ADJ', [('firmer', 'JJR'), ('and', 'CC'), ('tougher', 'JJR')])]\n",
      "[Tree('ADJ_AND_ADJ', [('amateurish', 'JJ'), ('and', 'CC'), ('monumental', 'JJ')])]\n"
     ]
    }
   ],
   "source": [
    "# write chunk rules\n",
    "pat_chunker = RegexpParser('''\n",
    "ADJ_AND_ADJ:\n",
    "    {<JJ.*><CC><JJ.*>}\n",
    "''')\n",
    "\n",
    "for sent in brown.tagged_sents()[:500]:\n",
    "    cur_t = pat_chunker.parse(sent)\n",
    "    cur_pat = [pat for pat in cur_t.subtrees(filter=lambda t: t.label().startswith(\"ADJ_AND\"))]\n",
    "    if len(cur_pat)>0:\n",
    "        print(cur_pat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns from Raw-Text Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_sents = [ ' '.join(sent) \n",
    "               for sent in gutenberg.sents(fileids='carroll-alice.txt')\n",
    "              if len(sent)>=5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[ Alice ' s Adventures in Wonderland by Lewis Carroll 1865 ]\",\n",
       " 'Down the Rabbit - Hole',\n",
       " \"Alice was beginning to get very tired of sitting by her sister on the bank , and of having nothing to do : once or twice she had peeped into the book her sister was reading , but it had no pictures or conversations in it , ' and what is the use of a book ,' thought Alice ' without pictures or conversation ?'\",\n",
       " 'So she was considering in her own mind ( as well as she could , for the hot day made her feel very sleepy and stupid ), whether the pleasure of making a daisy - chain would be worth the trouble of getting up and picking the daisies , when suddenly a White Rabbit with pink eyes ran close by her .',\n",
       " \"There was nothing so VERY remarkable in that ; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself , ' Oh dear !\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "all_matches= [re.findall(r'(?:have|has)(?: [^s]+){0,2}[^\\s]+(?:en|ed)', sent) for sent in alice_sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['have wondered'], ['have been changed'], ['have been changed'], ['have dropped'], ['have changed'], ['have happened'], ['have liked'], ['have been changed'], ['have answered'], ['have got altered'], ['have called'], ['have to beat time when'], [\"have done that , you know ,' Alice gently remarked ; ' they ' d have been\"], ['have been'], ['have croqueted the Queen'], ['have everybody executed'], ['have any pepper in my kitchen'], ['have the experiment tried'], ['have been', 'have appeared'], ['have ordered'], ['have wanted'], ['have been'], ['have lived'], ['have no notion how delightful it will be When'], ['have baked'], ['have it explained'], ['have finished'], ['have you executed'], ['have you executed'], ['have you executed'], ['have been'], ['have imitated']]\n"
     ]
    }
   ],
   "source": [
    "print([m for m in all_matches if len(m)!=0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The grouping parenthsis changes the behavior of `re.findall()`\n",
    "- With parenthesis, the regex engine automatically captures the matches in all the groups and return the results as a tuple.\n",
    "- Use `(?:...)` to create non-capturing gorups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was beginning\n",
      "was reading\n"
     ]
    }
   ],
   "source": [
    "match = re.findall(r'(?:is|was) (?:\\w+ing)', '''\n",
    "Alice was beginning to get very tired of sitting by her sister on the bank , \n",
    "and of having nothing to do : once or twice she had peeped into the book \n",
    "her sister was reading , but it had no pictures or conversations in it , \n",
    "and what is the use of a book , thought Alice  without pictures or conversation ?\n",
    "''')\n",
    "\n",
    "\n",
    "if match:\n",
    "    for m in match:\n",
    "        print(m.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{important}\n",
    "It seems that when we use `re.findall()`, the matches returned would only be the capturing groups; but when we use `re.finditer()`, it would return the whole match strings as well as every section of the capturing groups.\n",
    "\n",
    "I would prefer `re.finditer()`. Otherwise, `re.findall()` may need to specify the non-capturing group `(?:...)`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was beginning\n",
      "was reading\n"
     ]
    }
   ],
   "source": [
    "pat_perfect = re.compile(r'(is|was) (\\w+ing)')\n",
    "text = '''\n",
    "Alice was beginning to get very tired of sitting by her sister on the bank , \n",
    "and of having nothing to do : once or twice she had peeped into the book \n",
    "her sister was reading , but it had no pictures or conversations in it , \n",
    "and what is the use of a book , thought Alice  without pictures or conversation ?\n",
    "'''\n",
    "\n",
    "pat_perfect_matches = pat_perfect.finditer(text)\n",
    "\n",
    "if pat_perfect_matches:\n",
    "    for m in pat_perfect_matches:\n",
    "        print(m.group())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckiptagger",
   "language": "python",
   "name": "ckiptagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}