{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Lingustics Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With `nltk`, we can easily implement quite a few corpus-linguistic methods\n",
    "    - Concordance Analysis (Simple Words)\n",
    "    - Frequency Lists\n",
    "    - Collocations\n",
    "    - Data Analysis with R\n",
    "    - Concordance Analysis (Patterns, Constructions?)\n",
    "        - Patterns on sentence strings\n",
    "        - Patterns on sentence word-tag strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Corpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.text import Text\n",
    "import pandas as pd\n",
    "\n",
    "brown_text = Text(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "- Documentation [nltk.collocations](https://www.nltk.org/howto/collocations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collocations based on Text\n",
    "brown_text.collocation_list()[:10]\n",
    "#brown_text.collocations()\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$10,000-per-year', 'French-born'),\n",
       " ('$79.89', 'nothing-down'),\n",
       " ('$8.50', 'tab'),\n",
       " (\"'low\", 'nigras'),\n",
       " ('0.5-mv./m.', '50-percent'),\n",
       " ('0.78', 'mEq'),\n",
       " ('1,100', 'circumscriptions'),\n",
       " ('1,257,700', 'non-farm'),\n",
       " ('11-inch', 'headroom'),\n",
       " ('11-shot', 'hammerless')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## bigram collocations based on different association measures\n",
    "finder.nbest(bigram_measures.likelihood_ratio,10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hong', 'Kong'),\n",
       " ('Viet', 'Nam'),\n",
       " ('Pathet', 'Lao'),\n",
       " ('Simms', 'Purdew'),\n",
       " ('El', 'Paso'),\n",
       " ('Lo', 'Shu'),\n",
       " ('Internal', 'Revenue'),\n",
       " ('Puerto', 'Rico'),\n",
       " ('Saxon', 'Shore'),\n",
       " ('carbon', 'tetrachloride')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply freq-based filers for bigram collocations\n",
    "finder.apply_freq_filter(10)\n",
    "\n",
    "## Apply word filer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_en = stopwords.words('english')\n",
    "finder.apply_word_filter(lambda x: not x.isalpha())\n",
    "\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('A', 'AT-NC'), ('charge', 'NN-NC')),\n",
       " (('A', 'FW-IN'), ('quibusdam', 'FW-WPO')),\n",
       " (('ACTH', 'NP'), ('Gel', 'NN-TL')),\n",
       " (('Abner', 'NP'), ('Haynes', 'NP')),\n",
       " (('Ace', 'NN-TL'), ('Driver', 'NN-TL')),\n",
       " (('Acoustical', 'JJ-HL'), ('interferometer', 'NN-HL')),\n",
       " (('Actively', 'RB-HL'), ('modernizing', 'VBG-HL')),\n",
       " (('Ad', 'FW-IN-TL'), ('Amicam', 'FW-NN-TL')),\n",
       " (('Adjusted', 'VBN-HL'), ('gross', 'JJ-HL')),\n",
       " (('Aerobacter', 'NP'), ('aerogenes', 'NP'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collocations based on tagged words\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    brown.tagged_words())\n",
    "finder.apply_word_filter(lambda x: not x[0].isalpha())\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X', 'X'),\n",
       " ('PRON', 'VERB'),\n",
       " ('PRT', 'VERB'),\n",
       " ('ADP', 'DET'),\n",
       " ('DET', 'ADJ'),\n",
       " ('ADJ', 'NOUN'),\n",
       " ('.', 'CONJ'),\n",
       " ('DET', 'NOUN'),\n",
       " ('VERB', 'PRT'),\n",
       " ('.', 'PRON')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collcoations based on tags only\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    t for w, t in brown.tagged_words(tagset='universal'))\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hong', 'Kong'),\n",
       " ('Viet', 'Nam'),\n",
       " ('Pathet', 'Lao'),\n",
       " ('Simms', 'Purdew'),\n",
       " ('El', 'Paso'),\n",
       " ('Lo', 'Shu'),\n",
       " ('Internal', 'Revenue'),\n",
       " ('Puerto', 'Rico'),\n",
       " ('Saxon', 'Shore'),\n",
       " ('carbon', 'tetrachloride')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create collocations with intervneing words (gapped n-grams)\n",
    "finder = BigramCollocationFinder.from_words(brown.words(), window_size=2)\n",
    "finder.apply_word_filter(lambda x: not x.isalpha())\n",
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 0.008288896237659233),\n",
       " (('in', 'the'), 0.004776126600941102),\n",
       " (('to', 'the'), 0.002950416468594341),\n",
       " (('on', 'the'), 0.0019781397047172215),\n",
       " (('and', 'the'), 0.001839489076741831),\n",
       " (('for', 'the'), 0.0015148226994329964),\n",
       " (('to', 'be'), 0.0014614292899021006),\n",
       " (('at', 'the'), 0.0012969431411859538),\n",
       " (('with', 'the'), 0.0012676628843464302),\n",
       " (('of', 'a'), 0.0012581898600748196)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finders\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "scored[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "How to get the document frequency of the bigrams???\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq = nltk.FreqDist(brown.words())\n",
    "bigram_freq = nltk.FreqDist('_'.join(x) for x in nltk.bigrams(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq_per_file = [nltk.FreqDist(words) \n",
    "                         for words in [brown.words(fileids=f) for f in brown.fileids()]]\n",
    "bigram_freq_per_file = [nltk.FreqDist('_'.join(x) for x in nltk.bigrams(words))\n",
    "                         for words in [brown.words(fileids=f) for f in brown.fileids()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get unigram dispersion\n",
    "def createUnigramDipsersionDist(uni_freq, uni_freq_per_file):\n",
    "    len(uni_freq_per_file)\n",
    "    unigram_dispersion = {}\n",
    "\n",
    "    for fid in uni_freq_per_file:\n",
    "        for w, f in fid.items():\n",
    "            if w in unigram_dispersion:\n",
    "                unigram_dispersion[w] += 1\n",
    "            else:\n",
    "                unigram_dispersion[w] = 1\n",
    "    return(unigram_dispersion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 500),\n",
       " ('Fulton', 3),\n",
       " ('County', 45),\n",
       " ('Grand', 17),\n",
       " ('Jury', 4),\n",
       " ('said', 314),\n",
       " ('Friday', 34),\n",
       " ('an', 498),\n",
       " ('investigation', 34),\n",
       " ('of', 500),\n",
       " (\"Atlanta's\", 2),\n",
       " ('recent', 114),\n",
       " ('primary', 59),\n",
       " ('election', 28),\n",
       " ('produced', 66),\n",
       " ('``', 462),\n",
       " ('no', 455),\n",
       " ('evidence', 119),\n",
       " (\"''\", 463),\n",
       " ('that', 500)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_dispersion = createUnigramDipsersionDist(unigram_freq, unigram_freq_per_file)\n",
    "# Dictionary cannot be sliced/subset\n",
    "# Get the items() and convert to list for subsetting\n",
    "list(unigram_dispersion.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The_Fulton', 1),\n",
       " ('Fulton_County', 6),\n",
       " ('County_Grand', 1),\n",
       " ('Grand_Jury', 2),\n",
       " ('Jury_said', 1),\n",
       " ('said_Friday', 4),\n",
       " ('Friday_an', 1),\n",
       " ('an_investigation', 7),\n",
       " ('investigation_of', 15),\n",
       " (\"of_Atlanta's\", 1),\n",
       " (\"Atlanta's_recent\", 1),\n",
       " ('recent_primary', 1),\n",
       " ('primary_election', 2),\n",
       " ('election_produced', 1),\n",
       " ('produced_``', 1),\n",
       " ('``_no', 6),\n",
       " ('no_evidence', 14),\n",
       " (\"evidence_''\", 3),\n",
       " (\"''_that\", 16),\n",
       " ('that_any', 31)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dict(sorted(bigram_freq.items()[:3]))\n",
    "list(bigram_freq.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The_Fulton', 1),\n",
       " ('Fulton_County', 1),\n",
       " ('County_Grand', 1),\n",
       " ('Grand_Jury', 2),\n",
       " ('Jury_said', 1),\n",
       " ('said_Friday', 3),\n",
       " ('Friday_an', 1),\n",
       " ('an_investigation', 7),\n",
       " ('investigation_of', 14),\n",
       " (\"of_Atlanta's\", 1),\n",
       " (\"Atlanta's_recent\", 1),\n",
       " ('recent_primary', 1),\n",
       " ('primary_election', 2),\n",
       " ('election_produced', 1),\n",
       " ('produced_``', 1),\n",
       " ('``_no', 6),\n",
       " ('no_evidence', 12),\n",
       " (\"evidence_''\", 3),\n",
       " (\"''_that\", 16),\n",
       " ('that_any', 30)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_dispersion = createUnigramDipsersionDist(bigram_freq, bigram_freq_per_file)\n",
    "list(bigram_dispersion.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unigram_freq)\n",
    "type(unigram_dispersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inherit BigramAssocMeasures\n",
    "class AugmentedBigramAssocMeasures(BigramAssocMeasures):\n",
    "    @classmethod\n",
    "    def dp_fwd(self, *marginals):\n",
    "        \"\"\"Scores bigrams using DP forward\n",
    "        This may be shown with respect to a contingency table::\n",
    "\n",
    "                w1    ~w1\n",
    "             ------ ------\n",
    "         w2 | n_ii | n_oi | = n_xi\n",
    "             ------ ------\n",
    "        ~w2 | n_io | n_oo |\n",
    "             ------ ------\n",
    "             = n_ix        TOTAL = n_xx\n",
    "        \"\"\"\n",
    "        \n",
    "        n_ii, n_io, n_oi, n_oo = self._contingency(*marginals)\n",
    "\n",
    "        return (n_ii/(n_ii+n_io)) - (n_oi/(n_oi+n_oo))\n",
    "\n",
    "    @classmethod\n",
    "    def dp_bwd(self, *marginals):\n",
    "        \"\"\"Scores bigrams using DP backward\n",
    "        This may be shown with respect to a contingency table::\n",
    "\n",
    "                w1    ~w1\n",
    "             ------ ------\n",
    "         w2 | n_ii | n_oi | = n_xi\n",
    "             ------ ------\n",
    "        ~w2 | n_io | n_oo |\n",
    "             ------ ------\n",
    "             = n_ix        TOTAL = n_xx\n",
    "        \"\"\"\n",
    "        \n",
    "        n_ii, n_io, n_oi, n_oo = self._contingency(*marginals)\n",
    "\n",
    "        return (n_ii/(n_ii+n_oi)) - (n_io/(n_io+n_oo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = AugmentedBigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Hong', 'Kong'), 1.0),\n",
       " (('Lo', 'Shu'), 1.0),\n",
       " (('Viet', 'Nam'), 0.9999974164190018),\n",
       " (('Puerto', 'Rico'), 0.9999974164012019),\n",
       " (('Los', 'Angeles'), 0.9999965551244676),\n",
       " (('Simms', 'Purdew'), 0.9999956940353778),\n",
       " (('El', 'Paso'), 0.9999913880855886),\n",
       " (('Blue', 'Throat'), 0.9999698582736025),\n",
       " (('Du', \"Pont's\"), 0.999955217852231),\n",
       " (('sweet', 'clover'), 0.9999552178136648)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_dpfwd = finder.score_ngrams(bigram_measures.dp_fwd)\n",
    "bigrams_dpfwd[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Hong', 'Kong'), 1.0),\n",
       " (('Lo', 'Shu'), 1.0),\n",
       " (('Pathet', 'Lao'), 0.9999939716599121),\n",
       " (('Export-Import', 'Bank'), 0.9999870820838838),\n",
       " ((\"Drug's\", 'chemical'), 0.999963829803725),\n",
       " (('text-form', 'list'), 0.9998966570270638),\n",
       " (('Inquirer', ':'), 0.9984653528870225),\n",
       " (('AP', ')'), 0.9978883513122019),\n",
       " (('accordance', 'with'), 0.9939784975869208),\n",
       " (('compatible', 'with'), 0.9939750735461291)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_dpbwd = finder.score_ngrams(bigram_measures.dp_bwd)\n",
    "bigrams_dpbwd[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 569 matches:\n",
      "will deliver tomorrow night to the American people over nationwide television \n",
      "ocial security taxes on 70 million American workers would be raised to pay the\n",
      "o retired as vice president of the American Screw Co. in 1955 said , `` Both p\n",
      "wice elected overwhelmingly by the American people as president of the United \n",
      "n example : Last month in Ghana an American missionary discovered when he came\n"
     ]
    }
   ],
   "source": [
    "## Simple Concordances\n",
    "brown_text.concordance('American', width=79, lines = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.app.concordance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' .\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Regular Expression Concordances\n",
    "import re\n",
    "sents = [' '.join(s) for s in brown.sents()]\n",
    "regex_1 = r'(is|was) \\w+ing'\n",
    "targets = [sent for sent in sents[:100] if re.search(regex_1, sent)]\n",
    "targets[0]\n",
    "#if targets:\n",
    "#    for match in targets:\n",
    "#        print(match.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## word frequencies\n",
    "brown_fd_words = nltk.FreqDist(brown.words())\n",
    "brown_fd_words.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nouns freq\n",
    "brown_fd_nouns = nltk.FreqDist([w.lower() for w,t in brown.tagged_words() \n",
    "                                 if any (noun_tag in t for noun_tag in ['NP','NN'])])\n",
    "brown_fd_nouns.most_common(10)\n",
    "\n",
    "brown_fd_nouns_df = pd.DataFrame(brown_fd_nouns.items(), columns=['word','freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>time</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>man</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>af</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>years</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>way</td>\n",
       "      <td>899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>events</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>james</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>officer</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>test</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3644</th>\n",
       "      <td>trees</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  freq\n",
       "243      time  1597\n",
       "174       man  1203\n",
       "5114       af   995\n",
       "248     years   949\n",
       "779       way   899\n",
       "...       ...   ...\n",
       "1030   events   101\n",
       "204     james   101\n",
       "1723  officer   101\n",
       "272      test   101\n",
       "3644    trees   101\n",
       "\n",
       "[433 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_fd_nouns_df[brown_fd_nouns_df['freq']>100].sort_values([\"freq\",\"word\"],ascending=[False,True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "We can also pass the data frame to R for data exploration.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " freq"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "243 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1597"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "174 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         man"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1203"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5114"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          af"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  995"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "248 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       years"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  949"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "779 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         way"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  899"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "486 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      people"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  845"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1011"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mr."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  844"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "63  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       state"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  787"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1099"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       world"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  787"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1227"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         men"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  763"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1438"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        life"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  715"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "303 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         day"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  687"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "175 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        year"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  656"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "875 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      states"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  586"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "278 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        work"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  583"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "299 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       house"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  582"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "158 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        mrs."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  534"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "865 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        part"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  496"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       place"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  496"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "340 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      school"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  489"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "32  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      number"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  470"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1801"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      course"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  465"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1173"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         war"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  463"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "101 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        fact"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  447"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "590 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       water"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  444"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1343"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        hand"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  423"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "896 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  government"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  418"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "229 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      system"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  416"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "121 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       night"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  411"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1217"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        head"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  407"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1869"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        eyes"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  401"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "756 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    business"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  393"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        city"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  393"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "72  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     program"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  388"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "525 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       group"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  386"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "371 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        days"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  384"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "819 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        room"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  383"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "656 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   president"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  382"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1001"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        side"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  375"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "39  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         end"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  369"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1246"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       point"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  369"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1254"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      things"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  368"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "212 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        john"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  362"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1061"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  361"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "701 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        case"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  360"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "354 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       order"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  359"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "459 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    children"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  355"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "356 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      church"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  348"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1108"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       power"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  340"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "595 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " development"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  333"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%R -i brown_fd_nouns_df\n",
    "\n",
    "library(dplyr)\n",
    "brown_fd_nouns_df %>%\n",
    "filter(freq > 100) %>%\n",
    "arrange(desc(freq), word) %>% \n",
    "head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Frequency List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NOUN': 5, 'VERB': 1})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Word by POS Frequency Distribution\n",
    "\n",
    "brown_news_tagged_words = brown.tagged_words(categories='news', tagset='universal')\n",
    "brown_news_cfd = nltk.ConditionalFreqDist(brown_news_tagged_words)\n",
    "brown_news_cfd['yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 732),\n",
       " ('was', 717),\n",
       " ('be', 526),\n",
       " ('said', 402),\n",
       " ('will', 388),\n",
       " ('are', 328),\n",
       " ('has', 300),\n",
       " ('had', 279),\n",
       " ('have', 265),\n",
       " ('were', 252)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## POS by Word Frequency Distribution\n",
    "brown_news_cfd2 = nltk.ConditionalFreqDist([(t, w) for (w, t) in brown_news_tagged_words])\n",
    "brown_news_cfd2['VERB'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word by Genre Frequency Distribution\n",
    "brown_genre_cfd = nltk.ConditionalFreqDist(\n",
    "    (word, genre)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'belles_lettres': 6, 'fiction': 4, 'lore': 3, 'religion': 3, 'romance': 3, 'learned': 2, 'reviews': 2, 'adventure': 1, 'humor': 1, 'science_fiction': 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_genre_cfd.conditions()[:50]\n",
    "brown_genre_cfd['mysterious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('belles_lettres', 6), ('fiction', 4), ('lore', 3), ('religion', 3), ('romance', 3), ('learned', 2), ('reviews', 2), ('adventure', 1), ('humor', 1), ('science_fiction', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(brown_genre_cfd['mysterious'].items(),key=lambda x:x[1],reverse=True)) # with freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Genre by Word Frequency Distribution\n",
    "brown_genre_cdf2 = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Genre by Word Frequency Distribution\n",
    "brown_genre_cdf2 = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           the   of  and   to    a   in that   is  was  for \n",
      "adventure 3370 1322 1622 1309 1354  847  494   98  914  331 \n",
      "editorial 3508 1976 1302 1554 1095 1001  578  744  308  509 \n",
      "  fiction 3423 1419 1696 1489 1281  916  530  144 1082  392 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_n_word = [word for (word, freq) in brown_fd_words.most_common(20) if word[0].isalpha()]\n",
    "\n",
    "brown_genre_cdf2.tabulate(conditions=['adventure','editorial','fiction'],\n",
    "                         samples=top_n_word[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_word2 = [word for (word, tag) in brown.tagged_words(tagset='universal') \n",
    "               if tag.startswith('NOUN')]\n",
    "top_n_word2_fd = nltk.FreqDist(top_n_word2).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('time', 1555), ('man', 1148), ('Af', 994), ('years', 942), ('way', 883), ('Mr.', 844), ('people', 809), ('men', 736), ('world', 684), ('life', 676)]\n",
      "            time    man     Af  years    way    Mr. people    men  world   life \n",
      "adventure    127    165      0     32     65     22     24     81     15     29 \n",
      "editorial     72     56      0     63     43    110     75     38     66     35 \n",
      "  fiction     99    111      0     44     62     39     39     72     24     44 \n"
     ]
    }
   ],
   "source": [
    "print(top_n_word2_fd)\n",
    "brown_genre_cdf2.tabulate(conditions=['adventure','editorial','fiction'],\n",
    "                         samples=[w for (w, f) in top_n_word2_fd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}