{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.1 s, sys: 593 ms, total: 3.7 s\n",
      "Wall time: 9.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "## Data Import and Preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import text_normalizer as tn\n",
    "#import model_evaluation_utils as meu\n",
    "import nltk\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "\n",
    "dataset = pd.read_csv('../data/movie_reviews.csv')\n",
    "# take a peek at the data\n",
    "print(dataset.head())\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "type(reviews)\n",
    "reviews.shape\n",
    "sentiments.shape\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]\n",
    "\n",
    "## Processing is ignored\n",
    "\n",
    "norm_train_reviews = train_reviews\n",
    "norm_test_reviews = test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
      "Sample test label transformation:\n",
      "----------------------------------- \n",
      "Actual Labels: ['negative' 'positive' 'negative'] \n",
      "Encoded Labels: [0 1 0] \n",
      "One hot encoded Labels:\n",
      " [[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "le = LabelEncoder()\n",
    "num_classes=2 \n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = [tokenizer.tokenize(text)\n",
    "                   for text in norm_train_reviews]\n",
    "y_tr = le.fit_transform(train_sentiments)\n",
    "y_train = keras.utils.to_categorical(y_tr, num_classes)\n",
    "\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [tokenizer.tokenize(text)\n",
    "                   for text in norm_test_reviews]\n",
    "y_ts = le.fit_transform(test_sentiments)\n",
    "y_test = keras.utils.to_categorical(y_ts, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# print class label encoding map and encoded labels\n",
    "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print('Sample test label transformation:\\n'+'-'*35,\n",
    "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_ts[:3], \n",
    "      '\\nOne hot encoded Labels:\\n', y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# build word2vec model\n",
    "w2v_num_features = 512\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, \n",
    "                                   size=w2v_num_features, window=150,\n",
    "                                   min_count=10, sample=1e-3, workers=16)    \n",
    "\n",
    "## takes 5mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This model uses the document word vector averaging scheme\n",
    "## Use the average word vector representations to represent one document (movie reivew)\n",
    "\n",
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                                     num_features=w2v_num_features)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                                    num_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tn' is not defined"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # Use the 300-dimensional word vectors trained on the Common Crawl using the GloVe model\n",
    "# # Provided by spaCy\n",
    "\n",
    "# import spacy\n",
    "# #nlp = spacy.load('en', parse=False, tag=False, entity=False)\n",
    "# nlp_vec = spacy.load('en_vectors_web_lg', parse=False, tag=False, entity=False)\n",
    "\n",
    "# ## feature engineering with GloVe model\n",
    "# train_nlp = [nlp_vec(item) for item in norm_train_reviews]\n",
    "# train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "\n",
    "# test_nlp = [nlp_vec(item) for item in norm_test_reviews]\n",
    "# test_glove_features = np.array([item.vector for item in test_nlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\n",
    "# print('GloVe model:> Train features shape:', train_glove_features.shape, ' Test features shape:', test_glove_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model\n",
    "\n",
    "- A simple fully-connected 4 layer deep neural network\n",
    "    - input layer (not counted as one layer), i.e., the word embedding layer\n",
    "    - three dense hidden layers (with 512 neurons)\n",
    "    - one output layer (with 2 neurons for classification)\n",
    "- (aka. multi-layered perceptron or deep ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, input_shape=(num_input_features,), kernel_initializer='glorot_uniform'))\n",
    "    dnn_model.add(BatchNormalization()) # improve  stability of the network.\n",
    "    dnn_model.add(Activation('relu')) # relu better than sigmoid, to present vanishing gradient problem\n",
    "    dnn_model.add(Dropout(0.2)) # prevents overfitting\n",
    "    \n",
    "    dnn_model.add(Dense(512, kernel_initializer='glorot_uniform'))\n",
    "    dnn_model.add(BatchNormalization())\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(512, kernel_initializer='glorot_uniform'))\n",
    "    dnn_model.add(BatchNormalization())\n",
    "    dnn_model.add(Activation('relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(2))\n",
    "    dnn_model.add(Activation('softmax'))\n",
    "\n",
    "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Visualization\n",
    "\n",
    "- To make this work, install `pip3 install pydot`\n",
    "- and also install `!brew install graphviz` in terminal for mac\n",
    "    - that is, install [graphvis](https://graphviz.gitlab.io/download/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not working yet. Had a problem with the installation of graphviz on mac\n",
    "\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(w2v_dnn, show_shapes=True, show_layer_names=False, \n",
    "#                  rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting using self-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "315/315 [==============================] - 3s 10ms/step - loss: 0.3763 - accuracy: 0.8397 - val_loss: 0.3127 - val_accuracy: 0.8640\n",
      "Epoch 2/10\n",
      "315/315 [==============================] - 3s 9ms/step - loss: 0.3054 - accuracy: 0.8719 - val_loss: 0.3108 - val_accuracy: 0.8757\n",
      "Epoch 3/10\n",
      "315/315 [==============================] - 3s 9ms/step - loss: 0.2948 - accuracy: 0.8776 - val_loss: 0.3133 - val_accuracy: 0.8651\n",
      "Epoch 4/10\n",
      "315/315 [==============================] - 3s 9ms/step - loss: 0.2837 - accuracy: 0.8811 - val_loss: 0.3099 - val_accuracy: 0.8706\n",
      "Epoch 5/10\n",
      "315/315 [==============================] - 3s 10ms/step - loss: 0.2747 - accuracy: 0.8857 - val_loss: 0.3048 - val_accuracy: 0.8763\n",
      "Epoch 6/10\n",
      "315/315 [==============================] - 3s 10ms/step - loss: 0.2712 - accuracy: 0.8858 - val_loss: 0.3337 - val_accuracy: 0.8606\n",
      "Epoch 7/10\n",
      "315/315 [==============================] - 3s 10ms/step - loss: 0.2624 - accuracy: 0.8906 - val_loss: 0.3099 - val_accuracy: 0.8737\n",
      "Epoch 8/10\n",
      "315/315 [==============================] - 3s 10ms/step - loss: 0.2583 - accuracy: 0.8914 - val_loss: 0.3321 - val_accuracy: 0.8700\n",
      "Epoch 9/10\n",
      "315/315 [==============================] - 3s 10ms/step - loss: 0.2495 - accuracy: 0.8949 - val_loss: 0.3111 - val_accuracy: 0.8711\n",
      "Epoch 10/10\n",
      "315/315 [==============================] - 3s 9ms/step - loss: 0.2397 - accuracy: 0.9003 - val_loss: 0.3370 - val_accuracy: 0.8734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fae04772a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
    "            shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-bf19a67cc778>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
    "predictions = le.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions from Text Analytics with Python book\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        4))\n",
    "    print('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
    "    \n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n",
    "                                  labels=classes)\n",
    "    cm_frame = pd.DataFrame(data=cm, \n",
    "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
    "                                                  codes=level_labels), \n",
    "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
    "                                                codes=level_labels)) \n",
    "    print(cm_frame) \n",
    "def display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n",
    "\n",
    "    report = metrics.classification_report(y_true=true_labels, \n",
    "                                           y_pred=predicted_labels, \n",
    "                                           labels=classes) \n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "    \n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n",
    "    print('Model Performance metrics:')\n",
    "    print('-'*30)\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    print('\\nModel Classification report:')\n",
    "    print('-'*30)\n",
    "    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n",
    "                                  classes=classes)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*30)\n",
    "    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n",
    "                             classes=classes)\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8717\n",
      "Precision: 0.8719\n",
      "Recall: 0.8717\n",
      "F1 Score: 0.8717\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.86      0.88      0.87      7510\n",
      "    negative       0.88      0.86      0.87      7490\n",
      "\n",
      "    accuracy                           0.87     15000\n",
      "   macro avg       0.87      0.87      0.87     15000\n",
      "weighted avg       0.87      0.87      0.87     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       6628      882\n",
      "        negative       1042     6448\n"
     ]
    }
   ],
   "source": [
    "display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                      classes=['positive', 'negative'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting using pre-trained word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_dnn = construct_deepnn_architecture(num_input_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "# glove_dnn.fit(train_glove_features, y_train, epochs=10, batch_size=batch_size, \n",
    "#               shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = glove_dnn.predict_classes(test_glove_features)\n",
    "# predictions = le.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "#                                       classes=['positive', 'negative'])  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-tensorflow",
   "language": "python",
   "name": "r-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}