#!/usr/bin/env python
# coding: utf-8

# # Sentiment Analysis with LSTM

# In[1]:


import numpy as np


# ## Loading Data

# In[2]:


# Deep Learn with Python Version

## Loading data
from keras.datasets import imdb
from keras.preprocessing import sequence

max_features = 10000 # vocab size
max_len = 500 # text length to consider
batch_size = 128

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=max_features)


# In[3]:


print(train_data[0]) # first train doc
print(train_labels[0]) # first train label


# In[4]:


# check class
print(type(train_data))
print(type(train_labels))
# check dtype
print(train_data.dtype)
print(train_labels.dtype)

# check shape
print(train_data.shape)
print(test_data.shape)


# In[5]:


## vectorize labels
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
print(type(y_train))
print(y_train.dtype)


# ## Padding Text Length

# In[6]:


## Padding Texts Lengths
train_data = sequence.pad_sequences(train_data, maxlen=max_len)
test_data = sequence.pad_sequences(test_data, maxlen=max_len)
print(train_data.shape)
print(test_data.shape)


# ## Model Defining

# In[24]:


## Model Building

from keras.models import Sequential
from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D
from keras.layers import LSTM

EMBEDDING_DIM = 128 # dimension for dense embeddings for each token
LSTM_DIM = 64 # total LSTM units


model = Sequential()
model.add(Embedding(input_dim=max_features, output_dim=EMBEDDING_DIM, input_length=max_len))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation="sigmoid"))

model.compile(loss="binary_crossentropy", optimizer="adam",
              metrics=["accuracy"])


# ## Model Fitting

# In[25]:


# ## Model fitting
# history = model.fit(train_data, train_labels,
#                    epochs=10,
#                    batch_size= 128,
#                    validation_split=0.2)

## Comment out to skip


# ## Model Evaluation

# In[17]:


## Plotting results
def plot(history):
    import matplotlib.pyplot as plt
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(acc)+1)
    ## Accuracy plot
    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    ## Loss plot
    plt.figure()

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()
    


# In[1]:


# plot(history)


# ## Model Saving

# In[25]:


# model.save('../data/sent-analysis-lstm-v1.h5')
import keras
model = keras.models.load_model('../data/sent-analysis-lstm-v1.h5')


# ## Model Prediction

# In[26]:


## Prediction and model performance
pred_test = (model.predict(test_data) > 0.5).astype("int32")


# In[27]:


print(pred_test[:10,:])


# In[28]:


# functions from Text Analytics with Python book
def get_metrics(true_labels, predicted_labels):
    
    print('Accuracy:', np.round(
                        metrics.accuracy_score(true_labels, 
                                               predicted_labels),
                        4))
    print('Precision:', np.round(
                        metrics.precision_score(true_labels, 
                                               predicted_labels,
                                               average='weighted'),
                        4))
    print('Recall:', np.round(
                        metrics.recall_score(true_labels, 
                                               predicted_labels,
                                               average='weighted'),
                        4))
    print('F1 Score:', np.round(
                        metrics.f1_score(true_labels, 
                                               predicted_labels,
                                               average='weighted'),
                        4))

def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):
    
    total_classes = len(classes)
    level_labels = [total_classes*[0], list(range(total_classes))]

    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, 
                                  labels=classes)
    cm_frame = pd.DataFrame(data=cm, 
                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], 
                                                  codes=level_labels), 
                            index=pd.MultiIndex(levels=[['Actual:'], classes], 
                                                codes=level_labels)) 
    print(cm_frame) 
def display_classification_report(true_labels, predicted_labels, classes=[1,0]):

    report = metrics.classification_report(y_true=true_labels, 
                                           y_pred=predicted_labels, 
                                           labels=classes) 
    print(report)
    
    
    
def display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):
    print('Model Performance metrics:')
    print('-'*30)
    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)
    print('\nModel Classification report:')
    print('-'*30)
    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, 
                                  classes=classes)
    print('\nPrediction Confusion Matrix:')
    print('-'*30)
    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, 
                             classes=classes)
from sklearn import metrics
import pandas as pd


# In[29]:


display_model_performance_metrics(test_labels, pred_test.flatten(), classes=[0,1])

