{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sSBsVXBpwvc"
   },
   "source": [
    "# Neural Language Model of Chinese\n",
    "\n",
    "- [How to development a word-level neural language model in keras](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/)\n",
    "- Chinese texts\n",
    "- Word-based neural language model based on:\n",
    "    - character sequences\n",
    "    - word sequences\n",
    "- Use two novels by Jing-Yong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 29621,
     "status": "ok",
     "timestamp": 1603918207787,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "wtgcPXVLp7im",
    "outputId": "dc092527-8553-47a7-8e94-a36c5af1cb26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 31489,
     "status": "ok",
     "timestamp": 1603918209661,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "kVunfvneqFnc"
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.insert(1, '/content/drive/My Drive/_MySyncDrive/Repository/python-notes/nlp')\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/_MySyncDrive/Repository/python-notes/nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 36687,
     "status": "ok",
     "timestamp": 1603918214863,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "YmWy6y8Epwvd"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import text_normalizer_zh as tn\n",
    "import re\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc_paras(doc):\n",
    "    # get content paragraphs only\n",
    "    paras = [p for p in doc.split(sep=\"\\n\")[7:] if p.startswith('  ')]\n",
    "    paras = [tn.remove_symbols(p) for p in paras]\n",
    "    paras = [tn.remove_extra_spaces(p) for p in paras]\n",
    "    return paras\n",
    "\n",
    "def clean_doc_lines(doc):\n",
    "    # get content paragraphs only\n",
    "    paras = [p for p in re.split(\"[\\n，。]\", doc)[7:] if p.startswith('  ')]\n",
    "    paras = [tn.remove_symbols(p) for p in paras]\n",
    "    paras = [tn.remove_extra_spaces(p) for p in paras]\n",
    "    return paras\n",
    "\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 36684,
     "status": "ok",
     "timestamp": 1603918214864,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "ng3yUAjipwvg",
    "outputId": "bc6d7545-6c04-4419-a43c-c8102f6cd560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "『金庸作品集/作者:金庸』\n",
      "『狀態:已完結』\n",
      "『內容簡介:\n",
      "    金庸的所有的書&lt;/p&gt;\n",
      "』\n",
      "愛下電子書Txt版閱讀,下載和分享更多電子書請訪問:http://www.ixdzs.com,手機訪問:http://m.ixdzs.com,E-mail:support@ixdzs.com\n",
      "------章節內容開始-------\n",
      "天龍八部\n",
      "第一章 青衫磊落險峰行\n",
      "    青光閃動，一柄青鋼\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = \"../../../RepositoryData/data/jingyong-part-cht-utf8.txt\"\n",
    "doc=load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 36974,
     "status": "ok",
     "timestamp": 1603918215158,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "RoxSROTupwvj",
    "outputId": "b5ca9b7a-f95b-4646-b8bf-7e3f451265ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['青光閃動', '兩人劍法迅捷', '練武廳東坐著二人', '眼見那少年與中年漢子已拆到七十余招', '便在這時', '那長須老者滿臉得色', '這老者姓左', '無量劍原分東北西三宗', '西首錦凳上所坐的則是別派人士', '當下左子穆笑道辛師妹今年派出的四名弟子', '馬五德臉上微微一紅', '左子穆心想他若是你弟子', '那姓段青年微笑道在下單名一譽字', '馬五德和段譽也是初交', '左子穆道段兄既然不是馬五哥的好朋友', '那中年漢子龔光杰巴不得師父有這句話', '段譽輕揮折扇', '他這番說什麼你師父我師父的', '龔光杰大踏步過來', '段譽道你這位大爺怎地如此狠霸霸的我平生最不愛瞧人打架']\n",
      "Total Paragraphs: 34031\n",
      "Unique Tokens: 3388\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "paras = clean_doc_lines(doc)\n",
    "print(paras[:20])\n",
    "print('Total Paragraphs: %d' % len(paras))\n",
    "print('Unique Tokens: %d' % len(set(''.join(paras))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToWMwS1opwvl"
   },
   "source": [
    "## Line-based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 38820,
     "status": "ok",
     "timestamp": 1603918217009,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "GDmy9FZepwvm"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnYO3OCCpwvo"
   },
   "source": [
    "### Text to Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 38817,
     "status": "ok",
     "timestamp": 1603918217009,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "i9dOWLljpwvp",
    "outputId": "f463b22b-046c-4ce0-c98c-25a612c90750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3391\n",
      "Total Sequences: 222346\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "data = '\\n'.join(paras)  # collapse the entire corpus into one string\n",
    "# prepare the tokenizer on the source text\n",
    "tokenizer = Tokenizer(\n",
    "    oov_token=1, char_level=True\n",
    ")  ## specify the word id for unknown words + char_level tokenizer\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# determine the vocabulary size\n",
    "## zero index is reserved in keras as the padding token (+1) and one unknown word id\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# create paragraph-based sequences\n",
    "sequences = list()\n",
    "for line in data.split('\\n'):\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
    "    ## For each line, after converting words into indexes\n",
    "    ## prepare sequences for training\n",
    "    ## given a line, w1,w2,w3,w4\n",
    "    ## create input sequences:\n",
    "    ## w1,w2\n",
    "    ## w1,w2,w3\n",
    "    ## w1,w2,w3,w4\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i + 1]\n",
    "        sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GtgSyE4pwvr"
   },
   "source": [
    "### Word ID to Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 38814,
     "status": "ok",
     "timestamp": 1603918217010,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "74gBA-YZpwvs"
   },
   "outputs": [],
   "source": [
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "executionInfo": {
     "elapsed": 38811,
     "status": "ok",
     "timestamp": 1603918217011,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "BhoASBn5pwvu",
    "outputId": "d8c0193f-598c-4368-b458-ec2df7d0566c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[256, 105],\n",
       " [256, 105, 506],\n",
       " [256, 105, 506, 195],\n",
       " [82, 8],\n",
       " [82, 8, 34],\n",
       " [82, 8, 34, 90],\n",
       " [82, 8, 34, 90, 913],\n",
       " [82, 8, 34, 90, 913, 1650],\n",
       " [437, 129],\n",
       " [437, 129, 796]]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "executionInfo": {
     "elapsed": 38807,
     "status": "ok",
     "timestamp": 1603918217011,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "njqEC5Ztpwvx",
    "outputId": "c8c23665-ae02-4eee-adbd-c88a1798caac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['青', '光']\n",
      "['青', '光', '閃']\n",
      "['青', '光', '閃', '動']\n",
      "['兩', '人']\n",
      "['兩', '人', '劍']\n",
      "['兩', '人', '劍', '法']\n",
      "['兩', '人', '劍', '法', '迅']\n",
      "['兩', '人', '劍', '法', '迅', '捷']\n",
      "['練', '武']\n",
      "['練', '武', '廳']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(sequence_to_text(s)) for s in sequences[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFWPYTFmpwvz"
   },
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 39850,
     "status": "ok",
     "timestamp": 1603918218059,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "uqMwMVHIpwvz",
    "outputId": "a468ef07-5f37-40d5-cd1d-32a64e1985b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 133\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2mPNCwMpwv1"
   },
   "source": [
    "### Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 41762,
     "status": "ok",
     "timestamp": 1603918219975,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "t1UfTcHipwv2"
   },
   "outputs": [],
   "source": [
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj8dzy9Kpwv4"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "executionInfo": {
     "elapsed": 47193,
     "status": "ok",
     "timestamp": 1603918225410,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "dPB0xHSQpwv4",
    "outputId": "01d40c48-9950-4210-96b8-16fb30a95806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 132, 128)          434048    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 132, 100)          91600     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3391)              342491    \n",
      "=================================================================\n",
      "Total params: 958,639\n",
      "Trainable params: 958,639\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_length-1))\n",
    "model.add(LSTM(100, return_sequences=True))  # LSTM 1\n",
    "model.add(LSTM(100))  # LSTM 2\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUHsVPb7pwv6"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 47193,
     "status": "ok",
     "timestamp": 1603918225411,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "gorozb0vpwv6"
   },
   "outputs": [],
   "source": [
    "# # fit network\n",
    "# model.fit(X, y, batch_size= 256, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v00pwFDtqWb"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 47187,
     "status": "ok",
     "timestamp": 1603918225411,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "LTZ90tn0tsV4"
   },
   "outputs": [],
   "source": [
    "# from pickle import dump\n",
    "# # save the model to file\n",
    "# model.save('jing-yong-line-lm-model.h5')\n",
    "# # save the tokenizer\n",
    "# dump(tokenizer, open('jing-yong-line-lm-tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKRB10EZ9-yF"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1446,
     "status": "ok",
     "timestamp": 1603918272877,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "WoPZbAgw98dP"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "model.load_weights('jing-yong-line-lm-model.h5')\n",
    "pickle_in = open('jing-yong-line-lm-tokenizer.pkl',\n",
    "                 \"rb\")\n",
    "tokenizer = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "620lhtkYt9cH"
   },
   "source": [
    "### Generate Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1441,
     "status": "ok",
     "timestamp": 1603918272878,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "R-61bUdkuQa9"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "executionInfo": {
     "elapsed": 4024,
     "status": "ok",
     "timestamp": 1603918321665,
     "user": {
      "displayName": "Alvin Chen",
      "photoUrl": "",
      "userId": "12962786962925949010"
     },
     "user_tz": -480
    },
    "id": "scEKEon-ubsl",
    "outputId": "f8ad8a85-34de-4e1a-b644-77e96cfd3ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "師父 師 姊 娘 我 段 正 結 交 視 那 儀 雖 尚 後 裡 當 年 子 師 父 段 譽 頭 子 引 兒 兒 滑 叛 得 不 過 身 子 道 你 你 什 了 身 子 李 延 慶 忽 袱 畔 起 了 我 呢 尚 穆 道 有 段 正 淳 嗎 朝 皇 此 也 羞 著 譚 婆 兒 子 我 沒 給 丐 姊 平 之 負 他 不 讓 徒 主 不 知 道 人 的 好 你 我 早 睛 兩 個 英 雄 母 殿 想 起\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "print(generate_seq(model, tokenizer, max_length-1, '師父', 100))\n",
    "# print(generate_seq(model, tokenizer, max_length-1, 'Jill', 4))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "neural-language-model-zh.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}