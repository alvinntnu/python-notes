{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_DATA_ROOT = \"../../../RepositoryData/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Efficient ways to summarize the semantics of massive collections of documents\n",
    "- Three general methods:\n",
    "    - Keyphrase extraction\n",
    "    - Topic modeling\n",
    "    - Document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyphrase Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N-grams\n",
    "- Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import text_normalizer as tn\n",
    "import nltk\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading corpus, Alice in the Wonderland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
    "# concatenate each word token of a sentence\n",
    "alice = [' '.join(ts) for ts in alice]\n",
    "# normalize text\n",
    "# `filter()` removes tokens that are False after normalization\n",
    "norm_alice = list(filter(None, \n",
    "                         tn.normalize_corpus(alice, text_lemmatization=False))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare raw texts vs, noramlized texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Alice ' s Adventures in Wonderland by Lewis Carroll 1865 ] \n",
      " alice adventures wonderland lewis carroll\n"
     ]
    }
   ],
   "source": [
    "print(alice[0], '\\n', norm_alice[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to create n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngrams(sequence, n):\n",
    "    return list(\n",
    "        zip(*(sequence[index:]\n",
    "               for index in range(n)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'B'), ('B', 'C'), ('C', 'D')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ngrams(['A','B','C','D'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip() \n",
    "                     for document in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "    \n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq) \n",
    "                     for text, freq in sorted_ngrams]\n",
    "\n",
    "    return sorted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said alice', 123),\n",
       " ('mock turtle', 56),\n",
       " ('march hare', 31),\n",
       " ('said king', 29),\n",
       " ('thought alice', 26),\n",
       " ('white rabbit', 22),\n",
       " ('said hatter', 22),\n",
       " ('said mock', 20),\n",
       " ('said caterpillar', 18),\n",
       " ('said gryphon', 18)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_ngrams(corpus=norm_alice, ngram_val=2,\n",
    "               limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said mock turtle', 20),\n",
       " ('said march hare', 10),\n",
       " ('poor little thing', 6),\n",
       " ('little golden key', 5),\n",
       " ('certainly said alice', 5),\n",
       " ('white kid gloves', 5),\n",
       " ('march hare said', 5),\n",
       " ('mock turtle said', 5),\n",
       " ('know said alice', 4),\n",
       " ('might well say', 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_ngrams(corpus=norm_alice, ngram_val=3,\n",
    "               limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations (Bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x7ffc66265b70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "\n",
    "finder = BigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in norm_alice])\n",
    "finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply frequency filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inherit BigramAssocMeasures\n",
    "class AugmentedBigramAssocMeasures(BigramAssocMeasures):\n",
    "    @classmethod\n",
    "    def dp_fwd(self, *marginals):\n",
    "        \"\"\"Scores bigrams using phi-square, the square of the Pearson correlation\n",
    "        coefficient.\n",
    "        This may be shown with respect to a contingency table::\n",
    "\n",
    "                w1    ~w1\n",
    "             ------ ------\n",
    "         w2 | n_ii | n_oi | = n_xi\n",
    "             ------ ------\n",
    "        ~w2 | n_io | n_oo |\n",
    "             ------ ------\n",
    "             = n_ix        TOTAL = n_xx\n",
    "        \"\"\"\n",
    "        \n",
    "        n_ii, n_io, n_oi, n_oo = self._contingency(*marginals)\n",
    "\n",
    "        return (n_ii/(n_ii+n_io)) - (n_oi/(n_oi+n_oo))\n",
    "\n",
    "    @classmethod\n",
    "    def dp_bwd(self, *marginals):\n",
    "        \"\"\"Scores bigrams using phi-square, the square of the Pearson correlation\n",
    "        coefficient.\n",
    "        This may be shown with respect to a contingency table::\n",
    "\n",
    "                w1    ~w1\n",
    "             ------ ------\n",
    "         w2 | n_ii | n_oi | = n_xi\n",
    "             ------ ------\n",
    "        ~w2 | n_io | n_oo |\n",
    "             ------ ------\n",
    "             = n_ix        TOTAL = n_xx\n",
    "        \"\"\"\n",
    "        \n",
    "        n_ii, n_io, n_oi, n_oo = self._contingency(*marginals)\n",
    "\n",
    "        return (n_ii/(n_ii+n_oi)) - (n_io/(n_io+n_oo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations based on raw frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'alice'),\n",
       " ('mock', 'turtle'),\n",
       " ('march', 'hare'),\n",
       " ('said', 'king'),\n",
       " ('thought', 'alice'),\n",
       " ('said', 'hatter'),\n",
       " ('white', 'rabbit'),\n",
       " ('said', 'mock'),\n",
       " ('said', 'caterpillar'),\n",
       " ('said', 'gryphon')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = AugmentedBigramAssocMeasures()                                                \n",
    "finder.nbest(bigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations based on PMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soo', 'oop'),\n",
       " ('beg', 'pardon'),\n",
       " ('bread', 'butter'),\n",
       " ('golden', 'key'),\n",
       " ('kid', 'gloves'),\n",
       " ('twinkle', 'twinkle'),\n",
       " ('evening', 'beautiful'),\n",
       " ('join', 'dance'),\n",
       " ('play', 'croquet'),\n",
       " ('set', 'work')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(bigram_measures.pmi, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('soo', 'oop'), 10.771961015522411),\n",
       " (('beg', 'pardon'), 10.579315937580013),\n",
       " (('bread', 'butter'), 10.186998514801253),\n",
       " (('golden', 'key'), 10.186998514801253),\n",
       " (('kid', 'gloves'), 10.119884318942718),\n",
       " (('twinkle', 'twinkle'), 9.901244032467378),\n",
       " (('evening', 'beautiful'), 9.878876219438924),\n",
       " (('join', 'dance'), 9.878876219438924),\n",
       " (('play', 'croquet'), 9.731319031025066),\n",
       " (('set', 'work'), 9.356923516243567)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_pmi=finder.score_ngrams(bigram_measures.pmi)\n",
    "bigrams_pmi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('soo', 'oop'), 1.0),\n",
       " (('beg', 'pardon'), 0.9998365211705085),\n",
       " (('march', 'hare'), 0.9997542796297814),\n",
       " (('white', 'kid'), 0.9979566816510013),\n",
       " (('mock', 'turtle'), 0.9491525423728814),\n",
       " (('set', 'work'), 0.749345977763244),\n",
       " (('three', 'gardeners'), 0.7482014388489209),\n",
       " (('little', 'golden'), 0.7042309444009763),\n",
       " (('join', 'dance'), 0.6923076923076923),\n",
       " (('bread', 'butter'), 0.6665849072030087)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_dpfwd=finder.score_ngrams(bigram_measures.dp_fwd)\n",
    "bigrams_dpfwd[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('soo', 'oop'), 1.0),\n",
       " (('mock', 'turtle'), 0.9997537754432042),\n",
       " (('join', 'dance'), 0.9996729621453683),\n",
       " (('kid', 'gloves'), 0.9995096035962403),\n",
       " (('evening', 'beautiful'), 0.9993461381283204),\n",
       " (('march', 'hare'), 0.9117647058823529),\n",
       " (('bread', 'butter'), 0.8568976188529854),\n",
       " (('golden', 'key'), 0.8568976188529854),\n",
       " (('trembling', 'voice'), 0.8298185384992643),\n",
       " (('beg', 'pardon'), 0.75)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_dpbwd=finder.score_ngrams(bigram_measures.dp_bwd)\n",
    "bigrams_dpbwd[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations (N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "\n",
    "finder = TrigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in norm_alice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrams based on raw frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'mock', 'turtle'),\n",
       " ('said', 'march', 'hare'),\n",
       " ('poor', 'little', 'thing'),\n",
       " ('little', 'golden', 'key'),\n",
       " ('march', 'hare', 'said'),\n",
       " ('mock', 'turtle', 'said'),\n",
       " ('white', 'kid', 'gloves'),\n",
       " ('beau', 'ootiful', 'soo'),\n",
       " ('certainly', 'said', 'alice'),\n",
       " ('might', 'well', 'say')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trigram_measures = TrigramAssocMeasures()                                                \n",
    "finder.nbest(trigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrams based on PMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accustomed', 'usurpation', 'conquest'),\n",
       " ('adjourn', 'immediate', 'adoption'),\n",
       " ('adoption', 'energetic', 'remedies'),\n",
       " ('ancient', 'modern', 'seaography'),\n",
       " ('apple', 'roast', 'turkey'),\n",
       " ('arithmetic', 'ambition', 'distraction'),\n",
       " ('brother', 'latin', 'grammar'),\n",
       " ('canvas', 'bag', 'tied'),\n",
       " ('cherry', 'tart', 'custard'),\n",
       " ('circle', 'exact', 'shape')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(trigram_measures.pmi, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Tag-based Phrase Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(DEMO_DATA_ROOT+'/elephants.txt', 'r+').readlines()\n",
    "sentences = nltk.sent_tokenize(data[0])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elephants are large mammals of the family Elephantidae and the order Proboscidea',\n",
       " 'Three species are currently recognised the African bush elephant Loxodonta africana the African forest elephant L cyclotis and the Asian elephant Elephas maximus',\n",
       " 'Elephants are scattered throughout subSaharan Africa South Asia and Southeast Asia']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_sentences = tn.normalize_corpus(sentences, text_lower_case=False, \n",
    "                                     text_stemming=False, text_lemmatization=False, stopword_removal=False)\n",
    "norm_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define chunk-based tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}', stopword_list=stopwords):\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        tagged_sents = [nltk.pos_tag(nltk.word_tokenize(sentence))]   \n",
    "        \n",
    "        chunks = [chunker.parse(tagged_sent) \n",
    "                      for tagged_sent in tagged_sents]\n",
    "        \n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                         for chunk in chunks]    \n",
    "        \n",
    "        flattened_chunks = list(\n",
    "                            itertools.chain.from_iterable(\n",
    "                                wtc_sent for wtc_sent in wtc_sents)\n",
    "                           )\n",
    "        \n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) \n",
    "                                   for status, chunk \n",
    "                                       in itertools.groupby(flattened_chunks, \n",
    "                                                lambda word_pos_chunk: word_pos_chunk[2] != 'O')]\n",
    "        \n",
    "        valid_chunks = [' '.join(word.lower() \n",
    "                                for word, tag, chunk in wtc_group \n",
    "                                    if word.lower() not in stopword_list) \n",
    "                                        for status, wtc_group in valid_chunks_tagged\n",
    "                                            if status]\n",
    "                                            \n",
    "        all_chunks.append(valid_chunks)\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get chunks from texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['elephants', 'large mammals', 'family elephantidae', 'order proboscidea'],\n",
       " ['species',\n",
       "  'african bush elephant loxodonta',\n",
       "  'african forest elephant l cyclotis',\n",
       "  'asian elephant elephas maximus'],\n",
       " ['elephants', 'subsaharan africa south asia', 'southeast asia'],\n",
       " ['elephantidae',\n",
       "  'family',\n",
       "  'order proboscidea',\n",
       "  'extinct members',\n",
       "  'order',\n",
       "  'deinotheres gomphotheres mammoths',\n",
       "  'mastodons'],\n",
       " ['elephants',\n",
       "  'several distinctive features',\n",
       "  'long trunk',\n",
       "  'proboscis',\n",
       "  'many purposes',\n",
       "  'water',\n",
       "  'grasping objects'],\n",
       " ['incisors', 'tusks', 'weapons', 'tools', 'objects'],\n",
       " ['elephants', 'flaps', 'body temperature'],\n",
       " ['pillarlike legs', 'great weight'],\n",
       " ['african elephants',\n",
       "  'ears',\n",
       "  'backs',\n",
       "  'asian elephants',\n",
       "  'ears',\n",
       "  'convex',\n",
       "  'level backs'],\n",
       " ['elephants', 'different habitats', 'savannahs forests deserts', 'marshes'],\n",
       " ['water'],\n",
       " ['keystone species', 'impact', 'environments'],\n",
       " ['animals',\n",
       "  'distance',\n",
       "  'elephants',\n",
       "  'predators',\n",
       "  'lions tigers hyenas',\n",
       "  'wild dogs',\n",
       "  'young elephants',\n",
       "  'calves'],\n",
       " ['elephants', 'fissionfusion society', 'multiple family groups'],\n",
       " ['females cows',\n",
       "  'family groups',\n",
       "  'female',\n",
       "  'calves',\n",
       "  'several related females'],\n",
       " ['groups', 'individual known', 'matriarch', 'cow'],\n",
       " ['males bulls', 'family groups', 'males'],\n",
       " ['adult',\n",
       "  'family groups',\n",
       "  'mate',\n",
       "  'enter state',\n",
       "  'increased testosterone',\n",
       "  'aggression',\n",
       "  'musth',\n",
       "  'dominance',\n",
       "  'reproductive success'],\n",
       " ['calves', 'centre', 'attention', 'family groups', 'mothers', 'years'],\n",
       " ['elephants', 'years', 'wild'],\n",
       " ['touch sight smell',\n",
       "  'sound elephants',\n",
       "  'infrasound',\n",
       "  'seismic communication',\n",
       "  'long distances'],\n",
       " ['elephant intelligence', 'primates', 'cetaceans'],\n",
       " ['selfawareness', 'dead individuals', 'kind'],\n",
       " ['african elephants',\n",
       "  'international union',\n",
       "  'conservation',\n",
       "  'nature iucn',\n",
       "  'asian elephant'],\n",
       " ['threats', 'populations', 'ivory trade', 'animals', 'ivory tusks'],\n",
       " ['threats', 'elephants', 'habitat destruction', 'conflicts', 'local people'],\n",
       " ['elephants', 'animals', 'asia'],\n",
       " ['past', 'war today', 'display', 'zoos', 'entertainment', 'circuses'],\n",
       " ['elephants', 'art folklore religion literature', 'popular culture']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = get_chunks(norm_sentences)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "def get_tfidf_weighted_keyphrases(sentences, \n",
    "                                  grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',\n",
    "                                  top_n=10):\n",
    "    \n",
    "    valid_chunks = get_chunks(sentences, grammar=grammar)\n",
    "                                     \n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    weighted_phrases = {dictionary.get(idx): value \n",
    "                           for doc in corpus_tfidf \n",
    "                               for idx, value in doc}\n",
    "                            \n",
    "    weighted_phrases = sorted(weighted_phrases.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    weighted_phrases = [(term, round(wt, 3)) for term, wt in weighted_phrases]\n",
    "    \n",
    "    return weighted_phrases[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 1.0),\n",
       " ('asia', 0.807),\n",
       " ('wild', 0.764),\n",
       " ('great weight', 0.707),\n",
       " ('pillarlike legs', 0.707),\n",
       " ('southeast asia', 0.693),\n",
       " ('subsaharan africa south asia', 0.693),\n",
       " ('body temperature', 0.693),\n",
       " ('flaps', 0.693),\n",
       " ('fissionfusion society', 0.693),\n",
       " ('multiple family groups', 0.693),\n",
       " ('art folklore religion literature', 0.693),\n",
       " ('popular culture', 0.693),\n",
       " ('ears', 0.681),\n",
       " ('males', 0.653),\n",
       " ('males bulls', 0.653),\n",
       " ('family elephantidae', 0.607),\n",
       " ('large mammals', 0.607),\n",
       " ('years', 0.607),\n",
       " ('environments', 0.577),\n",
       " ('impact', 0.577),\n",
       " ('keystone species', 0.577),\n",
       " ('cetaceans', 0.577),\n",
       " ('elephant intelligence', 0.577),\n",
       " ('primates', 0.577),\n",
       " ('dead individuals', 0.577),\n",
       " ('kind', 0.577),\n",
       " ('selfawareness', 0.577),\n",
       " ('different habitats', 0.57),\n",
       " ('marshes', 0.57)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tfidf_weighted_keyphrases(sentences=norm_sentences, top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('african bush elephant', 0.261),\n",
       " ('including', 0.141),\n",
       " ('family', 0.137),\n",
       " ('cow', 0.124),\n",
       " ('forests', 0.108),\n",
       " ('female', 0.103),\n",
       " ('asia', 0.102),\n",
       " ('objects', 0.098),\n",
       " ('sight', 0.098),\n",
       " ('ivory', 0.098),\n",
       " ('tigers', 0.098),\n",
       " ('males', 0.088),\n",
       " ('folklore', 0.087),\n",
       " ('religion', 0.087),\n",
       " ('known', 0.087),\n",
       " ('larger ears', 0.085),\n",
       " ('water', 0.075),\n",
       " ('highly recognisable', 0.075),\n",
       " ('breathing lifting', 0.074),\n",
       " ('flaps', 0.073),\n",
       " ('africa', 0.072),\n",
       " ('gomphotheres', 0.072),\n",
       " ('animals tend', 0.071),\n",
       " ('success', 0.071),\n",
       " ('south', 0.07)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "key_words = keywords(data[0], ratio=1.0, scores=True, lemmatize=True)\n",
    "[(item, round(score, 3)) for item, score in key_words][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Text Analytics with Python (2nd Ed.) Chapter 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}