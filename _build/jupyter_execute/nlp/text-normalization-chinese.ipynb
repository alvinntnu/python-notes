{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization (Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `text_normalizer_zh.py`\n",
    "- Including functions for:\n",
    "    - word-seg chinese texts\n",
    "    - clean up texts by removing duplicate spaces and line breaks\n",
    "    - remove incompatible weird characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "#from nltk.corpus import wordnet\n",
    "#import collections\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import text_normalizer_zh as tnz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load text_normalizer_zh.py\n",
    "\"\"\"\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "    These functions are based on the text normalization functions \n",
    "    provided in Text Analytics with Python 2ed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "# from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "## Initialize Trad Chinese dictionary\n",
    "jieba.set_dictionary('../../../RepositoryData/data/jiaba/dict.txt.jiebatw.txt')\n",
    "\n",
    "\n",
    "## Normalize unicode characters\n",
    "def remove_weird_chars(text):\n",
    "    #     ```\n",
    "    #     (NFKD) will apply the compatibility decomposition, i.e.\n",
    "    #     replace all compatibility characters with their equivalents.\n",
    "    #     ```\n",
    "    text = unicodedata.normalize('NFKD', text).encode('utf-8',\n",
    "                                                      'ignore').decode(\n",
    "                                                          'utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "## Remove extra linebreaks\n",
    "def remove_extra_linebreaks(text):\n",
    "    lines = text.split(r'\\n+')\n",
    "    return '\\n'.join(\n",
    "        [re.sub(r'[\\s]+', ' ', l).strip() for l in lines if len(l) != 0])\n",
    "\n",
    "\n",
    "## Remove extra medial/trailing/leading spaces\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(\"\\\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "## Seg the text into words\n",
    "def seg(text):\n",
    "    text_seg = jieba.cut(text)\n",
    "    out = ' '.join(text_seg)\n",
    "    return out\n",
    "\n",
    "\n",
    "## Remove punctuation/symbols\n",
    "def remove_symbols(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    Unicode 6.0 has 7 character categories, and each category has subcategories:\n",
    "\n",
    "    Letter (L): lowercase (Ll), modifier (Lm), titlecase (Lt), uppercase (Lu), other (Lo)\n",
    "    Mark (M): spacing combining (Mc), enclosing (Me), non-spacing (Mn)\n",
    "    Number (N): decimal digit (Nd), letter (Nl), other (No)\n",
    "    Punctuation (P): connector (Pc), dash (Pd), initial quote (Pi), final quote (Pf), open (Ps), close (Pe), other (Po)\n",
    "    Symbol (S): currency (Sc), modifier (Sk), math (Sm), other (So)\n",
    "    Separator (Z): line (Zl), paragraph (Zp), space (Zs)\n",
    "    Other (C): control (Cc), format (Cf), not assigned (Cn), private use (Co), surrogate (Cs)\n",
    "    \n",
    "    \n",
    "    There are 3 ranges reserved for private use (Co subcategory): \n",
    "    U+E000—U+F8FF (6,400 code points), U+F0000—U+FFFFD (65,534) and U+100000—U+10FFFD (65,534). \n",
    "    Surrogates (Cs subcategory) use the range U+D800—U+DFFF (2,048 code points).\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ## Brute-force version: list all possible unicode ranges, but this list is not complete.\n",
    "    #   text = re.sub('[\\u0021-\\u002f\\u003a-\\u0040\\u005b-\\u0060\\u007b-\\u007e\\u00a1-\\u00bf\\u2000-\\u206f\\u2013-\\u204a\\u20a0-\\u20bf\\u2100-\\u214f\\u2150-\\u218b\\u2190-\\u21ff\\u2200-\\u22ff\\u2300-\\u23ff\\u2460-\\u24ff\\u2500-\\u257f\\u2580-\\u259f\\u25a0-\\u25ff\\u2600-\\u26ff\\u2e00-\\u2e7f\\u3000-\\u303f\\ufe50-\\ufe6f\\ufe30-\\ufe4f\\ufe10-\\ufe1f\\uff00-\\uffef─◆╱]+','',text)\n",
    "\n",
    "    text = ''.join(ch for ch in text\n",
    "                   if unicodedata.category(ch)[0] not in ['P', 'S'])\n",
    "    return text\n",
    "\n",
    "\n",
    "## Remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub('\\\\d+', \"\", text)\n",
    "\n",
    "\n",
    "## Remove alphabets\n",
    "def remove_alphabets(text):\n",
    "    return re.sub('[a-zA-Z]+', '', text)\n",
    "\n",
    "\n",
    "## Combine every step\n",
    "def normalize_corpus(corpus,\n",
    "                     is_remove_extra_linebreaks=True,\n",
    "                     is_remove_weird_chars=True,\n",
    "                     is_seg=True,\n",
    "                     is_remove_symbols=True,\n",
    "                     is_remove_numbers=True,\n",
    "                     is_remove_alphabets=True):\n",
    "\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "\n",
    "        if is_remove_extra_linebreaks:\n",
    "            doc = remove_extra_linebreaks(doc)\n",
    "\n",
    "        if is_remove_weird_chars:\n",
    "            doc = remove_weird_chars(doc)\n",
    "\n",
    "        if is_seg:\n",
    "            doc = seg(doc)\n",
    "\n",
    "        if is_remove_symbols:\n",
    "            doc = remove_symbols(doc)\n",
    "\n",
    "        if is_remove_alphabets:\n",
    "            doc = remove_alphabets(doc)\n",
    "\n",
    "        if is_remove_numbers:\n",
    "            doc = remove_numbers(doc)\n",
    "\n",
    "        normalized_corpus.append(remove_extra_spaces(doc))\n",
    "\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract an Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grab the first article from Google news for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '\\n', '\\n', '\\n', '\\r\\n國民黨台南市議員謝龍介和民進黨台中市議員謝志忠，今年年初因為立法委員選情互賭，謝龍介賭輸，今天到台中市豐原葫蘆墩公園準備跳河履行承諾。', '\\n', '\\r\\n他今天在跳水前接受媒體採訪，針對NCC昨天公聽會要審查中天換照公聽會，謝龍介，不要有用民粹殺死媒體 ，他這等於是公審方式，對任何媒體都不可以這樣做，他說每個媒體可能都不盡大家滿意，也不可能和所有人民立場一樣。', '\\n', '\\r\\n謝龍介說， 這種公聽會方式為了服務執政黨，不符合言論自由的真義，他說昨天的公聽會是錯誤的示範， 否則將來每個媒體要換照是否都要直播， 讓立場不同的人來撻伐。', '\\n', '\\r\\n謝龍介認為媒體應該回歸市場機制 ，讓市場決定媒體是否有能力存在經營， 他說這樣民主發展才可長可久， 這是我們奮鬥幾十年來得來不易的言論自由，他要NCC緊急煞車。台南市議員謝龍介今天下午，要在台中市豐原葫蘆墩公園跳水還賭約。記者游振昇/攝影', '\\n', '\\n', '\\n                    國民黨台南市議員謝龍介和民進黨台中市議員謝志忠，今年年初因為立法委員選情互賭，謝龍介賭輸，今天到台中市豐原葫蘆墩公園準備...                  ', '\\n                    針對10月26日中天新聞台換照聽證會，前立委孫大千在臉書表示，以聽證內容來看，顯然鑑定人已經有了預設立場，這根本就是一場...                  ', '\\n                    國家通訊傳播委員會昨天舉辦中天新聞台換照聽證會，7位受邀鑑定人一面倒認為中天有缺失。國民黨立委賴士葆在臉書批評，他同意台...                  ', '\\n                    中天電視台換照議題，是否衝擊蔡政府？財團法人台灣民意基金會上午公布最新民調顯示，有三成三民眾樂見中天電視台被撤照，五成三...                  ', '\\n                    民進黨台北市議員王世堅自振興三倍券後，屢屢槓上行政院長蘇貞昌，日前指蘇「不只想當行政院長」，這次中天換照案，王世堅也暗指...                  ', '\\n                    中天這次能否順利換照，其實，民進黨政府承受極大的壓力，因為關或不關閉這家新聞台，都會得罪另一群人，所以大家都在猜，蔡政府最後怎麼選擇？結果又將如何？                  ', '\\n                    國家通訊傳播委員會（ＮＣＣ）昨天針對中天新聞台申請換照案舉行聽證會，聲援中天的前新聞局長鍾琴與「公民監督平台」在立法院外...                  ', '\\n                    旺中董事長蔡衍明昨天以中天新聞台大股東神旺投資負責人名義，親上火線出席中天新聞台換照聽證會，他撇清自己不會控管新聞，「我...                  ', '\\n                    國家通訊傳播委員會（ＮＣＣ）昨天召開中天電視台換照案聽證會，現場委員聚焦在新聞室內控與人事調度議題，詢問中天電視新聞部總...                  ', '\\n                    眾所矚目的中天新聞台換照案，國家通訊傳播委員會（ＮＣＣ）昨天首度召開聽證會，議程破紀錄長達八個多小時，鑑定人質疑中天未履...                  ', '\\n                    民進黨發言人顏若芳今天指出，中天換照是媒體依法接受NCC專業審查的程序，NCC是獨立機關，相信會依照法規及專業，對審查電...                  ', '\\n                    今天NCC 舉行中天換照案的聽證會，會議對外直播，讓我們可以一窺NCC委員對中天新聞台的各種質疑與批判，其中一個令我印象深刻的問題是NCC委員詢問中天，一位編審是否每天有看完每一則中天推出的報導？藉此要凸顯中天的內控可能有問題。                  ', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFptTXpJU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant'\n",
    "r = requests.get(url)\n",
    "web_content = r.text\n",
    "soup = BeautifulSoup(web_content, 'lxml')\n",
    "title = soup.find_all('a', class_='DY5T1d')\n",
    "first_art_link = title[0]['href'].replace('.', 'https://news.google.com', 1)\n",
    "\n",
    "#print(first_art_link)\n",
    "art_request = requests.get(first_art_link)\n",
    "art_request.encoding = 'utf8'\n",
    "soup_art = BeautifulSoup(art_request.text, 'lxml')\n",
    "\n",
    "art_content = soup_art.find_all('p')\n",
    "art_texts = [p.text for p in art_content]\n",
    "print(art_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalized results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/Alvin/GoogleDrive/_MySyncDrive/RepositoryData/data/jiaba/dict.txt.jiebatw.txt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from cache /var/folders/n7/ltpzwx813c599nfxfb94s_640000gn/T/jieba.u1b52b47246a0f2e6497af6bbe107adac.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.572 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['國民黨 台南 市議員 謝龍介 和 民進黨 台中市 議員 謝志忠 今年 年初 因為 立法 委員 選情 互賭 謝龍介 賭輸 今天 到 台中市 豐原 葫蘆墩 公園 準備 跳河 履行 承諾 他 今天 在 跳水 前 接受 媒體 採訪 針對 昨天 公聽會 要 審查 中天 換照 公聽會 謝龍介 不要 有用 民粹 殺死 媒體 他 這 等於 是 公審 方式 對 任何 媒體 都 不可以 這樣 做 他 說 每個 媒體 可能 都 不盡 大家 滿意 也 不可能 和 所有 人民 立場 一樣 謝龍介 說 這種 公聽會 方式 為了 服務 執政黨 不 符合 言論 自由 的 真義 他 說 昨天 的 公聽會 是 錯誤 的 示範 否則 將來 每個 媒體 要 換照 是否 都 要 直播 讓 立場 不同 的 人來 撻伐 謝龍介 認為 媒體 應該 回歸 市場 機制 讓 市場 決定 媒體 是否 有 能力 存在 經營 他 說 這樣 民主 發展 才 可長 可久 這是 我們 奮鬥 幾十年 來得 來 不易 的 言論 自由 他 要 緊急 煞車 台南 市議員 謝龍介 今天 下午 要 在 台中市 豐原 葫蘆墩 公園 跳水 還賭 約 記者 游振 昇 攝影 國民黨 台南 市議員 謝龍介 和 民進黨 台中市 議員 謝志忠 今年 年初 因為 立法 委員 選情 互賭 謝龍介 賭輸 今天 到 台中市 豐原 葫蘆墩 公園 準備 針對 月 日 中天 新聞台 換照 聽證會 前立委 孫大千 在 臉書 表示 以 聽證 內容 來 看 顯然 鑑定人 已經 有 了 預設 立場 這 根本 就是 一場 國家 通訊 傳播 委員會 昨天 舉辦 中天 新聞台 換照 聽證會 位 受邀 鑑定人 一面倒 認為 中天 有 缺失 國民黨 立委 賴士葆 在 臉書 批評 他 同意 台 中天 電視台 換照 議題 是否 衝擊 蔡 政府 財團 法人 台灣 民意 基金會 上午 公布 最 新 民調 顯示 有三成 三 民眾 樂見 中天 電視台 被 撤照 五成 三 民進黨 台北 市議員 王世堅 自 振興 三倍券 後 屢屢 槓上 行政院長 蘇貞昌 日前 指蘇 不只 想當 行政院長 這次 中天 換照 案 王世堅 也 暗指 中天 這次 能否 順利 換照 其實 民進黨 政府 承受 極大 的 壓力 因為 關或 不 關閉 這家 新聞台 都 會 得罪 另 一群 人 所以 大家 都 在 猜 蔡 政府 最後 怎麼 選擇 結果 又將 如何 國家 通訊 傳播 委員會 昨天 針對 中天 新聞台 申請 換照 案 舉行 聽證會 聲援 中天 的 前新聞局長 鍾 琴 與 公民 監督 平台 在 立法院 外 旺中 董事長 蔡衍明 昨天 以 中天 新聞 台大 股東 神旺 投資 負責人 名義 親 上火線 出席 中天 新聞台 換照 聽證會 他 撇清 自己 不會 控管 新聞 我 國家 通訊 傳播 委員會 昨天 召開 中天 電視台 換照 案 聽證會 現場 委員 聚焦 在 新聞 室內 控與 人事 調度 議題 詢問 中天 電視 新聞部 總 眾所矚目 的 中天 新聞台 換照 案 國家 通訊 傳播 委員會 昨天 首度 召開 聽證會 議程 破紀錄 長達 八個 多 小時 鑑定人 質疑 中天 未履 民進黨 發言人 顏若芳 今天 指出 中天 換照 是 媒體 依法 接受 專業 審查 的 程序 是 獨立 機關 相信 會 依照 法規 及 專業 對 審查 電 今天 舉行 中天 換照 案 的 聽證會 會議 對外 直播 讓 我們 可以 一窺 委員 對 中天 新聞台 的 各種 質疑 與 批判 其中 一個 令 我 印象 深刻 的 問題 是 委員 詢問 中天 一位 編審 是否 每天 有 看完 每一則 中天 推出 的 報導 藉 此要 凸顯 中天 的 內控 可能 有 問題']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnz.normalize_corpus([' '.join(art_texts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-notes",
   "language": "python",
   "name": "python-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}