# Tokenization

- How to process texts for a computational understanding of the text
- Focus on tokenization, i.e., segmenting texts into shorter yet meaningful linguistic units